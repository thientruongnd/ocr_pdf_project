{
  "job_id": "0f6b97d6-0b92-4524-9363-47a17acee3cb",
  "original_filename": "Alex-System.pdf",
  "saved_pdf_path": "uploads\\0f6b97d6-0b92-4524-9363-47a17acee3cb.pdf",
  "total_pages": 269,
  "results": [
    {
      "page": 1,
      "text": "SECOND EDITION\n\nSYSTEM\nDESIGN\nINTERVIEW\n\nAN INSIDER'S GUIDE"
    },
    {
      "page": 2,
      "text": "System Design Interview: An Insider’s Guide\n\nAll rights reserved. This book or any portion thereof may not be reproduced or used in any\nmanner whatsoever without the express written permission of the publisher except for the use\nof brief quotations in a book review.\n\nAbout the author:\n\nAlex Xu is an experienced software engineer and entrepreneur. Previously, he worked at\nTwitter, Apple, Zynga and Oracle. He received his M.S. from Carnegie Mellon University.\nHe has a passion for designing and implementing complex systems.\n\nPlease subscribe to our email list if you want to be notified when new chapters are available:\nhttps://bit.ly/3dtIcsE\n\nFor more information, contact systemdesigninsider@gmail.com\n\nEditor: Paul Solomon"
    },
    {
      "page": 3,
      "text": "Table of Contents\n\nSystem Design Interview: An Insider’s Guide\n\nFORWARD\nCHAPTER 1:\n\nSCALE FROM ZERO TO MILLIONS OF USERS\n\nCHAPTER 2:\n\nBACK-OF-THE-ENVELOPE ESTIMATION\n\nCHAPTER 3:\n\nA FRAMEWORK FOR SYSTEM DESIGN INTERVIEWS\n\nCHAPTER 4:\n\nDESIGN A RATE LIMITER\n\nCHAPTER 5:\n\nDESIGN CONSISTENT HASHING\n\nCHAPTER 6:\n\nDESIGN A KEY-VALUE STORE\n\nCHAPTER 7:\n\nDESIGN A UNIQUE ID GENERATOR IN DISTRIBUTED SYSTEMS\n\nCHAPTER 8:\n\nDESIGN A URL SHORTENER\n\nCHAPTER 9:\n\nDESIGN A WEB CRAWLER\n\nCHAPTER 10:\n\nDESIGN A NOTIFICATION SYSTEM\n\nCHAPTER 11:\n\nDESIGN A NEWS FEED SYSTEM\n\nCHAPTER 12:\n\nDESIGN A CHAT SYSTEM\n\nCHAPTER 13:\n\nDESIGN A SEARCH AUTOCOMPLETE SYSTEM\n\nCHAPTER 14:\n\nDESIGN YOUTUBE\n\nCHAPTER 15:\n\nDESIGN GOOGLE DRIVE\n\nCHAPTER 16:\n\nTHE LEARNING CONTINUES\n\nAFTERWORD"
    },
    {
      "page": 4,
      "text": "FORWARD\n\nWe are delighted that you have decided to join us in learning the system design interviews.\nSystem design interview questions are the most difficult to tackle among all the technical\ninterviews. The questions require the interviewees to design an architecture for a software\nsystem, which could be a news feed, Google search, chat system, etc. These questions are\nintimidating, and there is no certain pattern to follow. The questions are usually very big\nscoped and vague. The processes are open-ended and unclear without a standard or correct\nanswer.\n\nCompanies widely adopt system design interviews because the communication and problem-\nsolving skills tested in these interviews are similar to those required by a software engineer’s\ndaily work. An interviewee is evaluated based on how she analyzes a vague problem and how\nshe solves the problem step by step. The abilities tested also involve how she explains the\nidea, discusses with others, and evaluates and optimizes the system. In English, using “she”\nflows better than “he or she” or jumping between the two. To make reading easier, we use the\nfeminine pronoun throughout this book. No disrespect is intended for male engineers.\n\nThe system design questions are open-ended. Just like in the real world, there are many\ndifferences and variations in the system. The desired outcome is to come up with an\narchitecture to achieve system design goals. The discussions could go in different ways\ndepending on the interviewer. Some interviewers may choose high-level architecture to cover\nall aspects; whereas some might choose one or more areas to focus on. Typically, system\nrequirements, constraints and bottlenecks should be well understood to shape the direction of\nboth the interviewer and interviewee.\n\nThe objective of this book is to provide a reliable strategy to approach the system design\nquestions. The right strategy and knowledge are vital to the success of an interview.\n\nThis book provides solid knowledge in building a scalable system. The more knowledge\ngained from reading this book, the better you are equipped in solving the system design\nquestions.\n\nThis book also provides a step by step framework on how to tackle a system design question.\nIt provides many examples to illustrate the systematic approach with detailed steps that you\ncan follow. With constant practice, you will be well-equipped to tackle system design\ninterview questions."
    },
    {
      "page": 5,
      "text": "CHAPTER 1: SCALE FROM ZERO TO MILLIONS OF\nUSERS\n\nDesigning a system that supports millions of users is challenging, and it is a journey that\nrequires continuous refinement and endless improvement. In this chapter, we build a system\nthat supports a single user and gradually scale it up to serve millions of users. After reading\nthis chapter, you will master a handful of techniques that will help you to crack the system\ndesign interview questions."
    },
    {
      "page": 6,
      "text": "Single server setup\n\nA journey of a thousand miles begins with a single step, and building a complex system is no\ndifferent. To start with something simple, everything is running on a single server. Figure 1-1\nshows the illustration of a single server setup where everything is running on one server: web\napp, database, cache, etc.\n\n_ CI api.mysite.com (SNS)\n=—_——=— en,\n\nA £7\n\nWeb browser Mobile app IP address\n\nwww.mysite.com api.mysite.com\n\n' Web server\n\nFigure 1-1\n\nTo understand this setup, it is helpful to investigate the request flow and traffic source. Let us\nfirst look at the request flow (Figure 1-2).\n\nU\nser if @ api.mysite.com (SNS)\n>\n—=— We,\nWeb browser Mobile app @) 15.125.23.214\n@ 15.125.23.214 (@) HTML page\n\nWeb server :\n\nFigure 1-2"
    },
    {
      "page": 7,
      "text": "1. Users access websites through domain names, such as api.mysite.com. Usually, the\nDomain Name System (DNS) is a paid service provided by 3rd parties and not hosted by\nour servers.\n\n2. Internet Protocol (IP) address is returned to the browser or mobile app. In the example,\nIP address 15.125.23.214 is returned.\n\n3. Once the IP address is obtained, Hypertext Transfer Protocol (HTTP) [1] requests are\nsent directly to your web server.\n\n4. The web server returns HTML pages or JSON response for rendering.\n\nNext, let us examine the traffic source. The traffic to your web server comes from two\nsources: web application and mobile application.\n\n* Web application: it uses a combination of server-side languages (Java, Python, etc.) to\nhandle business logic, storage, etc., and client-side languages (HTML and JavaScript) for\npresentation.\n\n* Mobile application: HTTP protocol is the communication protocol between the mobile\napp and the web server. JavaScript Object Notation (JSON) is commonly used API\nresponse format to transfer data due to its simplicity. An example of the API response in\nJSON format is shown below:\n\nGET /users/12 — Retrieve user object for id = 12\n{\n\"id\": 12,\n“firstName\": \"John\",\n\"lastName\": \"Smith\",\n\"address\": {\n\"streetAddress\": \"21 2nd Street\",\n\"city\": \"New York\",\n\"state\": \"NY\",\n\"postalCode\": 10021\n}\n\"phoneNumbers\": [\n\"212 555-1234\",\n\"646 555-4567\""
    },
    {
      "page": 8,
      "text": "Database\n\nWith the growth of the user base, one server is not enough, and we need multiple servers: one\nfor web/mobile traffic, the other for the database (Figure 1-3). Separating web/mobile traffic\n(web tier) and database (data tier) servers allows them to be scaled independently.\n\nUser\n—] C] www.mysite.com\n—— <\nWeb browser Mobile app IP address\nwww.mysite.com api.mysite.com\n\nread/write/update\n\n: return data\n: Web server « ; Database\n\nFigure 1-3\nWhich databases to use?\n\nYou can choose between a traditional relational database and a non-relational database. Let\nus examine their differences.\n\nRelational databases are also called a relational database management system (RDBMS) or\nSQL database. The most popular ones are MySQL, Oracle database, PostgreSQL, etc.\nRelational databases represent and store data in tables and rows. You can perform join\noperations using SQL across different database tables.\n\nNon-Relational databases are also called NoSQL databases. Popular ones are CouchDB,\nNeo4j, Cassandra, HBase, Amazon DynamoDB, etc. [2]. These databases are grouped into\nfour categories: key-value stores, graph stores, column stores, and document stores. Join\noperations are generally not supported in non-relational databases.\nFor most developers, relational databases are the best option because they have been around\nfor over 40 years and historically, they have worked well. However, if relational databases\nare not suitable for your specific use cases, it is critical to explore beyond relational\ndatabases. Non-relational databases might be the right choice if:\n\n* Your application requires super-low latency.\n\n* Your data are unstructured, or you do not have any relational data.\n\n* You only need to serialize and deserialize data (JSON, XML, YAML, etc.).\n\n* You need to store a massive amount of data."
    },
    {
      "page": 9,
      "text": "Vertical scaling vs horizontal scaling\n\nVertical scaling, referred to as “scale up”, means the process of adding more power (CPU,\nRAM, etc.) to your servers. Horizontal scaling, referred to as “scale-out”, allows you to scale\nby adding more servers into your pool of resources.\n\nWhen traffic is low, vertical scaling is a great option, and the simplicity of vertical scaling is\nits main advantage. Unfortunately, it comes with serious limitations.\n\n* Vertical scaling has a hard limit. It is impossible to add unlimited CPU and memory to a\nsingle server.\n\n* Vertical scaling does not have failover and redundancy. If one server goes down, the\nwebsite/app goes down with it completely.\n\nHorizontal scaling is more desirable for large scale applications due to the limitations of\nvertical scaling.\n\nIn the previous design, users are connected to the web server directly. Users will unable to\naccess the website if the web server is offline. In another scenario, if many users access the\nweb server simultaneously and it reaches the web server’s load limit, users generally\nexperience slower response or fail to connect to the server. A load balancer is the best\ntechnique to address these problems."
    },
    {
      "page": 10,
      "text": "Load balancer\n\nA load balancer evenly distributes incoming traffic among web servers that are defined in a\nload-balanced set. Figure 1-4 shows how a load balancer works.\n\nm\n\nWeb browser Mobile app\n\nPublic IP: 88.88.88.1\nIP Address\n\nUser\n\nLoad balancer\n\n/\n\nPrivate IP: 10.0.0.1 Private IP: 10.0.0.2\n\nFigure 1-4\nAs shown in Figure 1-4, users connect to the public IP of the load balancer directly. With this\nsetup, web servers are unreachable directly by clients anymore. For better security, private\nIPs are used for communication between servers. A private IP is an IP address reachable only\nbetween servers in the same network; however, it is unreachable over the internet. The load\nbalancer communicates with web servers through private IPs.\nIn Figure 1-4, after a load balancer and a second web server are added, we successfully\nsolved no failover issue and improved the availability of the web tier. Details are explained\nbelow:\n+ If server 1 goes offline, all the traffic will be routed to server 2. This prevents the website\nfrom going offline. We will also add a new healthy web server to the server pool to\nbalance the load.\n+ If the website traffic grows rapidly, and two servers are not enough to handle the traffic,\nthe load balancer can handle this problem gracefully. You only need to add more servers\nto the web server pool, and the load balancer automatically starts to send requests to them.\n\nNow the web tier looks good, what about the data tier? The current design has one database,"
    },
    {
      "page": 11,
      "text": "so it does not support failover and redundancy. Database replication is a common technique\nto address those problems. Let us take a look."
    },
    {
      "page": 12,
      "text": "Database replication\n\nQuoted from Wikipedia: “Database replication can be used in many database management\nsystems, usually with a master/slave relationship between the original (master) and the copies\n(slaves)” [3].\n\nA master database generally only supports write operations. A slave database gets copies of\nthe data from the master database and only supports read operations. All the data-modifying\ncommands like insert, delete, or update must be sent to the master database. Most\napplications require a much higher ratio of reads to writes; thus, the number of slave\ndatabases in a system is usually larger than the number of master databases. Figure 1-5 shows\na master database with multiple slave databases.\n\n' 1\n| Eafe |\n1 1\n1 1\n1 1\n1 1\n1 1\n' 1\n1 1\n‘ !\n\nWeb servers\n-\n\nx\n\nwrites\n\nDB replication\n\nSlave DB1\n\nreads\n\nSlave DB2\n\nSlave DB3\n\nDB replication\n\nMaster DB\n\nDB replication\n\nFigure 1-5\nAdvantages of database replication:\n\n+ Better performance: In the master-slave model, all writes and updates happen in master\nnodes; whereas, read operations are distributed across slave nodes. This model improves\nperformance because it allows more queries to be processed in parallel.\n\n+ Reliability: If one of your database servers is destroyed by a natural disaster, such as a\ntyphoon or an earthquake, data is still preserved. You do not need to worry about data loss\nbecause data is replicated across multiple locations.\n\n+ High availability: By replicating data across different locations, your website remains in"
    },
    {
      "page": 13,
      "text": "operation even if a database is offline as you can access data stored in another database\nserver.\n\nIn the previous section, we discussed how a load balancer helped to improve system\navailability. We ask the same question here: what if one of the databases goes offline? The\narchitectural design discussed in Figure 1-5 can handle this case:\n\n« If only one slave database is available and it goes offline, read operations will be directed\nto the master database temporarily. As soon as the issue is found, a new slave database\n\nwill replace the old one. In case multiple slave databases are available, read operations are\nredirected to other healthy slave databases. A new database server will replace the old one.\n\n« If the master database goes offline, a slave database will be promoted to be the new\nmaster. All the database operations will be temporarily executed on the new master\ndatabase. A new slave database will replace the old one for data replication immediately.\nIn production systems, promoting a new master is more complicated as the data in a slave\ndatabase might not be up to date. The missing data needs to be updated by running data\nrecovery scripts. Although some other replication methods like multi-masters and circular\nreplication could help, those setups are more complicated; and their discussions are\nbeyond the scope of this book. Interested readers should refer to the listed reference\nmaterials [4] [5].\n\nFigure 1-6 shows the system design after adding the load balancer and database replication."
    },
    {
      "page": 14,
      "text": "qT NS\n\nJ O www.mysite.com\nSE\n\nWeb browser Mobile app IP address\n\nA 2\n\nwww.mysite.com api.mysite.com\n\nLoad balancer\n\n1\n1\n| Web tier\ni\n|\nL Write Read Read\nns oo ‘\ni Replicate ;\n; ' Data tier\nt Master DB Slave DB ;\nFigure 1-6\n\nLet us take a look at the design:\n+ A user gets the IP address of the load balancer from DNS.\n« A user connects the load balancer with this IP address.\n« The HTTP request is routed to either Server 1 or Server 2.\n+ A web server reads user data from a slave database.\n+ A web server routes any data-modifying operations to the master database. This includes\nwrite, update, and delete operations.\n\nNow, you have a solid understanding of the web and data tiers, it is time to improve the\nload/response time. This can be done by adding a cache layer and shifting static content\n(JavaScript/CSS/image/video files) to the content delivery network (CDN)."
    },
    {
      "page": 15,
      "text": "Cache\n\nA cache is a temporary storage area that stores the result of expensive responses or frequently\naccessed data in memory so that subsequent requests are served more quickly. As illustrated\nin Figure 1-6, every time a new web page loads, one or more database calls are executed to\nfetch data. The application performance is greatly affected by calling the database repeatedly.\nThe cache can mitigate this problem.\n\nCache tier\n\nThe cache tier is a temporary data store layer, much faster than the database. The benefits of\nhaving a separate cache tier include better system performance, ability to reduce database\nworkloads, and the ability to scale the cache tier independently. Figure 1-7 shows a possible\nsetup of a cache server:\n\n1. If data exists in cache, read data from cache a\n\nWeb server 2.2 Return data to the web server Cache 2.1 If data doesn't exist in cache,\n\nsave data to cache Database\n\nFigure 1-7\n\nAfter receiving a request, a web server first checks if the cache has the available response. If\nit has, it sends data back to the client. If not, it queries the database, stores the response in\ncache, and sends it back to the client. This caching strategy is called a read-through cache.\nOther caching strategies are available depending on the data type, size, and access patterns. A\nprevious study explains how different caching strategies work [6].\n\nInteracting with cache servers is simple because most cache servers provide APIs for\ncommon programming languages. The following code snippet shows typical Memcached\nAPIs:\n\nSECONDS = 1\ncache.set('‘myKey’, ‘hi there’, 3600 * SECONDS)\ncache.get(‘myKey’)\n\nConsiderations for using cache\nHere are a few considerations for using a cache system:\n\n* Decide when to use cache. Consider using cache when data is read frequently but\nmodified infrequently. Since cached data is stored in volatile memory, a cache server is\nnot ideal for persisting data. For instance, if a cache server restarts, all the data in memory\nis lost. Thus, important data should be saved in persistent data stores.\n\n¢ Expiration policy. It is a good practice to implement an expiration policy. Once cached\ndata is expired, it is removed from the cache. When there is no expiration policy, cached\ndata will be stored in the memory permanently. It is advisable not to make the expiration\ndate too short as this will cause the system to reload data from the database too frequently.\nMeanwhile, it is advisable not to make the expiration date too long as the data can become\nstale.\n\n* Consistency: This involves keeping the data store and the cache in sync. Inconsistency\ncan happen because data-modifying operations on the data store and cache are not ina\nsingle transaction. When scaling across multiple regions, maintaining consistency between"
    },
    {
      "page": 16,
      "text": "the data store and cache is challenging. For further details, refer to the paper titled\n“Scaling Memcache at Facebook” published by Facebook [7].\n\n* Mitigating failures: A single cache server represents a potential single point of failure\n(SPOF), defined in Wikipedia as follows: “A single point of failure (SPOF) is a part of a\nsystem that, if it fails, will stop the entire system from working” [8]. As a result, multiple\ncache servers across different data centers are recommended to avoid SPOF. Another\nrecommended approach is to overprovision the required memory by certain percentages.\nThis provides a buffer as the memory usage increases.\n\nDp SINGLE POINT\nOF FAILURE\n\nApplication\nServer\n\nFigure 1-8 (source: https://bit.ly/3eGsnyH)\n\n* Eviction Policy: Once the cache is full, any requests to add items to the cache might\ncause existing items to be removed. This is called cache eviction. Least-recently-used\n(LRU) is the most popular cache eviction policy. Other eviction policies, such as the Least\nFrequently Used (LFU) or First in First Out (FIFO), can be adopted to satisfy different use\ncases."
    },
    {
      "page": 17,
      "text": "Content delivery network (CDN)\n\nA CDN is a network of geographically dispersed servers used to deliver static content. CDN\nservers cache static content like images, videos, CSS, JavaScript files, etc.\n\nDynamic content caching is a relatively new concept and beyond the scope of this book. It\n\nenables the caching of HTML pages that are based on request path, query strings, cookies,\n\nand request headers. Refer to the article mentioned in reference material [9] for more about\nthis. This book focuses on how to use CDN to cache static content.\n\nHere is how CDN works at the high-level: when a user visits a website, a CDN server closest\nto the user will deliver static content. Intuitively, the further users are from CDN servers, the\nslower the website loads. For example, if CDN servers are in San Francisco, users in Los\nAngeles will get content faster than users in Europe. Figure 1-9 is a great example that shows\nhow CDN improves load time.\n\nOrigin\n\nFigure 1-9 (source: https://bit.ly/2yv7DJK)\n\nFigure 1-10 demonstrates the CDN workflow.\n\n2. if not in CDN, get image.png from server\n\n3. store image.png in CDN\n\nFigure 1-10\n\n1. User A tries to get image.png by using an image URL. The URL’s domain is provided\nby the CDN provider. The following two image URLs are samples used to demonstrate\nwhat image URLs look like on Amazon and Akamai CDNs:\n\n+ https://mysite.cloudfront.net/logo.jpg\n\n+ https://mysite.akamai.com/image-manager/img/logo.jpg\n2. If the CDN server does not have image.png in the cache, the CDN server requests the\nfile from the origin, which can be a web server or online storage like Amazon S3.\n\n3. The origin returns image.png to the CDN server, which includes optional HTTP header\nTime-to-Live (TTL) which describes how long the image is cached."
    },
    {
      "page": 18,
      "text": "4. The CDN caches the image and returns it to User A. The image remains cached in the\nCDN until the TTL expires.\n\n5. User B sends a request to get the same image.\n\n6. The image is returned from the cache as long as the TTL has not expired.\n\nConsiderations of using a CDN\n\n* Cost: CDNs are run by third-party providers, and you are charged for data transfers in\nand out of the CDN. Caching infrequently used assets provides no significant benefits so\nyou should consider moving them out of the CDN.\n\n* Setting an appropriate cache expiry: For time-sensitive content, setting a cache expiry\ntime is important. The cache expiry time should neither be too long nor too short. If it is\ntoo long, the content might no longer be fresh. If it is too short, it can cause repeat\nreloading of content from origin servers to the CDN.\n\n* CDN fallback: You should consider how your website/application copes with CDN\nfailure. If there is a temporary CDN outage, clients should be able to detect the problem\nand request resources from the origin.\n* Invalidating files: You can remove a file from the CDN before it expires by performing\none of the following operations:\n« Invalidate the CDN object using APIs provided by CDN vendors.\n* Use object versioning to serve a different version of the object. To version an object,\nyou can add a parameter to the URL, such as a version number. For example, version\nnumber 2 is added to the query string: image.png?v=2.\n\nFigure 1-11 shows the design after the CDN and cache are added."
    },
    {
      "page": 19,
      "text": "User\n\nm\n\nWeb browser Mobile app\n\n@ CDN\n\nra b\n1 !\n' 1\n1 1\n: : Web tier\n1 1\n1 !\n‘ ’\n\nCACHE\n\nCache\nReplicate @\n\naL\n\n' Data tier\nMaster DB Slave DB !}\n\na\n\nFigure 1-11\n\n1. Static assets (JS, CSS, images, etc.,) are no longer served by web servers. They are\nfetched from the CDN for better performance.\n\n2. The database load is lightened by caching data."
    },
    {
      "page": 20,
      "text": "Stateless web tier\n\nNow it is time to consider scaling the web tier horizontally. For this, we need to move state\n(for instance user session data) out of the web tier. A good practice is to store session data in\nthe persistent storage such as relational database or NoSQL. Each web server in the cluster\ncan access state data from databases. This is called stateless web tier.\n\nStateful architecture\n\nA stateful server and stateless server has some key differences. A stateful server remembers\nclient data (state) from one request to the next. A stateless server keeps no state information.\n\nFigure 1-12 shows an example of a stateful architecture.\n\n¢ Session data for User A ¢ Session data for User B ¢ Session data for User C\n\ne Profile image for User A e Profile image for User B e Profile image for User C\n\nFigure 1-12\n\nIn Figure 1-12, user A’s session data and profile image are stored in Server 1. To authenticate\nUser A, HTTP requests must be routed to Server 1. If a request is sent to other servers like\nServer 2, authentication would fail because Server 2 does not contain User A’s session data.\nSimilarly, all HTTP requests from User B must be routed to Server 2; all requests from User\nC must be sent to Server 3.\n\nThe issue is that every request from the same client must be routed to the same server. This\ncan be done with sticky sessions in most load balancers [10]; however, this adds the\noverhead. Adding or removing servers is much more difficult with this approach. It is also\nchallenging to handle server failures.\n\nStateless architecture\nFigure 1-13 shows the stateless architecture."
    },
    {
      "page": 21,
      "text": "User A User B User C\n\n=\n&\nLP\nSs\n2\nysanbal dyy\n\nWeb servers\n\nMeee eee ee ee\n\nayeys Yo}9}\n\nShared Storage\nFigure 1-13\n\nIn this stateless architecture, HTTP requests from users can be sent to any web servers, which\nfetch state data from a shared data store. State data is stored in a shared data store and kept\nout of web servers. A stateless system is simpler, more robust, and scalable.\n\nFigure 1-14 shows the updated design with a stateless web tier."
    },
    {
      "page": 22,
      "text": "User\n\nmm 6\n\nWeb browser Mobile app\n\nCDN\nwww.mysite.com api.mysite.com\nLoad balancer\nee ae ee eee ~\\\n1 1\n1 1\nt 1\n‘ | @)Auto scale\nt 1\n\\ Server1 Server2 Server3  Server4 }\n\nReplicate Replicate\n\nSlave DB\n\nMaster DB Slave DB Cache NoSQL\n\nFigure 1-14\n\nIn Figure 1-14, we move the session data out of the web tier and store them in the persistent\ndata store. The shared data store could be a relational database, Memcached/Redis, NoSQL,\netc. The NoSQL data store is chosen as it is easy to scale. Autoscaling means adding or\nremoving web servers automatically based on the traffic load. After the state data is removed\nout of web servers, auto-scaling of the web tier is easily achieved by adding or removing\nservers based on traffic load.\n\nYour website grows rapidly and attracts a significant number of users internationally. To\nimprove availability and provide a better user experience across wider geographical areas,\nsupporting multiple data centers is crucial."
    },
    {
      "page": 23,
      "text": "Data centers\n\nFigure 1-15 shows an example setup with two data centers. In normal operation, users are\ngeoDNS-routed, also known as geo-routed, to the closest data center, with a split traffic of\nx% in US-East and (100 — x)% in US-West. geoDNS is a DNS service that allows domain\nnames to be resolved to IP addresses based on the location of a user.\n\nUser\n\nm\n\nWeb browser Mobile app\n\nCDN\n\napi.mysite.com\n\nwww.mysite.com\n\nLoad balancer\n\nSeocoeseuas\n\n. ,\n\nF - . F , “\n\nDpc1 ; 1 ’ DCc2 ! ry ‘\n\nUS-East ! —|_— ; US-West | =] =] : 4\ni 1\n\n' 1 q ' 1 4\n\n1 1 q 1 1 4\n\n1 1 4 1 1 4\n\n1 1 q 1 1 q\n\n1 1 1 1 1 q\n\n| Web servers | 1 | Web servers } 1\n\ni\n\n: 1\n\n1 f\n\n1\n\n1\n\n1\n\nNoSQL\n\nFigure 1-15\n\nIn the event of any significant data center outage, we direct all traffic to a healthy data center.\nIn Figure 1-16, data center 2 (US-West) is offline, and 100% of the traffic is routed to data\ncenter 1 (US-East)."
    },
    {
      "page": 24,
      "text": "User\n\nom G\n\nWeb browser Mobile app CDN\n\nwww.mysite.com api.mysite.com\n\nLoad balancer\n\nNoSQL\n\nFigure 1-16\n\nSeveral technical challenges must be resolved to achieve multi-data center setup:\n\n* Traffic redirection: Effective tools are needed to direct traffic to the correct data center.\nGeoDNS can be used to direct traffic to the nearest data center depending on where a user\nis located.\n\n* Data synchronization: Users from different regions could use different local databases or\ncaches. In failover cases, traffic might be routed to a data center where data is unavailable.\nA common strategy is to replicate data across multiple data centers. A previous study\nshows how Netflix implements asynchronous multi-data center replication [11].\n\n* Test and deployment: With multi-data center setup, it is important to test your\nwebsite/application at different locations. Automated deployment tools are vital to keep\nservices consistent through all the data centers [11].\n\nTo further scale our system, we need to decouple different components of the system so they\ncan be scaled independently. Messaging queue is a key strategy employed by many real-\nworld distributed systems to solve this problem."
    },
    {
      "page": 25,
      "text": "Message queue\n\nA message queue is a durable component, stored in memory, that supports asynchronous\ncommunication. It serves as a buffer and distributes asynchronous requests. The basic\narchitecture of a message queue is simple. Input services, called producers/publishers, create\nmessages, and publish them to a message queue. Other services or servers, called\nconsumers/subscribers, connect to the queue, and perform actions defined by the messages.\nThe model is shown in Figure 1-17.\n\nconsume\n\nProducer subscribe Consumer\n\nMessage Queue\n\nFigure 1-17\n\nDecoupling makes the message queue a preferred architecture for building a scalable and\nreliable application. With the message queue, the producer can post a message to the queue\nwhen the consumer is unavailable to process it. The consumer can read messages from the\nqueue even when the producer is unavailable.\n\nConsider the following use case: your application supports photo customization, including\ncropping, sharpening, blurring, etc. Those customization tasks take time to complete. In\nFigure 1-18, web servers publish photo processing jobs to the message queue. Photo\nprocessing workers pick up jobs from the message queue and asynchronously perform photo\ncustomization tasks. The producer and the consumer can be scaled independently. When the\nsize of the queue becomes large, more workers are added to reduce the processing time.\nHowever, if the queue is empty most of the time, the number of workers can be reduced.\n\netccece = ecttccess\n\n' AY\noe ——1—| i—|— an\n' blish ' 4\n' = M M M —\n\nt\n; 1 ; 1\n1 t\n‘ t\n\n:\n\n‘ i\n7 Producer yf queue for photo processing § consumer 3\nWeb servers Photo processing\nworkers\n\nFigure 1-18"
    },
    {
      "page": 26,
      "text": "Logging, metrics, automation\nWhen working with a small website that runs on a few servers, logging, metrics, and\n\nautomation support are good practices but not a necessity. However, now that your site has\ngrown to serve a large business, investing in those tools is essential.\n\nLogging: Monitoring error logs is important because it helps to identify errors and problems\nin the system. You can monitor error logs at per server level or use tools to aggregate them to\na centralized service for easy search and viewing.\n\nMetrics: Collecting different types of metrics help us to gain business insights and understand\nthe health status of the system. Some of the following metrics are useful:\n* Host level metrics: CPU, Memory, disk I/O, etc.\n\n* Aggregated level metrics: for example, the performance of the entire database tier, cache\ntier, etc.\n\n« Key business metrics: daily active users, retention, revenue, etc.\n\nAutomation: When a system gets big and complex, we need to build or leverage automation\ntools to improve productivity. Continuous integration is a good practice, in which each code\ncheck-in is verified through automation, allowing teams to detect problems early. Besides,\nautomating your build, test, deploy process, etc. could improve developer productivity\nsignificantly.\n\nAdding message queues and different tools\n\nFigure 1-19 shows the updated design. Due to the space constraint, only one data center is\nshown in the figure.\n\n1. The design includes a message queue, which helps to make the system more loosely\ncoupled and failure resilient.\n\n2. Logging, monitoring, metrics, and automation tools are included."
    },
    {
      "page": 27,
      "text": "m © aa\n\nWeb browser Mobile app\n\nCDN\n\n@) Message Queue\n\nNoSQL\n\nCaches J j  Swwana==-5\n\n@ Tools\n\nFigure 1-19\n\nAs the data grows every day, your database gets more overloaded. It is time to scale the data\ntier."
    },
    {
      "page": 28,
      "text": "Database scaling\nThere are two broad approaches for database scaling: vertical scaling and horizontal scaling.\n\nVertical scaling\nVertical scaling, also known as scaling up, is the scaling by adding more power (CPU, RAM,\nDISK, etc.) to an existing machine. There are some powerful database servers. According to\nAmazon Relational Database Service (RDS) [12], you can get a database server with 24 TB\nof RAM. This kind of powerful database server could store and handle lots of data. For\nexample, stackoverflow.com in 2013 had over 10 million monthly unique visitors, but it only\nhad 1 master database [13]. However, vertical scaling comes with some serious drawbacks:\n\n* You can add more CPU, RAM, etc. to your database server, but there are hardware\n\nlimits. If you have a large user base, a single server is not enough.\n\n* Greater risk of single point of failures.\n\n+ The overall cost of vertical scaling is high. Powerful servers are much more expensive.\n\nHorizontal scaling\n\nHorizontal scaling, also known as sharding, is the practice of adding more servers. Figure 1-\n20 compares vertical scaling with horizontal scaling.\n\n.\n\n‘ ° Vertical Scaling ‘\n,\n/ (Increase CPU, RAM, DISK, etc) »\n\nHorizontal Scaling\n(Add more servers)\n\nFigure 1-20\n\nSharding separates large databases into smaller, more easily managed parts called shards.\nEach shard shares the same schema, though the actual data on each shard is unique to the\nshard.\n\nFigure 1-21 shows an example of sharded databases. User data is allocated to a database\nserver based on user IDs. Anytime you access data, a hash function is used to find the\ncorresponding shard. In our example, user_id % 4 is used as the hash function. If the result"
    },
    {
      "page": 29,
      "text": "equals to 0, shard 0 is used to store and fetch data. If the result equals to 1, shard 1 is used.\nThe same logic applies to other shards.\n\nuser_id % 4\n\n=> =\n0 1\n\nFigure 1-21\n\nFigure 1-22 shows the user table in sharded databases.\n\nuser_id\n\nuser_id\n\n2\n6\n10\n14\n\n3\n7\nan\n15\n\nFigure 1-22\n\nThe most important factor to consider when implementing a sharding strategy is the choice of\nthe sharding key. Sharding key (known as a partition key) consists of one or more columns\nthat determine how data is distributed. As shown in Figure 1-22, “user_id” is the sharding\nkey. A sharding key allows you to retrieve and modify data efficiently by routing database\nqueries to the correct database. When choosing a sharding key, one of the most important"
    },
    {
      "page": 30,
      "text": "criteria is to choose a key that can evenly distributed data.\n\nSharding is a great technique to scale the database but it is far from a perfect solution. It\nintroduces complexities and new challenges to the system:\n\nResharding data: Resharding data is needed when 1) a single shard could no longer hold\nmore data due to rapid growth. 2) Certain shards might experience shard exhaustion faster\nthan others due to uneven data distribution. When shard exhaustion happens, it requires\nupdating the sharding function and moving data around. Consistent hashing, which will be\ndiscussed in Chapter 5, is a commonly used technique to solve this problem.\n\nCelebrity problem: This is also called a hotspot key problem. Excessive access to a specific\nshard could cause server overload. Imagine data for Katy Perry, Justin Bieber, and Lady\nGaga all end up on the same shard. For social applications, that shard will be overwhelmed\nwith read operations. To solve this problem, we may need to allocate a shard for each\ncelebrity. Each shard might even require further partition.\n\nJoin and de-normalization: Once a database has been sharded across multiple servers, it is\nhard to perform join operations across database shards. A common workaround is to de-\nnormalize the database so that queries can be performed in a single table.\n\nIn Figure 1-23, we shard databases to support rapidly increasing data traffic. At the same\ntime, some of the non-relational functionalities are moved to a NoSQL data store to reduce\nthe database load. Here is an article that covers many use cases of NoSQL [14]."
    },
    {
      "page": 31,
      "text": "User\n\nm\n\nWeb browser Mobile app CDN\n\napi.mysite.com\n\nwww.mysite.com\n\nLoad balancer\n\ni—\n\nMessage Queue\n\n1\nWeb servers }\n\n@ NoS@L\n\nCaches\n\nTools Figure 1-23"
    },
    {
      "page": 32,
      "text": "Millions of users and beyond\n\nScaling a system is an iterative process. Iterating on what we have learned in this chapter\ncould get us far. More fine-tuning and new strategies are needed to scale beyond millions of\nusers. For example, you might need to optimize your system and decouple the system to even\nsmaller services. All the techniques learned in this chapter should provide a good foundation\nto tackle new challenges. To conclude this chapter, we provide a summary of how we scale\nour system to support millions of users:\n\n* Keep web tier stateless\n\n* Build redundancy at every tier\n\n* Cache data as much as you can\n\n* Support multiple data centers\n\n* Host static assets in CDN\n\n* Scale your data tier by sharding\n\n* Split tiers into individual services\n\n* Monitor your system and use automation tools\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 33,
      "text": "Reference materials\n[1] Hypertext Transfer Protocol: https://en.wikipedia.org/wiki/Hypertext Transfer Protocol\n\n[2] Should you go Beyond Relational Databases?:\nhttps://blog.teamtreehouse.com/should-you-go-beyond-relational-databases\n\n[3] Replication: https://en.wikipedia.org/wiki/Replication (computing)\n\n[4] Multi-master replication:\nhttps://en. wikipedia.org/wiki/Multi-master_replication\n\n[5] NDB Cluster Replication: Multi-Master and Circular Replication:\nhttps://dev.mysql.com/doc/refman/5.7/en/mysql-cluster-replication-multi-master.htm]\n\n[6] Caching Strategies and How to Choose the Right One:\nhttps://codeahoy.com/2017/08/11/caching-strategies-and-how-to-choose-the-right-one/\n\n[7] R. Nishtala, \"Facebook, Scaling Memcache at,\" 10th USENIX Symposium on Networked\nSystems Design and Implementation (NSDI ’13).\n\n[8] Single point of failure: https://en.wikipedia.org/wiki/Single point of failure\n[9] Amazon CloudFront Dynamic Content Delivery:\n\nhttps://aws.amazon.com/cloudfront/dynamic-content/\n\n[10] Configure Sticky Sessions for Your Classic Load Balancer:\nhttps://docs.aws.amazon.com/elasticloadbalancing/latest/classic/elb-sticky-sessions.html\n\n[11] Active-Active for Multi-Regional Resiliency:\nhttps://netflixtechblog.com/active-active-for-multi-regional-resiliency-c47719f6685b\n[12] Amazon EC2 High Memory Instances:\nhttps://aws.amazon.com/ec2/instance-types/high-memory/\n\n[13] What it takes to run Stack Overflow:\nhttp://nickcraver.com/blog/2013/11/22/what-it-takes-to-run-stack-overflow\n\n[14] What The Heck Are You Actually Using NoSQL For:\n\nhttp://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-\nor.html"
    },
    {
      "page": 34,
      "text": "CHAPTER 2: BACK-OF-THE-ENVELOPE ESTIMATION\n\nIn a system design interview, sometimes you are asked to estimate system capacity or\nperformance requirements using a back-of-the-envelope estimation. According to Jeff Dean,\nGoogle Senior Fellow, “back-of-the-envelope calculations are estimates you create using a\ncombination of thought experiments and common performance numbers to get a good feel for\nwhich designs will meet your requirements” [1].\n\nYou need to have a good sense of scalability basics to effectively carry out back-of-the-\nenvelope estimation. The following concepts should be well understood: power of two [2],\nlatency numbers every programmer should know, and availability numbers."
    },
    {
      "page": 35,
      "text": "Power of two\n\nAlthough data volume can become enormous when dealing with distributed systems,\ncalculation all boils down to the basics. To obtain correct calculations, it is critical to know\n\nthe data volume unit using the power of 2. A byte is a sequence of 8 bits. An ASCII character\nuses one byte of memory (8 bits). Below is a table explaining the data volume unit (Table 2-\n1).\n\nPower Approximate value | Full name Short name\n\n10 1 Thousand 1 Kilobyte 1 KB\n\n20 1 Million 1 Megabyte 1 MB\n\n30 1 Billion 1 Gigabyte 1GB\n\n40 1 Trillion 1 Terabyte 1TB\n\n50 1 Quadrillion 1 Petabyte 1 PB\n\nTable 2-1"
    },
    {
      "page": 36,
      "text": "Latency numbers every programmer should know\n\nDr. Dean from Google reveals the length of typical computer operations in 2010 [1]. Some\nnumbers are outdated as computers become faster and more powerful. However, those\nnumbers should still be able to give us an idea of the fastness and slowness of different\ncomputer operations.\n\nOperation name Time\nL1 cache reference 0.5 ns\nBranch mispredict 5 ns\nL2 cache reference 7ns\nMutex lock/unlock 100 ns\nMain memory reference 100 ns\nCompress 1K bytes with Zippy 10,000 ns = 10 us\nSend 2K bytes over 1 Gbps network 20,000 ns = 20 us\nRead 1 MB sequentially from memory 250,000 ns = 250 us\nRound trip within the same datacenter 500,000 ns = 500 us\nDisk seek 10,000,000 ns = 10 ms\nRead 1 MB sequentially from the network 10,000,000 ns = 10 ms\nRead 1 MB sequentially from disk 30,000,000 ns = 30 ms\nSend packet CA (California) ->Netherlands->CA 150,000,000 ns = 150 ms\nTable 2-2\nNotes\n\nns = nanosecond, pis = microsecond, ms = millisecond\n1 ns = 104-9 seconds\n\n1 ps= 104-6 seconds = 1,000 ns\n\n1 ms = 104-3 seconds = 1,000 jis = 1,000,000 ns"
    },
    {
      "page": 37,
      "text": "A Google software engineer built a tool to visualize Dr. Dean’s numbers. The tool also takes\nthe time factor into consideration. Figures 2-1 shows the visualized latency numbers as of\n\n2020 (source of figures: reference material [3]).\n\nins\n\nL1 cache reference: ins\n\nBranch mispredict: 3ns\n\nL2 cache reference: 4ns\n\nMutex lock/unlock: 17ns\n\n100ns = @\n\nSend 2,000 bytes over commodity\nnetwork: 44ns\n\nSSD random read: 16,000ns =\n16us\n\nRead 1,000,000 bytes sequentially\nfrom memory: 3,000ns = 3yus\n\nRound trip in same datacenter:\n500,000ns = 500us\n\n1,000,000ns = ims = @\n\nFigure 2-1\n\nMain memory reference: 100ns\n\n1,000ns = ips\n\nCompress 1KB wth Zippy: 2,000ns\n= 2us\n\n10,000ns = 10us = @\n\nRead 1,000,000 bytes sequentially\nfrom SSD: 49,000ns ~ 49us\n\nDisk seek: 2,000,000ns = 2ms\n\nRead 1,000,000 bytes sequentially\nfrom disk: 825,000ns = 825us\n\nPacket roundtrip CA to\nNetherlands: 150,000,000ns =\n150ms\n\nBy analyzing the numbers in Figure 2-1, we get the following conclusions:\n\n* Memory is fast but the disk is slow.\n\n+ Avoid disk seeks if possible.\n\n* Simple compression algorithms are fast.\n\n* Compress data before sending it over the internet if possible.\n\n+ Data centers are usually in different regions, and it takes time to send data between them."
    },
    {
      "page": 38,
      "text": "Availability numbers\n\nHigh availability is the ability of a system to be continuously operational for a desirably long\nperiod of time. High availability is measured as a percentage, with 100% means a service that\nhas 0 downtime. Most services fall between 99% and 100%.\n\nA service level agreement (SLA) is a commonly used term for service providers. This is an\nagreement between you (the service provider) and your customer, and this agreement\nformally defines the level of uptime your service will deliver. Cloud providers Amazon [4],\nGoogle [5] and Microsoft [6] set their SLAs at 99.9% or above. Uptime is traditionally\nmeasured in nines. The more the nines, the better. As shown in Table 2-3, the number of\nnines correlate to the expected system downtime.\n\nAvailability % Downtime per day Downtime per year\n99% 14.40 minutes 3.65 days\n\n99.9% 1.44 minutes 8.77 hours\n\n99.99% 8.64 seconds 52.60 minutes\n99.999% 864.00 milliseconds 5.26 minutes\n99.9999% 86.40 milliseconds 31.56 seconds\n\nTable 2-3"
    },
    {
      "page": 39,
      "text": "Example: Estimate Twitter QPS and storage requirements\nPlease note the following numbers are for this exercise only as they are not real numbers\nfrom Twitter.\nAssumptions:\n¢ 300 million monthly active users.\n* 50% of users use Twitter daily.\n* Users post 2 tweets per day on average.\n* 10% of tweets contain media.\n* Data is stored for 5 years.\nEstimations:\nQuery per second (QPS) estimate:\n* Daily active users (DAU) = 300 million * 50% = 150 million\n¢ Tweets QPS = 150 million * 2 tweets / 24 hour / 3600 seconds = ~3500\n* Peek QPS = 2 * QPS = ~7000\nWe will only estimate media storage here.\n« Average tweet size:\n* tweet_id 64 bytes\n* text 140 bytes\n* media 1 MB\n* Media storage: 150 million * 2 * 10% * 1 MB = 30 TB per day\n« 5-year media storage: 30 TB * 365 * 5 = ~55 PB"
    },
    {
      "page": 40,
      "text": "Tips\nBack-of-the-envelope estimation is all about the process. Solving the problem is more\nimportant than obtaining results. Interviewers may test your problem-solving skills. Here are\na few tips to follow:\n* Rounding and Approximation. It is difficult to perform complicated math operations\nduring the interview. For example, what is the result of “99987 / 9.1”? There is no need to\nspend valuable time to solve complicated math problems. Precision is not expected. Use\nround numbers and approximation to your advantage. The division question can be\nsimplified as follows: “100,000 / 10”.\n* Write down your assumptions. It is a good idea to write down your assumptions to be\nreferenced later.\n* Label your units. When you write down “5”, does it mean 5 KB or 5 MB? You might\nconfuse yourself with this. Write down the units because “S MB” helps to remove\nambiguity.\n* Commonly asked back-of-the-envelope estimations: QPS, peak QPS, storage, cache,\nnumber of servers, etc. You can practice these calculations when preparing for an\ninterview. Practice makes perfect.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 41,
      "text": "Reference materials\n[1] J. Dean.Google Pro Tip: Use Back-Of-The-Envelope-Calculations To Choose The Best\nDesign:\n\nhttp://highscalability.com/blog/2011/1/26/google-pro-tip-use-back-of-the-envelope-\ncalculations-to-choo.html\n\n[2] System design primer: https://github.com/donnemartin/system-design-primer\n\n[3] Latency Numbers Every Programmer Should Know:\nhttps://colin-scott.github.io/personal_website/research/interactive_latency.html\n\n[4] Amazon Compute Service Level Agreement:\nhttps://aws.amazon.com/compute/sla/\n\n[5] Compute Engine Service Level Agreement (SLA):\nhttps://cloud.google.com/compute/sla\n\n[6] SLA summary for Azure services: https://azure.microsoft.com/en-\nus/support/legal/sla/summary/"
    },
    {
      "page": 42,
      "text": "CHAPTER 3: A FRAMEWORK FOR SYSTEM DESIGN\nINTERVIEWS\n\nYou have just landed a coveted on-site interview at your dream company. The hiring\ncoordinator sends you a schedule for that day. Scanning down the list, you feel pretty good\nabout it until your eyes land on this interview session - System Design Interview.\n\nSystem design interviews are often intimidating. It could be as vague as “designing a well-\nknown product X?”. The questions are ambiguous and seem unreasonably broad. Your\nweariness is understandable. After all, how could anyone design a popular product in an hour\nthat has taken hundreds if not thousands of engineers to build?\n\nThe good news is that no one expects you to. Real-world system design is extremely\ncomplicated. For example, Google search is deceptively simple; however, the amount of\ntechnology that underpins that simplicity is truly astonishing. If no one expects you to design\na real-world system in an hour, what is the benefit of a system design interview?\n\nThe system design interview simulates real-life problem solving where two co-workers\ncollaborate on an ambiguous problem and come up with a solution that meets their goals. The\nproblem is open-ended, and there is no perfect answer. The final design is less important\ncompared to the work you put in the design process. This allows you to demonstrate your\ndesign skill, defend your design choices, and respond to feedback in a constructive manner.\n\nLet us flip the table and consider what goes through the interviewer’s head as she walks into\nthe conference room to meet you. The primary goal of the interviewer is to accurately assess\nyour abilities. The last thing she wants is to give an inconclusive evaluation because the\nsession has gone poorly and there are not enough signals. What is an interviewer looking for\nin a system design interview?\n\nMany think that system design interview is all about a person's technical design skills. It is\nmuch more than that. An effective system design interview gives strong signals about a\nperson's ability to collaborate, to work under pressure, and to resolve ambiguity\nconstructively. The ability to ask good questions is also an essential skill, and many\ninterviewers specifically look for this skill.\n\nA good interviewer also looks for red flags. Over-engineering is a real disease of many\nengineers as they delight in design purity and ignore tradeoffs. They are often unaware of the\ncompounding costs of over-engineered systems, and many companies pay a high price for\nthat ignorance. You certainly do not want to demonstrate this tendency in a system design\ninterview. Other red flags include narrow mindedness, stubbornness, etc.\n\nIn this chapter, we will go over some useful tips and introduce a simple and effective\nframework to solve system design interview problems."
    },
    {
      "page": 43,
      "text": "A 4-step process for effective system design interview\n\nEvery system design interview is different. A great system design interview is open-ended\nand there is no one-size-fits-all solution. However, there are steps and common ground to\ncover in every system design interview.\n\nStep 1 - Understand the problem and establish design scope\n\n\"Why did the tiger roar?\"\n\nA hand shot up in the back of the class.\n\"Yes, Jimmy?\", the teacher responded.\n\"Because he was HUNGRY\".\n\n\"Very good Jimmy.\"\n\nThroughout his childhood, Jimmy has always been the first to answer questions in the class.\nWhenever the teacher asks a question, there is always a kid in the classroom who loves to\ntake a crack at the question, no matter if he knows the answer or not. That is Jimmy.\n\nJimmy is an ace student. He takes pride in knowing all the answers fast. In exams, he is\nusually the first person to finish the questions. He is a teacher's top choice for any academic\ncompetition.\n\nDON'T be like Jimmy.\n\nIn a system design interview, giving out an answer quickly without thinking gives you no\nbonus points. Answering without a thorough understanding of the requirements is a huge red\nflag as the interview is not a trivia contest. There is no right answer.\n\nSo, do not jump right in to give a solution. Slow down. Think deeply and ask questions to\nclarify requirements and assumptions. This is extremely important.\n\nAs an engineer, we like to solve hard problems and jump into the final design; however, this\napproach is likely to lead you to design the wrong system. One of the most important skills as\nan engineer is to ask the right questions, make the proper assumptions, and gather all the\ninformation needed to build a system. So, do not be afraid to ask questions.\n\nWhen you ask a question, the interviewer either answers your question directly or asks you to\nmake your assumptions. If the latter happens, write down your assumptions on the\nwhiteboard or paper. You might need them later.\n\nWhat kind of questions to ask? Ask questions to understand the exact requirements. Here is a\nlist of questions to help you get started:\n\n* What specific features are we going to build?\n\n* How many users does the product have?\n\n* How fast does the company anticipate to scale up? What are the anticipated scales in 3\nmonths, 6 months, and a year?\n\n* What is the company’s technology stack? What existing services you might leverage to\nsimplify the design?\nExample\n\nIf you are asked to design a news feed system, you want to ask questions that help you clarify\nthe requirements. The conversation between you and the interviewer might look like this:\n\nCandidate: Is this a mobile app? Or a web app? Or both?"
    },
    {
      "page": 44,
      "text": "Interviewer: Both.\n\nCandidate: What are the most important features for the product?\nInterviewer: Ability to make a post and see friends’ news feed.\n\nCandidate: Is the news feed sorted in reverse chronological order or a particular order? The\nparticular order means each post is given a different weight. For instance, posts from your\nclose friends are more important than posts from a group.\n\nInterviewer: To keep things simple, let us assume the feed is sorted by reverse chronological\norder.\n\nCandidate: How many friends can a user have?\nInterviewer: 5000\n\nCandidate: What is the traffic volume?\nInterviewer: 10 million daily active users (DAU)\n\nCandidate: Can feed contain images, videos, or just text?\nInterviewer: It can contain media files, including both images and videos.\n\nAbove are some sample questions that you can ask your interviewer. It is important to\nunderstand the requirements and clarify ambiguities\n\nStep 2 - Propose high-level design and get buy-in\nIn this step, we aim to develop a high-level design and reach an agreement with the\ninterviewer on the design. It is a great idea to collaborate with the interviewer during the\nprocess.\n* Come up with an initial blueprint for the design. Ask for feedback. Treat your\ninterviewer as a teammate and work together. Many good interviewers love to talk and get\ninvolved.\n* Draw box diagrams with key components on the whiteboard or paper. This might include\nclients (mobile/web), APIs, web servers, data stores, cache, CDN, message queue, etc.\n* Do back-of-the-envelope calculations to evaluate if your blueprint fits the scale\nconstraints. Think out loud. Communicate with your interviewer if back-of-the-envelope is\nnecessary before diving into it.\n\nIf possible, go through a few concrete use cases. This will help you frame the high-level\ndesign. It is also likely that the use cases would help you discover edge cases you have not\nyet considered.\n\nShould we include API endpoints and database schema here? This depends on the problem.\nFor large design problems like “Design Google search engine”, this is a bit of too low level.\nFor a problem like designing the backend for a multi-player poker game, this is a fair game.\nCommunicate with your interviewer.\n\nExample\n\nLet us use “Design a news feed system” to demonstrate how to approach the high-level\ndesign. Here you are not required to understand how the system actually works. All the\ndetails will be explained in Chapter 11.\n\nAt the high level, the design is divided into two flows: feed publishing and news feed\nbuilding.\n¢ Feed publishing: when a user publishes a post, corresponding data is written into\ncache/database, and the post will be populated into friends’ news feed."
    },
    {
      "page": 45,
      "text": "* Newsfeed building: the news feed is built by aggregating friends’ posts in a reverse\nchronological order.\n\nFigure 3-1 and Figure 3-2 present high-level designs for feed publishing and news feed\nbuilding flows, respectively.\n\n‘a>\n\nUser oO A\n\nWeb browser Mobile app\n\nvi/me/feed?\ncontent=Hello&\nauth_token={auth_token}\n\nLoad balancer\n\ne my\n4‘ 1\na —— | 1\n' 1\n1 1\ni 1\n' 1\n1 1\n, Web servers ,\n| Post Service Fanout Service Notification\nService\n\nNews Feed\nCache\n\nPost Cache\n\nPost DB\n\nFigure 3-1"
    },
    {
      "page": 46,
      "text": "User = ‘7\n\nWeb browser Mobile app\n\nv1/me/feed\n\nLoad balancer\n\nNews Feed\nCache\n\nFigure 3-2\n\nStep 3 - Design deep dive\nAt this step, you and your interviewer should have already achieved the following objectives:\n\n* Agreed on the overall goals and feature scope\n\n* Sketched out a high-level blueprint for the overall design\n\n* Obtained feedback from your interviewer on the high-level design\n\n* Had some initial ideas about areas to focus on in deep dive based on her feedback\nYou shall work with the interviewer to identify and prioritize components in the architecture.\nIt is worth stressing that every interview is different. Sometimes, the interviewer may give off\nhints that she likes focusing on high-level design. Sometimes, for a senior candidate\ninterview, the discussion could be on the system performance characteristics, likely focusing\non the bottlenecks and resource estimations. In most cases, the interviewer may want you to\ndig into details of some system components. For URL shortener, it is interesting to dive into\n\nthe hash function design that converts a long URL to a short one. For a chat system, how to\nreduce latency and how to support online/offline status are two interesting topics.\n\nTime management is essential as it is easy to get carried away with minute details that do not\ndemonstrate your abilities. You must be armed with signals to show your interviewer. Try not"
    },
    {
      "page": 47,
      "text": "to get into unnecessary details. For example, talking about the EdgeRank algorithm of\nFacebook feed ranking in detail is not ideal during a system design interview as this takes\nmuch precious time and does not prove your ability in designing a scalable system.\nExample\n\nAt this point, we have discussed the high-level design for a news feed system, and the\ninterviewer is happy with your proposal. Next, we will investigate two of the most important\nuse cases:\n\n1. Feed publishing\n2. News feed retrieval\n\nFigure 3-3 and Figure 3-4 show the detailed design for the two use cases, which will be\nexplained in detail in Chapter 11."
    },
    {
      "page": 48,
      "text": "me) User = 5\n\nSO, Web browser Mobile app\n\nauth_token={auth_token}\n\nvifme/feed?\ncontent=Hello&\n\nAuthentication Notification\nRate Limiting Service\n\nGraph DB\n\nUser Cache User DB\n\n° 2s\n1\n\nFanout Workers\n. ra\n\nNews Feed\nCache\n\nFigure 3-3"
    },
    {
      "page": 49,
      "text": "SZ, Web browser Mobile app\n\nCDN\n\nlvilme/feed\n\n©\na —|—| ‘\ni Authentication ,\n, A G Rate Limiting i\n1 Web servers i\n\nUser Cache\n\nNews Feed\nCache Post Cache Post DB\n\nFigure 3-4\nStep 4 - Wrap up\nIn this final step, the interviewer might ask you a few follow-up questions or give you the\nfreedom to discuss other additional points. Here are a few directions to follow:\n* The interviewer might want you to identify the system bottlenecks and discuss potential\nimprovements. Never say your design is perfect and nothing can be improved. There is\nalways something to improve upon. This is a great opportunity to show your critical\nthinking and leave a good final impression.\n+ It could be useful to give the interviewer a recap of your design. This is particularly\nimportant if you suggested a few solutions. Refreshing your interviewer’s memory can be\nhelpful after a long session.\n+ Error cases (server failure, network loss, etc.) are interesting to talk about.\n* Operation issues are worth mentioning. How do you monitor metrics and error logs?\nHow to roll out the system?\n* How to handle the next scale curve is also an interesting topic. For example, if your\ncurrent design supports 1 million users, what changes do you need to make to support 10\nmillion users?\n\n+ Propose other refinements you need if you had more time."
    },
    {
      "page": 50,
      "text": "To wrap up, we summarize a list of the Dos and Don’ts.\n\nDos\n* Always ask for clarification. Do not assume your assumption is correct.\n* Understand the requirements of the problem.\n\n¢ There is neither the right answer nor the best answer. A solution designed to solve the\nproblems of a young startup is different from that of an established company with millions\nof users. Make sure you understand the requirements.\n\nLet the interviewer know what you are thinking. Communicate with your interview.\n* Suggest multiple approaches if possible.\n\n* Once you agree with your interviewer on the blueprint, go into details on each\ncomponent. Design the most critical components first.\n\n* Bounce ideas off the interviewer. A good interviewer works with you as a teammate.\n« Never give up.\nDon’ts\n* Don't be unprepared for typical interview questions.\n* Don’t jump into a solution without clarifying the requirements and assumptions.\n\n* Don’t go into too much detail on a single component in the beginning. Give the high-\nlevel design first then drills down.\n\n+ If you get stuck, don't hesitate to ask for hints.\n« Again, communicate. Don't think in silence.\n* Don’t think your interview is done once you give the design. You are not done until your\ninterviewer says you are done. Ask for feedback early and often.\nTime allocation on each step\n\nSystem design interview questions are usually very broad, and 45 minutes or an hour is not\nenough to cover the entire design. Time management is essential. How much time should you\nspend on each step? The following is a very rough guide on distributing your time in a 45-\nminute interview session. Please remember this is a rough estimate, and the actual time\ndistribution depends on the scope of the problem and the requirements from the interviewer.\n\nStep 1 Understand the problem and establish design scope: 3 - 10 minutes\nStep 2 Propose high-level design and get buy-in: 10 - 15 minutes\n\nStep 3 Design deep dive: 10 - 25 minutes\n\nStep 4 Wrap: 3 - 5 minutes"
    },
    {
      "page": 51,
      "text": "CHAPTER 4: DESIGN A RATE LIMITER\n\nIn a network system, a rate limiter is used to control the rate of traffic sent by a client or a\nservice. In the HTTP world, a rate limiter limits the number of client requests allowed to be\nsent over a specified period. If the API request count exceeds the threshold defined by the\nrate limiter, all the excess calls are blocked. Here are a few examples:\n\n« A user can write no more than 2 posts per second.\n* You can create a maximum of 10 accounts per day from the same IP address.\n* You can claim rewards no more than 5 times per week from the same device.\n\nIn this chapter, you are asked to design a rate limiter. Before starting the design, we first look\nat the benefits of using an API rate limiter:\n+ Prevent resource starvation caused by Denial of Service (DoS) attack [1]. Almost all\nAPIs published by large tech companies enforce some form of rate limiting. For example,\nTwitter limits the number of tweets to 300 per 3 hours [2]. Google docs APIs have the\nfollowing default limit: 300 per user per 60 seconds for read requests [3]. A rate limiter\nprevents DoS attacks, either intentional or unintentional, by blocking the excess calls.\n\n* Reduce cost. Limiting excess requests means fewer servers and allocating more\nresources to high priority APIs. Rate limiting is extremely important for companies that\nuse paid third party APIs. For example, you are charged on a per-call basis for the\nfollowing external APIs: check credit, make a payment, retrieve health records, etc.\nLimiting the number of calls is essential to reduce costs.\n\n* Prevent servers from being overloaded. To reduce server load, a rate limiter is used to\nfilter out excess requests caused by bots or users’ misbehavior."
    },
    {
      "page": 52,
      "text": "Step 1 - Understand the problem and establish design scope\n\nRate limiting can be implemented using different algorithms, each with its pros and cons. The\ninteractions between an interviewer and a candidate help to clarify the type of rate limiters we\nare trying to build.\n\nCandidate: What kind of rate limiter are we going to design? Is it a client-side rate limiter or\n\nserver-side API rate limiter?\n\nInterviewer: Great question. We focus on the server-side API rate limiter.\n\nCandidate: Does the rate limiter throttle API requests based on IP, the user ID, or other\nproperties?\n\nInterviewer: The rate limiter should be flexible enough to support different sets of throttle\nrules.\n\nCandidate: What is the scale of the system? Is it built for a startup or a big company with a\nlarge user base?\nInterviewer: The system must be able to handle a large number of requests.\n\nCandidate: Will the system work in a distributed environment?\nInterviewer: Yes.\n\nCandidate: Is the rate limiter a separate service or should it be implemented in application\ncode?\nInterviewer: It is a design decision up to you.\n\nCandidate: Do we need to inform users who are throttled?\nInterviewer: Yes.\nRequirements\nHere is a summary of the requirements for the system:\n* Accurately limit excessive requests.\n* Low latency. The rate limiter should not slow down HTTP response time.\n* Use as little memory as possible.\n* Distributed rate limiting. The rate limiter can be shared across multiple servers or\nprocesses.\n* Exception handling. Show clear exceptions to users when their requests are throttled.\n\n* High fault tolerance. If there are any problems with the rate limiter (for example, a cache\nserver goes offline), it does not affect the entire system."
    },
    {
      "page": 53,
      "text": "Step 2 - Propose high-level design and get buy-in\n\nLet us keep things simple and use a basic client and server model for communication.\n\nWhere to put the rate limiter?\n\nIntuitively, you can implement a rate limiter at either the client or server-side.\n* Client-side implementation. Generally speaking, client is an unreliable place to enforce\nrate limiting because client requests can easily be forged by malicious actors. Moreover,\nwe might not have control over the client implementation.\n+ Server-side implementation. Figure 4-1 shows a rate limiter that is placed on the server-\nside.\n\nHTTP request\n\nFigure 4-1\n\nBesides the client and server-side implementations, there is an alternative way. Instead of\nputting a rate limiter at the API servers, we create a rate limiter middleware, which throttles\nrequests to your APIs as shown in Figure 4-2.\n\nClient PLE\nmo ,\n\ny API Servers\n\nRate limiter\nFigure 4-2\n\nLet us use an example in Figure 4-3 to illustrate how rate limiting works in this design.\nAssume our API allows 2 requests per second, and a client sends 3 requests to the server\nwithin a second. The first two requests are routed to API servers. However, the rate limiter\nmiddleware throttles the third request and returns a HTTP status code 429. The HTTP 429\nresponse status code indicates a user has sent too many requests."
    },
    {
      "page": 54,
      "text": "Gi\n~ ae\n1\nes: ead | API Servers\n429: Too many requests\n\norent 7\n\neae\n\nt {\n\nRate limiter\nFigure 4-3\n\nCloud microservices [4] have become widely popular and rate limiting is usually\nimplemented within a component called API gateway. API gateway is a fully managed\nservice that supports rate limiting, SSL termination, authentication, IP whitelisting, servicing\nstatic content, etc. For now, we only need to know that the API gateway is a middleware that\nsupports rate limiting.\n\nWhile designing a rate limiter, an important question to ask ourselves is: where should the\nrater limiter be implemented, on the server-side or in a gateway? There is no absolute answer.\nIt depends on your company’s current technology stack, engineering resources, priorities,\ngoals, etc. Here are a few general guidelines:\n\n¢ Evaluate your current technology stack, such as programming language, cache service,\netc. Make sure your current programming language is efficient to implement rate limiting\non the server-side.\n\n+ Identify the rate limiting algorithm that fits your business needs. When you implement\neverything on the server-side, you have full control of the algorithm. However, your\nchoice might be limited if you use a third-party gateway.\n\n* If you have already used microservice architecture and included an API gateway in the\ndesign to perform authentication, IP whitelisting, etc., you may add a rate limiter to the\nAPI gateway.\n\n* Building your own rate limiting service takes time. If you do not have enough\nengineering resources to implement a rate limiter, a commercial API gateway is a better\noption.\n\nAlgorithms for rate limiting\n\nRate limiting can be implemented using different algorithms, and each of them has distinct\npros and cons. Even though this chapter does not focus on algorithms, understanding them at\nhigh-level helps to choose the right algorithm or combination of algorithms to fit our use\ncases. Here is a list of popular algorithms:\n\n* Token bucket\n* Leaking bucket\n+ Fixed window counter\n* Sliding window log\n* Sliding window counter\nToken bucket algorithm\nThe token bucket algorithm is widely used for rate limiting. It is simple, well understood and"
    },
    {
      "page": 55,
      "text": "commonly used by internet companies. Both Amazon [5] and Stripe [6] use this algorithm to\nthrottle their API requests.\n\nThe token bucket algorithm work as follows:\n\n+ A token bucket is a container that has pre-defined capacity. Tokens are put in the bucket\nat preset rates periodically. Once the bucket is full, no more tokens are added. As shown in\nFigure 4-4, the token bucket capacity is 4. The refiller puts 2 tokens into the bucket every\nsecond. Once the bucket is full, extra tokens will overflow.\n\n@\n@® refiller\n\noverflow\n\nFigure 4-4\n* Each request consumes one token. When a request arrives, we check if there are enough\ntokens in the bucket. Figure 4-5 explains how it works.\n\n+ If there are enough tokens, we take one token out for each request, and the request\ngoes through.\n\n+ If there are not enough tokens, the request is dropped."
    },
    {
      "page": 56,
      "text": "' : : forward requests ' !\n: > <enough tokens? : @\n—_ '@|\n\nrequests nnn\n\nno,\n\nFigure 4-5\n\nFigure 4-6 illustrates how token consumption, refill, and rate limiting logic work. In this\nexample, the token bucket size is 4, and the refill rate is 4 per 1 minute."
    },
    {
      "page": 57,
      "text": "ZS —request @ |v.\nrequest >\nrequest @ ¥v >\nrequest Se v >\n1:00:00 1:00:05\n\n¢ Start with 4 tokens e Start with 3 tokens\ne The request will go through e All three requests will go through\n¢ 1 token is consumed e 3 tokens are consumed\n\n©) ®\n\n©\n—request | @\n@\n@\n1:00:20 1:01:00\n¢ Start with 0 token ¢ 4 tokens are refilled at\n¢ The request will be dropped. 1 minute interval\n\nFigure 4-6\nThe token bucket algorithm takes two parameters:\n* Bucket size: the maximum number of tokens allowed in the bucket\n* Refill rate: number of tokens put into the bucket every second\n\nHow many buckets do we need? This varies, and it depends on the rate-limiting rules. Here\nare a few examples.\n\n+ It is usually necessary to have different buckets for different API endpoints. For instance,\nif a user is allowed to make 1 post per second, add 150 friends per day, and like 5 posts per\nsecond, 3 buckets are required for each user.\n\n+ If we need to throttle requests based on IP addresses, each IP address requires a bucket.\n\n+ If the system allows a maximum of 10,000 requests per second, it makes sense to have a\nglobal bucket shared by all requests.\n\nPros:\n¢ The algorithm is easy to implement.\n* Memory efficient.\n\n* Token bucket allows a burst of traffic for short periods. A request can go through as long\nas there are tokens left.\n\nCons:\n\n* Two parameters in the algorithm are bucket size and token refill rate. However, it might"
    },
    {
      "page": 58,
      "text": "be challenging to tune them properly.\n\nLeaking bucket algorithm\n\nThe leaking bucket algorithm is similar to the token bucket except that requests are processed\nat a fixed rate. It is usually implemented with a first-in-first-out (FIFO) queue. The algorithm\nworks as follows:\n\n* When a request arrives, the system checks if the queue is full. If it is not full, the request\nis added to the queue.\n\n* Otherwise, the request is dropped.\n* Requests are pulled from the queue and processed at regular intervals.\n\nFigure 4-7 explains how the algorithm works.\n\nprocessed at fixed rate\n\nqueue ‘\" |\n\nrequests requests go\nyes, through\n\ndrop requests\n\nFigure 4-7\nLeaking bucket algorithm takes the following two parameters:\n\n* Bucket size: it is equal to the queue size. The queue holds the requests to be processed at\na fixed rate.\n\n* Outflow rate: it defines how many requests can be processed at a fixed rate, usually in\nseconds.\n\nShopify, an ecommerce company, uses leaky buckets for rate-limiting [7].\n\nPros:\n* Memory efficient given the limited queue size.\n* Requests are processed at a fixed rate therefore it is suitable for use cases that a stable\noutflow rate is needed.\n\nCons:\n\n+ A burst of traffic fills up the queue with old requests, and if they are not processed in\ntime, recent requests will be rate limited.\n\n¢ There are two parameters in the algorithm. It might not be easy to tune them properly.\n\nFixed window counter algorithm\nFixed window counter algorithm works as follows:\n\n* The algorithm divides the timeline into fix-sized time windows and assign a counter for\neach window.\n\n« Each request increments the counter by one.\n\n* Once the counter reaches the pre-defined threshold, new requests are dropped until a new\ntime window starts."
    },
    {
      "page": 59,
      "text": "Let us use a concrete example to see how it works. In Figure 4-8, the time unit is 1 second\nand the system allows a maximum of 3 requests per second. In each second window, if more\nthan 3 requests are received, extra requests are dropped as shown in Figure 4-8.\n\nA\nrate limited request\ni successful request\n£\na\nO)\n]\n3\n|x imi |\n> Zl hrTlhlUC«dH\n7; on\n1:00:00 1:00:01 1:00:02 1:00:03 1:00:04\n>\nTime\nFigure 4-8\n\nA major problem with this algorithm is that a burst of traffic at the edges of time windows\ncould cause more requests than allowed quota to go through. Consider the following case:\n\n< ——10 requests———_»\n\nre\n\n2:00:00\n\n2:01:00 2:02:00\n\nFigure 4-9\n\nIn Figure 4-9, the system allows a maximum of 5 requests per minute, and the available quota\nresets at the human-friendly round minute. As seen, there are five requests between 2:00:00\nand 2:01:00 and five more requests between 2:01:00 and 2:02:00. For the one-minute window\nbetween 2:00:30 and 2:01:30, 10 requests go through. That is twice as many as allowed\n\nrequests.\n\nPros:\n* Memory efficient."
    },
    {
      "page": 60,
      "text": "Easy to understand.\n* Resetting available quota at the end of a unit time window fits certain use cases.\n\nCons:\n* Spike in traffic at the edges of a window could cause more requests than the allowed\nquota to go through.\n\nSliding window log algorithm\n\nAs discussed previously, the fixed window counter algorithm has a major issue: it allows\n\nmore requests to go through at the edges of a window. The sliding window log algorithm\nfixes the issue. It works as follows:\n\n* The algorithm keeps track of request timestamps. Timestamp data is usually kept in\ncache, such as sorted sets of Redis [8].\n\n« When a new request comes in, remove all the outdated timestamps. Outdated timestamps\nare defined as those older than the start of the current time window.\n\n« Add timestamp of the new request to the log.\n\n* If the log size is the same or lower than the allowed count, a request is accepted.\nOtherwise, it is rejected.\n\nWe explain the algorithm with an example as revealed in Figure 4-10.\n\nAllow 2 requests per minute\n\n> 1:00:30\n\nFigure 4-10\nIn this example, the rate limiter allows 2 requests per minute. Usually, Linux timestamps are\nstored in the log. However, human-readable representation of time is used in our example for\nbetter readability.\n* The log is empty when a new request arrives at 1:00:01. Thus, the request is allowed."
    },
    {
      "page": 61,
      "text": "+ A new request arrives at 1:00:30, the timestamp 1:00:30 is inserted into the log. After the\ninsertion, the log size is 2, not larger than the allowed count. Thus, the request is allowed.\n+ A new request arrives at 1:00:50, and the timestamp is inserted into the log. After the\ninsertion, the log size is 3, larger than the allowed size 2. Therefore, this request is rejected\neven though the timestamp remains in the log.\n\n« A new request arrives at 1:01:40. Requests in the range [1:00:40,1:01:40) are within the\nlatest time frame, but requests sent before 1:00:40 are outdated. Two outdated timestamps,\n1:00:01 and 1:00:30, are removed from the log. After the remove operation, the log size\nbecomes 2; therefore, the request is accepted.\n\nPros:\n+ Rate limiting implemented by this algorithm is very accurate. In any rolling window,\nrequests will not exceed the rate limit.\n\nCons:\n* The algorithm consumes a lot of memory because even if a request is rejected, its\ntimestamp might still be stored in memory.\n\nSliding window counter algorithm\n\nThe sliding window counter algorithm is a hybrid approach that combines the fixed window\ncounter and sliding window log. The algorithm can be implemented by two different\napproaches. We will explain one implementation in this section and provide reference for the\nother implementation at the end of the section. Figure 4-11 illustrates how this algorithm\nworks.\n\nA Current time\n\nRate limit: 5 requests/min\n\nNumber of requests\n\n<<previous minute—><—\\current minute—>\n\nTime Figure 4-11\n\nAssume the rate limiter allows a maximum of 7 requests per minute, and there are 5 requests\nin the previous minute and 3 in the current minute. For a new request that arrives at a 30%\nposition in the current minute, the number of requests in the rolling window is calculated\nusing the following formula:\n\n* Requests in current window + requests in the previous window * overlap percentage of\nthe rolling window and previous window"
    },
    {
      "page": 62,
      "text": "* Using this formula, we get 3 + 5 * 0.7% = 6.5 request. Depending on the use case, the\nnumber can either be rounded up or down. In our example, it is rounded down to 6.\n\nSince the rate limiter allows a maximum of 7 requests per minute, the current request can go\nthrough. However, the limit will be reached after receiving one more request.\n\nDue to the space limitation, we will not discuss the other implementation here. Interested\nreaders should refer to the reference material [9]. This algorithm is not perfect. It has pros and\ncons.\n\nPros\n+ It smooths out spikes in traffic because the rate is based on the average rate of the\nprevious window.\n* Memory efficient.\n\nCons\n* It only works for not-so-strict look back window. It is an approximation of the actual rate\nbecause it assumes requests in the previous window are evenly distributed. However, this\nproblem may not be as bad as it seems. According to experiments done by Cloudflare [10],\nonly 0.003% of requests are wrongly allowed or rate limited among 400 million requests.\n\nHigh-level architecture\n\nThe basic idea of rate limiting algorithms is simple. At the high-level, we need a counter to\nkeep track of how many requests are sent from the same user, IP address, etc. If the counter is\nlarger than the limit, the request is disallowed.\n\nWhere shall we store counters? Using the database is not a good idea due to slowness of disk\naccess. In-memory cache is chosen because it is fast and supports time-based expiration\nstrategy. For instance, Redis [11] is a popular option to implement rate limiting. It is an in-\nmemory store that offers two commands: INCR and EXPIRE.\n\n* INCR: It increases the stored counter by 1.\n\n* EXPIRE: It sets a timeout for the counter. If the timeout expires, the counter is\n\nautomatically deleted.\n\nFigure 4-12 shows the high-level architecture for rate limiting, and this works as follows:\n\n‘oe |\nRate limiter 1 eliapal ;\nmiddleware : 1\nAPI Servers !\n\n’\n\n1\n.\n\nRedis\n\nFigure 4-12\n\n« The client sends a request to rate limiting middleware.\n* Rate limiting middleware fetches the counter from the corresponding bucket in Redis and"
    },
    {
      "page": 63,
      "text": "checks if the limit is reached or not.\n+ If the limit is reached, the request is rejected.\n\n« If the limit is not reached, the request is sent to API servers. Meanwhile, the system\nincrements the counter and saves it back to Redis."
    },
    {
      "page": 64,
      "text": "Step 3 - Design deep dive\n\nThe high-level design in Figure 4-12 does not answer the following questions:\n* How are rate limiting rules created? Where are the rules stored?\n* How to handle requests that are rate limited?\n\nIn this section, we will first answer the questions regarding rate limiting rules and then go\nover the strategies to handle rate-limited requests. Finally, we will discuss rate limiting in\ndistributed environment, a detailed design, performance optimization and monitoring.\n\nRate limiting rules\n\nLyft open-sourced their rate-limiting component [12]. We will peek inside of the component\nand look at some examples of rate limiting rules:\n\ndomain: messaging\ndescriptors:\n\n- key: message_type\nValue: marketing\nrate_limit:\n\nunit: day\nrequests_per_unit: 5\n\nIn the above example, the system is configured to allow a maximum of 5 marketing messages\nper day. Here is another example:\n\ndomain: auth\ndescriptors:\n\n- key: auth_type\nValue: login\nrate_limit:\n\nunit: minute\nrequests_per_unit: 5\n\nThis rule shows that clients are not allowed to login more than 5 times in 1 minute. Rules are\ngenerally written in configuration files and saved on disk.\nExceeding the rate limit\n\nIn case a request is rate limited, APIs return a HTTP response code 429 (too many requests)\nto the client. Depending on the use cases, we may enqueue the rate-limited requests to be\nprocessed later. For example, if some orders are rate limited due to system overload, we may\nkeep those orders to be processed later.\n\nRate limiter headers\n\nHow does a client know whether it is being throttled? And how does a client know the\nnumber of allowed remaining requests before being throttled? The answer lies in HTTP\nresponse headers. The rate limiter returns the following HTTP headers to clients:\n\nX-Ratelimit-Remaining: The remaining number of allowed requests within the window.\nX-Ratelimit-Limit: It indicates how many calls the client can make per time window.\n\nX-Ratelimit-Retry-After: The number of seconds to wait until you can make a request again\nwithout being throttled.\n\nWhen a user has sent too many requests, a 429 too many requests error and X-Ratelimit-"
    },
    {
      "page": 65,
      "text": "Retry-After header are returned to the client.\n\nDetailed design\nFigure 4-13 presents a detailed design of the system.\n\nos :\n1\n'\n\n' Workers 1\n\nen\n\nCached rules\nae\n\n1\n\n: ae\n\n| API Servers J\n\n'\naaa:\n\nRate limiter success\n\nmiddleware\n\ned\n\nan A\n\nrate limited\n\nRedis\noption 1 )\n429: too many requests Sy 74\na Request dropped\npy.\na ton\na .\nBN\nSei\nMessage queue\nFigure 4-13\n\n+ Rules are stored on the disk. Workers frequently pull rules from the disk and store them\nin the cache.\n\n« When a client sends a request to the server, the request is sent to the rate limiter\nmiddleware first.\n\n+ Rate limiter middleware loads rules from the cache. It fetches counters and last request\ntimestamp from Redis cache. Based on the response, the rate limiter decides:\n\n+ if the request is not rate limited, it is forwarded to API servers.\n\n+ if the request is rate limited, the rate limiter returns 429 too many requests error to\nthe client. In the meantime, the request is either dropped or forwarded to the queue."
    },
    {
      "page": 66,
      "text": "Rate limiter in a distributed environment\n\nBuilding a rate limiter that works in a single server environment is not difficult. However,\nscaling the system to support multiple servers and concurrent threads is a different story.\nThere are two challenges:\n\n* Race condition\n* Synchronization issue\nRace condition\nAs discussed earlier, rate limiter works as follows at the high-level:\n* Read the counter value from Redis.\n* Check if ( counter + 1) exceeds the threshold.\n¢ If not, increment the counter value by 1 in Redis.\n\nRace conditions can happen in a highly concurrent environment as shown in Figure 4-14.\n\nOriginal counter value: 3\n\nrequest 1\n\nread_counter\n\ncounter: 3\n\nrequest 2\n\nread_counter\n\ncounter: 3\n\ncounter: 4\n\ncheck_and_increment\nIcheck_and_increment} counter: 4 Counter should be: 5\n\nFigure 4-14\n\nAssume the counter value in Redis is 3. If two requests concurrently read the counter value\nbefore either of them writes the value back, each will increment the counter by one and write\nit back without checking the other thread. Both requests (threads) believe they have the\ncorrect counter value 4. However, the correct counter value should be 5.\n\nLocks are the most obvious solution for solving race condition. However, locks will\nsignificantly slow down the system. Two strategies are commonly used to solve the problem:\nLua script [13] and sorted sets data structure in Redis [8]. For readers interested in these\nstrategies, refer to the corresponding reference materials [8] [13].\n\nSynchronization issue\n\nSynchronization is another important factor to consider in a distributed environment. To\nsupport millions of users, one rate limiter server might not be enough to handle the traffic.\nWhen multiple rate limiter servers are used, synchronization is required. For example, on the\nleft side of Figure 4-15, client 1 sends requests to rate limiter 1, and client 2 sends requests to"
    },
    {
      "page": 67,
      "text": "rate limiter 2. As the web tier is stateless, clients can send requests to a different rate limiter\nas shown on the right side of Figure 4-15. If no synchronization happens, rate limiter 1 does\nnot contain any data about client 2. Thus, the rate limiter cannot work properly.\n\nFigure 4-15\n\nOne possible solution is to use sticky sessions that allow a client to send traffic to the same\nrate limiter. This solution is not advisable because it is neither scalable nor flexible. A better\napproach is to use centralized data stores like Redis. The design is shown in Figure 4-16.\n\nFigure 4-16\n\nPerformance optimization\n\nPerformance optimization is a common topic in system design interviews. We will cover two\nareas to improve.\n\nFirst, multi-data center setup is crucial for a rate limiter because latency is high for users\nlocated far away from the data center. Most cloud service providers build many edge server\nlocations around the world. For example, as of 5/20 2020, Cloudflare has 194 geographically\ndistributed edge servers [14]. Traffic is automatically routed to the closest edge server to\nreduce latency."
    },
    {
      "page": 68,
      "text": "Figure 4-17 (source: [10])\n\nSecond, synchronize data with an eventual consistency model. If you are unclear about the\neventual consistency model, refer to the “Consistency” section in “Chapter 6: Design a Key-\nvalue Store.”\n\nMonitoring\n\nAfter the rate limiter is put in place, it is important to gather analytics data to check whether\nthe rate limiter is effective. Primarily, we want to make sure:\n\n+ The rate limiting algorithm is effective.\n\n+ The rate limiting rules are effective.\nFor example, if rate limiting rules are too strict, many valid requests are dropped. In this case,\nwe want to relax the rules a little bit. In another example, we notice our rate limiter becomes\nineffective when there is a sudden increase in traffic like flash sales. In this scenario, we may\nreplace the algorithm to support burst traffic. Token bucket is a good fit here."
    },
    {
      "page": 69,
      "text": "Step 4 - Wrap up\nIn this chapter, we discussed different algorithms of rate limiting and their pros/cons.\nAlgorithms discussed include:\n* Token bucket\n* Leaking bucket\n+ Fixed window\n* Sliding window log\n* Sliding window counter\nThen, we discussed the system architecture, rate limiter in a distributed environment,\nperformance optimization and monitoring. Similar to any system design interview questions,\nthere are additional talking points you can mention if time allows:\n+ Hard vs soft rate limiting.\n* Hard: The number of requests cannot exceed the threshold.\n* Soft: Requests can exceed the threshold for a short period.\n* Rate limiting at different levels. In this chapter, we only talked about rate limiting at the\napplication level (HTTP: layer 7). It is possible to apply rate limiting at other layers. For\nexample, you can apply rate limiting by IP addresses using Iptables [15] (IP: layer 3).\nNote: The Open Systems Interconnection model (OSI model) has 7 layers [16]: Layer 1:\nPhysical layer, Layer 2: Data link layer, Layer 3: Network layer, Layer 4: Transport layer,\nLayer 5: Session layer, Layer 6: Presentation layer, Layer 7: Application layer.\n« Avoid being rate limited. Design your client with best practices:\n* Use client cache to avoid making frequent API calls.\n¢ Understand the limit and do not send too many requests in a short time frame.\n* Include code to catch exceptions or errors so your client can gracefully recover from\nexceptions.\n¢ Add sufficient back off time to retry logic.\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 70,
      "text": "Reference materials\n\n[1] Rate-limiting strategies and techniques: https://cloud.google.com/solutions/rate-limiting-\nstrategies-techniques\n[2] Twitter rate limits: https://developer.twitter.com/en/docs/basics/rate-limits\n\n[3] Google docs usage limits: https://developers.google.com/docs/api/limits\n\n[4] IBM microservices: https://www.ibm.com/cloud/learn/microservices\n\n[5] Throttle API requests for better throughput:\nhttps://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-request-\nthrottling.html\n\n[6] Stripe rate limiters: https://stripe.com/blog/rate-limiters\n\n[7] Shopify REST Admin API rate limits: https://help.shopify.com/en/api/reference/rest-\nadmin-api-rate-limits\n\n[8] Better Rate Limiting With Redis Sorted Sets:\nhttps://engineering.classdojo.com/blog/2015/02/06/rolling-rate-limiter/\n\n[9] System Design — Rate limiter and Data modelling:\nhttps://medium.com/@saisandeepmopuri/system-design-rate-limiter-and-data-modelling-\n9304b0d18250\n\n[10] How we built rate limiting capable of scaling to millions of domains:\nhttps://blog.cloudflare.com/counting-things-a-lot-of-different-things/\n\n[11] Redis website: https://redis.io/\n[12] Lyft rate limiting: https://github.com/lyft/ratelimit\n\n[13] Scaling your API with rate limiters:\nhttps://gist.github.com/ptarjan/e38f45f2dfe601419ca3af937fff574d#request-rate-limiter\n\n[14] What is edge computing: https://www.cloudflare.com/learning/serverless/glossary/what-\nis-edge-computing/\n\n[15] Rate Limit Requests with Iptables: https://blog.programster.org/rate-limit-requests-with-\niptables\n\n[16] OSI model: https://en.wikipedia.org/wiki/OSI_model#Layer_ architecture"
    },
    {
      "page": 71,
      "text": "CHAPTER 5: DESIGN CONSISTENT HASHING\n\nTo achieve horizontal scaling, it is important to distribute requests/data efficiently and evenly\nacross servers. Consistent hashing is a commonly used technique to achieve this goal. But\nfirst, let us take an in-depth look at the problem."
    },
    {
      "page": 72,
      "text": "The rehashing problem\n\nIf you have n cache servers, a common way to balance the load is to use the following hash\nmethod:\n\nserverIndex = hash(key) % N, where N is the size of the server pool.\n\nLet us use an example to illustrate how it works. As shown in Table 5-1, we have 4 servers\nand 8 string keys with their hashes.\n\nhash hash % 4\n18358617\nkey1 26143584 }\nkey2 18131146 2\nkey3 35863496 0\nkey4 34085809 1\nkey5 27581703 3\nkey6 38164978 2\nkey7 22530351 3\nTable 5-1\n\nTo fetch the server where a key is stored, we perform the modular operation f(key) % 4. For\ninstance, hash(key0) % 4 = 1 means a client must contact server 1 to fetch the cached data.\nFigure 5-1 shows the distribution of keys based on Table 5-1."
    },
    {
      "page": 73,
      "text": "serverIndex = hash % 4\n\nServer Index 0 1 2 3\nServers server O server 1 server 2 server 3\nKeys key1 keyo key2 key5\nkey3 key4 key6 key7\nFigure 5-1\n\nThis approach works well when the size of the server pool is fixed, and the data distribution\n\nis even. However, problems arise when new servers are added, or existing servers are\n\nremoved. For example, if server 1 goes offline, the size of the server pool becomes 3. Using\n\nthe same hash function, we get the same hash value for a key. But applying modular\n\noperation gives us different server indexes because the number of servers is reduced by 1. We\n\nget the results as shown in Table 5-2 by applying hash % 3:\n\nkey Hash hash % 3\nkeyO 18358617 ft)\nkey1 26143584 0\nkey2 18131146 1\nkey3 35863496 2\nkey4 34085809 1\nkey5 27581703 0\nkey6 38164978 1\nkey7 22530351 0\nTable 5-2\n\nFigure 5-2 shows the new distribution of keys based on Table 5-2."
    },
    {
      "page": 74,
      "text": "serverIndex = hash % 3\n\nServer Index 0 1 2\nServers server O server 1 server 2 server 3\nKeys keyo key2 key3\nkey1 key4\nkey5 key6\nkey7\nFigure 5-2\n\nAs shown in Figure 5-2, most keys are redistributed, not just the ones originally stored in the\noffline server (server 1). This means that when server 1 goes offline, most cache clients will\nconnect to the wrong servers to fetch data. This causes a storm of cache misses. Consistent\nhashing is an effective technique to mitigate this problem."
    },
    {
      "page": 75,
      "text": "Consistent hashing\n\nQuoted from Wikipedia: \"Consistent hashing is a special kind of hashing such that when a\nhash table is re-sized and consistent hashing is used, only k/n keys need to be remapped on\naverage, where k is the number of keys, and n is the number of slots. In contrast, in most\ntraditional hash tables, a change in the number of array slots causes nearly all keys to be\nremapped [1]”.\n\nHash space and hash ring\n\nNow we understand the definition of consistent hashing, let us find out how it works. Assume\nSHA-1 is used as the hash function f, and the output range of the hash function is: x0, x1, x2,\nx3, ..., xn. In cryptography, SHA-1’s hash space goes from 0 to 2160 - 1. That means x0\ncorresponds to 0, xn corresponds to 24160 — 1, and all the other hash values in the middle fall\nbetween 0 and 24160 - 1. Figure 5-3 shows the hash space.\n\n| xn\nFigure 5-3\nBy collecting both ends, we get a hash ring as shown in Figure 5-4:\n\nFigure 5-4\n\nHash servers\n\nUsing the same hash function f, we map servers based on server IP or name onto the ring.\nFigure 5-5 shows that 4 servers are mapped on the hash ring."
    },
    {
      "page": 76,
      "text": "f (server 0)\n\nServers\n\nsO = server 0\ns1 = server 1\ns2 = server 2\ns3 = server 3\n\nFigure 5-5\nHash keys\n\nOne thing worth mentioning is that hash function used here is different from the one in “the\nrehashing problem,” and there is no modular operation. As shown in Figure 5-6, 4 cache keys\n(key0, key1, key2, and key3) are hashed onto the hash ring\n\nServers\nsO = server 0\ns1 = server 1\ns2 = server 2\n\ns3 = server 3\n\nkO = keyO\nk1 = key1\nk2 = key2\nk3 = key3\n\nFigure 5-6\n\nServer lookup"
    },
    {
      "page": 77,
      "text": "To determine which server a key is stored on, we go clockwise from the key position on the\nring until a server is found. Figure 5-7 explains this process. Going clockwise, key0 is stored\non server 0; key1 is stored on server 1; key2 is stored on server 2 and key3 is stored on server\n3.\n\nServers\n\nsO = server 0\ns1 = server 1\ns2 = server 2\ns3 = server 3\n\nserver 0\n\nkO = keyO\nk1 = key1\nk2 = key2\nk3 = key3\n\nFigure 5-7\n\nAdd a server\n\nUsing the logic described above, adding a new server will only require redistribution of a\nfraction of keys.\n\nIn Figure 5-8, after a new server 4 is added, only key0 needs to be redistributed. k1, k2, and\nk3 remain on the same servers. Let us take a close look at the logic. Before server 4 is added,\nkey0 is stored on server 0. Now, key0 will be stored on server 4 because server 4 is the first\nserver it encounters by going clockwise from key0’s position on the ring. The other keys are\nnot redistributed based on consistent hashing algorithm."
    },
    {
      "page": 78,
      "text": "Servers\n\nsO = server 0\ns1 = server 1\n$2 = server 2\ns3 = server 3\n\nserver 0\n\nkO = keyO\nk1 = key1\nk2 = key2\nk3 = key3\n\nkey2\n\nFigure 5-8\nRemove a server\n\nWhen a server is removed, only a small fraction of keys require redistribution with consistent\nhashing. In Figure 5-9, when server 1 is removed, only key1 must be remapped to server 2.\nThe rest of the keys are unaffected."
    },
    {
      "page": 79,
      "text": "Servers\n\nsO = server 0\ns1 = server 1\n$2 = server 2\ns3 = server 3\n\nserver 0\n\nkO = keyO\nk1 = key1\nk2 = key2\nk3 = key3\n\nFigure 5-9\n\nTwo issues in the basic approach\n\nThe consistent hashing algorithm was introduced by Karger et al. at MIT [1]. The basic steps\nare:\n\n* Map servers and keys on to the ring using a uniformly distributed hash function.\n\n* To find out which server a key is mapped to, go clockwise from the key position until the\nfirst server on the ring is found.\n\nTwo problems are identified with this approach. First, it is impossible to keep the same size\nof partitions on the ring for all servers considering a server can be added or removed. A\npartition is the hash space between adjacent servers. It is possible that the size of the\npartitions on the ring assigned to each server is very small or fairly large. In Figure 5-10, if s1\nis removed, s2’s partition (highlighted with the bidirectional arrows) is twice as large as sO\nand s3’s partition."
    },
    {
      "page": 80,
      "text": "Servers\n\nso\nsO = server 0\nserver 0 $1 = server 1\ns2 = server 2\n$3 = server 3\n\nFigure 5-10\n\nSecond, it is possible to have a non-uniform key distribution on the ring. For instance, if\nservers are mapped to positions listed in Figure 5-11, most of the keys are stored on server 2.\nHowever, server 1 and server 3 have no data.\n\nServers\n\nsO = server 0\nserver 0 s1 = server 1\ns2 = server 2\ns3 = server 3\n\nFigure 5-11\nA technique called virtual nodes or replicas is used to solve these problems.\n\nVirtual nodes\n\nA virtual node refers to the real node, and each server is represented by multiple virtual nodes\non the ring. In Figure 5-12, both server 0 and server 1 have 3 virtual nodes. The 3 is"
    },
    {
      "page": 81,
      "text": "arbitrarily chosen; and in real-world systems, the number of virtual nodes is much larger.\nInstead of using sO, we have s0_0, sO_1, and sO_2 to represent server 0 on the ring. Similarly,\ns1_0,s1_1, and s1_2 represent server 1 on the ring. With virtual nodes, each server is\nresponsible for multiple partitions. Partitions (edges) with label sO are managed by server 0.\nOn the other hand, partitions with label s1 are managed by server 1.\n\n1\nServers\nso_2\n\n80 50 = server 0\n\nserver 0 s1 = server 1\nsO_o\nsO\ns1\nsO_1 50\n—$—$—<———\n\nFigure 5-12\n\nTo find which server a key is stored on, we go clockwise from the key’s location and find the\nfirst virtual node encountered on the ring. In Figure 5-13, to find out which server kO is stored\non, we go clockwise from k0’s location and find virtual node s1_1, which refers to server 1."
    },
    {
      "page": 82,
      "text": "Servers\nso_2\n\nsO = server 0\nserver 0 s1 = server 1\n\nsO_O\n\nsO_1\n\nFigure 5-13\n\nAs the number of virtual nodes increases, the distribution of keys becomes more balanced.\nThis is because the standard deviation gets smaller with more virtual nodes, leading to\nbalanced data distribution. Standard deviation measures how data are spread out. The\noutcome of an experiment carried out by online research [2] shows that with one or two\nhundred virtual nodes, the standard deviation is between 5% (200 virtual nodes) and 10%\n(100 virtual nodes) of the mean. The standard deviation will be smaller when we increase the\nnumber of virtual nodes. However, more spaces are needed to store data about virtual nodes.\nThis is a tradeoff, and we can tune the number of virtual nodes to fit our system requirements.\n\nFind affected keys\n\nWhen a server is added or removed, a fraction of data needs to be redistributed. How can we\nfind the affected range to redistribute the keys?\n\nIn Figure 5-14, server 4 is added onto the ring. The affected range starts from s4 (newly\nadded node) and moves anticlockwise around the ring until a server is found (s3). Thus, keys\nlocated between s3 and s4 need to be redistributed to s4."
    },
    {
      "page": 83,
      "text": "Servers\n\nsO = server 0\ns1 = server 1\n$2 = server 2\ns3 = server 3\n\nkO = keyO\nk1 = key1\nk2 = key2\nk3 = key3\n\nkey2\n\nFigure 5-14\n\nWhen a server (s1) is removed as shown in Figure 5-15, the affected range starts from s1\n(removed node) and moves anticlockwise around the ring until a server is found (s0). Thus,\nkeys located between sO and s1 must be redistributed to s2."
    },
    {
      "page": 84,
      "text": "Servers\n\nKy)\n\nFigure 5-15\n\nsO = server 0\ns1 = server 1\ns2 = server 2\ns3 = server 3\n\nkO = keyO\nk1 = key1\nk2 = key2\nk3 = key3"
    },
    {
      "page": 85,
      "text": "Wrap up\nIn this chapter, we had an in-depth discussion about consistent hashing, including why it is\nneeded and how it works. The benefits of consistent hashing include:\n* Minimized keys are redistributed when servers are added or removed.\n* It is easy to scale horizontally because data are more evenly distributed.\n* Mitigate hotspot key problem. Excessive access to a specific shard could cause server\noverload. Imagine data for Katy Perry, Justin Bieber, and Lady Gaga all end up on the\nsame shard. Consistent hashing helps to mitigate the problem by distributing the data more\nevenly.\nConsistent hashing is widely used in real-world systems, including some notable ones:\n¢ Partitioning component of Amazon’s Dynamo database [3]\n* Data partitioning across the cluster in Apache Cassandra [4]\n* Discord chat application [5]\n« Akamai content delivery network [6]\n* Maglev network load balancer [7]\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 86,
      "text": "Reference materials\n\n[1] Consistent hashing: https://en.wikipedia.org/wiki/Consistent_hashing\n[2] Consistent Hashing:\n\nhttps://tom-e-white.com/2007/11/consistent-hashing.html\n\n[3] Dynamo: Amazon’s Highly Available Key-value Store:\nhttps://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\n\n[4] Cassandra - A Decentralized Structured Storage System:\nhttp://www.cs.cornell.edu/Projects/ladis2009/papers/Lakshman-ladis2009.PDF\n\n[5] How Discord Scaled Elixir to 5,000,000 Concurrent Users:\nhttps://blog.discord.com/scaling-elixir-f9b8ele7c29b\n\n[6] CS168: The Modern Algorithmic Toolbox Lecture #1: Introduction and Consistent\nHashing: http://theory.stanford.edu/~tim/s16/1/11.pdf\n\n[7] Maglev: A Fast and Reliable Software Network Load Balancer:\nhttps://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44824.pdf"
    },
    {
      "page": 87,
      "text": "CHAPTER 6: DESIGN A KEY-VALUE STORE\n\nA key-value store, also referred to as a key-value database, is a non-relational database. Each\nunique identifier is stored as a key with its associated value. This data pairing is known as a\n“key-value” pair.\n\nIn a key-value pair, the key must be unique, and the value associated with the key can be\naccessed through the key. Keys can be plain text or hashed values. For performance reasons,\na short key works better. What do keys look like? Here are a few examples:\n\n* Plain text key: “last_logged_in_at”\n\n* Hashed key: 253DDEC4\n\nThe value in a key-value pair can be strings, lists, objects, etc. The value is usually treated as\nan opaque object in key-value stores, such as Amazon dynamo [1], Memcached [2], Redis\n[3], etc.\n\nHere is a data snippet in a key-value store:\n\nKey value\n145 john\n147 bob\n160 Julia\nTable 6-1\n\nIn this chapter, you are asked to design a key-value store that supports the following\noperations:\n\n- put(key, value) // insert “value” associated with “key”\n\n- get(key) // get “value” associated with “key”"
    },
    {
      "page": 88,
      "text": "Understand the problem and establish design scope\n\nThere is no perfect design. Each design achieves a specific balance regarding the tradeoffs of\nthe read, write, and memory usage. Another tradeoff has to be made was between consistency\nand availability. In this chapter, we design a key-value store that comprises of the following\ncharacteristics:\n\n¢ The size of a key-value pair is small: less than 10 KB.\n\n« Ability to store big data.\n\n* High availability: The system responds quickly, even during failures.\n\n* High scalability: The system can be scaled to support large data set.\n\n« Automatic scaling: The addition/deletion of servers should be automatic based on traffic.\n¢ Tunable consistency.\n\n* Low latency."
    },
    {
      "page": 89,
      "text": "Single server key-value store\n\nDeveloping a key-value store that resides in a single server is easy. An intuitive approach is\nto store key-value pairs in a hash table, which keeps everything in memory. Even though\nmemory access is fast, fitting everything in memory may be impossible due to the space\nconstraint. Two optimizations can be done to fit more data in a single server:\n\n¢ Data compression\n\n* Store only frequently used data in memory and the rest on disk\n\nEven with these optimizations, a single server can reach its capacity very quickly. A\ndistributed key-value store is required to support big data."
    },
    {
      "page": 90,
      "text": "Distributed key-value store\n\nA distributed key-value store is also called a distributed hash table, which distributes key-\nvalue pairs across many servers. When designing a distributed system, it is important to\nunderstand CAP (Consistency, Availability, Partition Tolerance) theorem.\n\nCAP theorem\n\nCAP theorem states it is impossible for a distributed system to simultaneously provide more\nthan two of these three guarantees: consistency, availability, and partition tolerance. Let us\nestablish a few definitions.\n\nConsistency: consistency means all clients see the same data at the same time no matter\nwhich node they connect to.\n\nAvailability: availability means any client which requests data gets a response even if some\nof the nodes are down.\n\nPartition Tolerance: a partition indicates a communication break between two nodes.\nPartition tolerance means the system continues to operate despite network partitions.\n\nCAP theorem states that one of the three properties must be sacrificed to support 2 of the 3\nproperties as shown in Figure 6-1.\n\nFigure 6-1\n\nNowadays, key-value stores are classified based on the two CAP characteristics they support:\n\nCP (consistency and partition tolerance) systems: a CP key-value store supports\nconsistency and partition tolerance while sacrificing availability.\n\nAP (availability and partition tolerance) systems: an AP key-value store supports\navailability and partition tolerance while sacrificing consistency.\n\nCA (consistency and availability) systems: a CA key-value store supports consistency and"
    },
    {
      "page": 91,
      "text": "availability while sacrificing partition tolerance. Since network failure is unavoidable, a\ndistributed system must tolerate network partition. Thus, a CA system cannot exist in real-\nworld applications.\n\nWhat you read above is mostly the definition part. To make it easier to understand, let us take\na look at some concrete examples. In distributed systems, data is usually replicated multiple\ntimes. Assume data are replicated on three replica nodes, n1, n2 and n3 as shown in Figure 6-\n2.\n\nIdeal situation\n\nIn the ideal world, network partition never occurs. Data written to n1 is automatically\nreplicated to n2 and n3. Both consistency and availability are achieved.\n\nn2 n3\n\nFigure 6-2\nReal-world distributed systems\n\nIn a distributed system, partitions cannot be avoided, and when a partition occurs, we must\nchoose between consistency and availability. In Figure 6-3, n3 goes down and cannot\ncommunicate with n1 and n2. If clients write data to n1 or n2, data cannot be propagated to\nn3. If data is written to n3 but not propagated to nl and n2 yet, n1 and n2 would have stale\ndata."
    },
    {
      "page": 92,
      "text": "n2 n3\n\nFigure 6-3\n\nIf we choose consistency over availability (CP system), we must block all write operations to\nnl and n2 to avoid data inconsistency among these three servers, which makes the system\nunavailable. Bank systems usually have extremely high consistent requirements. For\nexample, it is crucial for a bank system to display the most up-to-date balance info. If\ninconsistency occurs due to a network partition, the bank system returns an error before the\ninconsistency is resolved.\n\nHowever, if we choose availability over consistency (AP system), the system keeps accepting\nreads, even though it might return stale data. For writes, n1 and n2 will keep accepting writes,\nand data will be synced to n3 when the network partition is resolved.\n\nChoosing the right CAP guarantees that fit your use case is an important step in building a\ndistributed key-value store. You can discuss this with your interviewer and design the system\naccordingly.\n\nSystem components\n\nIn this section, we will discuss the following core components and techniques used to build a\nkey-value store:\n\nData partition\n\n* Data replication\n\n* Consistency\n\n* Inconsistency resolution\n\n* Handling failures\n\n* System architecture diagram\n* Write path\n\n* Read path"
    },
    {
      "page": 93,
      "text": "The content below is largely based on three popular key-value store systems: Dynamo [4],\nCassandra [5], and BigTable [6].\nData partition\n\nFor large applications, it is infeasible to fit the complete data set in a single server. The\nsimplest way to accomplish this is to split the data into smaller partitions and store them in\nmultiple servers. There are two challenges while partitioning the data:\n\n* Distribute data across multiple servers evenly.\n\n* Minimize data movement when nodes are added or removed.\nConsistent hashing discussed in Chapter 5 is a great technique to solve these problems. Let us\nrevisit how consistent hashing works at a high-level.\n\n¢ First, servers are placed on a hash ring. In Figure 6-4, eight servers, represented by sO,\nsl, ..., S7, are placed on the hash ring.\n\n* Next, a key is hashed onto the same ring, and it is stored on the first server encountered\nwhile moving in the clockwise direction. For instance, key0 is stored in s1 using this logic.\n\nkeyo\n\nFigure 6-4\n\nUsing consistent hashing to partition data has the following advantages:\n\nAutomatic scaling: servers could be added and removed automatically depending on the"
    },
    {
      "page": 94,
      "text": "load.\n\nHeterogeneity: the number of virtual nodes for a server is proportional to the server capacity.\nFor example, servers with higher capacity are assigned with more virtual nodes.\n\nData replication\n\nTo achieve high availability and reliability, data must be replicated asynchronously over N\nservers, where N is a configurable parameter. These N servers are chosen using the following\nlogic: after a key is mapped to a position on the hash ring, walk clockwise from that position\nand choose the first N servers on the ring to store data copies. In Figure 6-5 (N = 3), key0 is\nreplicated at s1, s2, and s3.\n\nkeyo\n\nFigure 6-5\n\nWith virtual nodes, the first N nodes on the ring may be owned by fewer than N physical\nservers. To avoid this issue, we only choose unique servers while performing the clockwise\nwalk logic.\n\nNodes in the same data center often fail at the same time due to power outages, network\nissues, natural disasters, etc. For better reliability, replicas are placed in distinct data centers,\nand data centers are connected through high-speed networks."
    },
    {
      "page": 95,
      "text": "Consistency\n\nSince data is replicated at multiple nodes, it must be synchronized across replicas. Quorum\nconsensus can guarantee consistency for both read and write operations. Let us establish a\nfew definitions first.\n\nN = The number of replicas\n\nW = A write quorum of size W. For a write operation to be considered as successful, write\noperation must be acknowledged from W replicas.\n\nR = A read quorum of size R. For a read operation to be considered as successful, read\noperation must wait for responses from at least R replicas.\n\nConsider the following example shown in Figure 6-6 with N = 3.\n\nput(key1, vali)\n\nFigure 6-6 (ACK = acknowledgement)\n\nW = 1 does not mean data is written on one server. For instance, with the configuration in\nFigure 6-6, data is replicated at sO, s1, and s2. W = 1 means that the coordinator must receive\nat least one acknowledgment before the write operation is considered as successful. For\ninstance, if we get an acknowledgment from s1, we no longer need to wait for\nacknowledgements from sO and s2. A coordinator acts as a proxy between the client and the\nnodes.\n\nThe configuration of W, R and N is a typical tradeoff between latency and consistency. If W =\n1 or R = 1, an operation is returned quickly because a coordinator only needs to wait for a\nresponse from any of the replicas. If W or R > 1, the system offers better consistency;\nhowever, the query will be slower because the coordinator must wait for the response from\nthe slowest replica.\n\nIf W + R > N, strong consistency is guaranteed because there must be at least one"
    },
    {
      "page": 96,
      "text": "overlapping node that has the latest data to ensure consistency.\n\nHow to configure N, W, and R to fit our use cases? Here are some of the possible setups:\n\nIf R = 1 and W = VN, the system is optimized for a fast read.\n\nIf W = 1 and R = N, the system is optimized for fast write.\n\nIf W + R > N, strong consistency is guaranteed (Usually N = 3, W = R = 2).\n\nIf W + R <= N, strong consistency is not guaranteed.\n\nDepending on the requirement, we can tune the values of W, R, N to achieve the desired level\nof consistency.\n\nConsistency models\n\nConsistency model is other important factor to consider when designing a key-value store. A\nconsistency model defines the degree of data consistency, and a wide spectrum of possible\nconsistency models exist:\n\n* Strong consistency: any read operation returns a value corresponding to the result of the\nmost updated write data item. A client never sees out-of-date data.\n\n« Weak consistency: subsequent read operations may not see the most updated value.\n\n* Eventual consistency: this is a specific form of weak consistency. Given enough time, all\nupdates are propagated, and all replicas are consistent.\n\nStrong consistency is usually achieved by forcing a replica not to accept new reads/writes\nuntil every replica has agreed on current write. This approach is not ideal for highly available\nsystems because it could block new operations. Dynamo and Cassandra adopt eventual\nconsistency, which is our recommended consistency model for our key-value store. From\nconcurrent writes, eventual consistency allows inconsistent values to enter the system and\nforce the client to read the values to reconcile. The next section explains how reconciliation\nworks with versioning."
    },
    {
      "page": 97,
      "text": "Inconsistency resolution: versioning\n\nReplication gives high availability but causes inconsistencies among replicas. Versioning and\nvector locks are used to solve inconsistency problems. Versioning means treating each data\nmodification as a new immutable version of data. Before we talk about versioning, let us use\nan example to explain how inconsistency happens:\n\nAs shown in Figure 6-7, both replica nodes ni and n2 have the same value. Let us call this\nvalue the original value. Server 1 and server 2 get the same value for get(“name”) operation.\n\nget(\"name\")\n7\nWem name: john\nreturn \"john\nserver 1 n1\nget(\"name\")\nreturn \"john\" name: john\nserver 2 n2\nFigure 6-7\n\nNext, server 1 changes the name to “johnSanFrancisco”, and server 2 changes the name to\n“SjohnNewY ork” as shown in Figure 6-8. These two changes are performed simultaneously.\nNow, we have conflicting values, called versions v1 and v2.\n\nput(\"name\", \"johnSanFrancisco) ; ;\nname: johnSanFrancisco\n\nserver 1\nput(\"name\", \"johnNewYork) Ra\n| name: johnNewYork\nserver 2 n2\n\nFigure 6-8\n\nIn this example, the original value could be ignored because the modifications were based on\nit. However, there is no clear way to resolve the conflict of the last two versions. To resolve\nthis issue, we need a versioning system that can detect conflicts and reconcile conflicts. A\nvector clock is a common technique to solve this problem. Let us examine how vector clocks\nwork."
    },
    {
      "page": 98,
      "text": "A vector clock is a [server, version] pair associated with a data item. It can be used to check\nif one version precedes, succeeds, or in conflict with others.\n\nAssume a vector clock is represented by D([S1, v1], [S2, v2], ..., [Sn, vn]), where D is a data\nitem, v1 is a version counter, and s1 is a server number, etc. If data item D is written to server\nSi, the system must perform one of the following tasks.\n\n¢ Increment vi if [Si, vi] exists.\n* Otherwise, create a new entry [Si, 1].\n\nThe above abstract logic is explained with a concrete example as shown in Figure 6-9.\n\n@) write handled by Sx\nD1([Sx, 1])\n\n@)write handled by Sx\nD2([Sx, 2])\n\n@ write handled by Sy @)write handled by Sz\n\nD3([Sx, 2], [Sy, 1]) D4([Sx, 2], [Sz, 1])\n\n6) reconciled and written by Sx\n\nD5([Sx, 3], [Sy, 1], [Sz, 1])\n\nFigure 6-9 (source: [4])\n1. A client writes a data item D1 to the system, and the write is handled by server Sx,\nwhich now has the vector clock D1[(Sx, 1)].\n2. Another client reads the latest D1, updates it to D2, and writes it back. D2 descends\nfrom D1 so it overwrites D1. Assume the write is handled by the same server Sx, which\nnow has vector clock D2([Sx, 2]).\n3. Another client reads the latest D2, updates it to D3, and writes it back. Assume the write\nis handled by server Sy, which now has vector clock D3([Sx, 2], [Sy, 1])).\n4. Another client reads the latest D2, updates it to D4, and writes it back. Assume the write"
    },
    {
      "page": 99,
      "text": "is handled by server Sz, which now has D4([Sx, 2], [Sz, 1])).\n\n5. When another client reads D3 and D4, it discovers a conflict, which is caused by data\nitem D2 being modified by both Sy and Sz. The conflict is resolved by the client and\nupdated data is sent to the server. Assume the write is handled by Sx, which now has\nD5([Sx, 3], [Sy, 1], [Sz, 1]). We will explain how to detect conflict shortly.\n\nUsing vector clocks, it is easy to tell that a version X is an ancestor (i.e. no conflict) of\nversion Y if the version counters for each participant in the vector clock of Y is greater than or\nequal to the ones in version X. For example, the vector clock D([s0, 1], [s1, 1])] is an\nancestor of D([s0, 1], [s1, 2]). Therefore, no conflict is recorded.\n\nSimilarly, you can tell that a version X is a sibling (i.e., a conflict exists) of Y if there is any\nparticipant in Y's vector clock who has a counter that is less than its corresponding counter in\nX. For example, the following two vector clocks indicate there is a conflict: D([s0, 1], [s1,\n2]) and D([s0, 2], [s1, 1]).\n\nEven though vector clocks can resolve conflicts, there are two notable downsides. First,\nvector clocks add complexity to the client because it needs to implement conflict resolution\nlogic.\n\nSecond, the [server: version] pairs in the vector clock could grow rapidly. To fix this\nproblem, we set a threshold for the length, and if it exceeds the limit, the oldest pairs are\nremoved. This can lead to inefficiencies in reconciliation because the descendant relationship\ncannot be determined accurately. However, based on Dynamo paper [4], Amazon has not yet\nencountered this problem in production; therefore, it is probably an acceptable solution for\nmost companies.\n\nHandling failures\n\nAs with any large system at scale, failures are not only inevitable but common. Handling\nfailure scenarios is very important. In this section, we first introduce techniques to detect\nfailures. Then, we go over common failure resolution strategies.\n\nFailure detection\n\nIn a distributed system, it is insufficient to believe that a server is down because another\nserver says so. Usually, it requires at least two independent sources of information to mark a\nserver down.\n\nAs shown in Figure 6-10, all-to-all multicasting is a straightforward solution. However, this is\ninefficient when many servers are in the system."
    },
    {
      "page": 100,
      "text": "Figure 6-10\nA better solution is to use decentralized failure detection methods like gossip protocol.\nGossip protocol works as follows:\n\n* Each node maintains a node membership list, which contains member IDs and heartbeat\ncounters.\n\n* Each node periodically increments its heartbeat counter.\n\n* Each node periodically sends heartbeats to a set of random nodes, which in turn\npropagate to another set of nodes.\n\n* Once nodes receive heartbeats, membership list is updated to the latest info.\n\n+ If the heartbeat has not increased for more than predefined periods, the member is\nconsidered as offline.\n\ndetected s2 is down\n\ns0's membership list\n\nMember ID Heartbeat counter Time\n0 10232 12:00:01\n1 10224 12:00:10\n2 9908 11:58:02\n3 10237 12:00:20\n4 10234 12:00:34\n\nFigure 6-11\nAs shown in Figure 6-11:"
    },
    {
      "page": 101,
      "text": "* Node sO maintains a node membership list shown on the left side.\n\n* Node sO notices that node s2’s (member ID = 2) heartbeat counter has not increased for a\nlong time.\n\n* Node sO sends heartbeats that include s2’s info to a set of random nodes. Once other\nnodes confirm that s2’s heartbeat counter has not been updated for a long time, node s2 is\nmarked down, and this information is propagated to other nodes.\n\nHandling temporary failures\n\nAfter failures have been detected through the gossip protocol, the system needs to deploy\ncertain mechanisms to ensure availability. In the strict quorum approach, read and write\noperations could be blocked as illustrated in the quorum consensus section.\n\nA technique called “sloppy quorum” [4] is used to improve availability. Instead of enforcing\nthe quorum requirement, the system chooses the first W healthy servers for writes and first R\nhealthy servers for reads on the hash ring. Offline servers are ignored.\n\nIf a server is unavailable due to network or server failures, another server will process\nrequests temporarily. When the down server is up, changes will be pushed back to achieve\ndata consistency. This process is called hinted handoff. Since s2 is unavailable in Figure 6-\n12, reads and writes will be handled by s3 temporarily. When s2 comes back online, s3 will\nhand the data back to s2.\n\ncoordinator\n\nFigure 6-12\n\nHandling permanent failures"
    },
    {
      "page": 102,
      "text": "Hinted handoff is used to handle temporary failures. What if a replica is permanently\nunavailable? To handle such a situation, we implement an anti-entropy protocol to keep\nreplicas in sync. Anti-entropy involves comparing each piece of data on replicas and updating\neach replica to the newest version. A Merkle tree is used for inconsistency detection and\nminimizing the amount of data transferred.\n\nQuoted from Wikipedia [7]: “A hash tree or Merkle tree is a tree in which every non-leaf\nnode is labeled with the hash of the labels or values (in case of leaves) of its child nodes.\nHash trees allow efficient and secure verification of the contents of large data structures”.\n\nAssuming key space is from 1 to 12, the following steps show how to build a Merkle tree.\nHighlighted boxes indicate inconsistency.\n\nStep 1: Divide key space into buckets (4 in our example) as shown in Figure 6-13. A bucket\nis used as the root level node to maintain a limited depth of the tree.\n\nserver 1 server 2\n\nFigure 6-13\n\nStep 2: Once the buckets are created, hash each key in a bucket using a uniform hashing\nmethod (Figure 6-14).\n\nserver 1\n\nFigure 6-14\nStep 3: Create a single hash node per bucket (Figure 6-15).\n\nserver 1 server 2\n\n‘ |40-> 3542 |} i [12943 : 10-> 3542 | !\n\n| [11> 8708 | ! i |2-> 1456 5-> 2145 11> 8705 |!\n\n: 12-> 3697 | 1 [3-5 0865 6 > 7456 42 -> 3697 | |\nFigure 6-15\n\nStep 4: Build the tree upwards till root by calculating hashes of children (Figure 6-16)."
    },
    {
      "page": 103,
      "text": "server 1 server 2\n\n2-> 1456 5 -> 2145 11 -> 8705 2 -> 1456 5 -> 2145 11 -> 8705\n\nFigure 6-16\n\nTo compare two Merkle trees, start by comparing the root hashes. If root hashes match, both\nservers have the same data. If root hashes disagree, then the left child hashes are compared\nfollowed by right child hashes. You can traverse the tree to find which buckets are not\nsynchronized and synchronize those buckets only.\n\nUsing Merkle trees, the amount of data needed to be synchronized is proportional to the\ndifferences between the two replicas, and not the amount of data they contain. In real-world\nsystems, the bucket size is quite big. For instance, a possible configuration is one million\nbuckets per one billion keys, so each bucket only contains 1000 keys.\n\nHandling data center outage\n\nData center outage could happen due to power outage, network outage, natural disaster, etc.\nTo build a system capable of handling data center outage, it is important to replicate data\nacross multiple data centers. Even if a data center is completely offline, users can still access\ndata through the other data centers."
    },
    {
      "page": 104,
      "text": "System architecture diagram\n\nNow that we have discussed different technical considerations in designing a key-value store,\nwe can shift our focus on the architecture diagram, shown in Figure 6-17.\n\nread/write\n\nresponse —<-\ncoordinator\n\nFigure 6-17\n\nMain features of the architecture are listed as follows:\n* Clients communicate with the key-value store through simple APIs: get(key) and put(key,\nvalue).\n« A coordinator is a node that acts as a proxy between the client and the key-value store.\n* Nodes are distributed on a ring using consistent hashing.\n¢ The system is completely decentralized so adding and moving nodes can be automatic.\n+ Data is replicated at multiple nodes.\n* There is no single point of failure as every node has the same set of responsibilities.\nAs the design is decentralized, each node performs many tasks as presented in Figure 6-18."
    },
    {
      "page": 105,
      "text": "Write path\n\nFailure detection\n\nClient API\n\nConflict\nresolution\n\n[ren | meee ener\n\nFailure repair\nmechanism\n\nFigure 6-18\n\nFigure 6-19 explains what happens after a write request is directed to a specific node. Please\nnote the proposed designs for write/read paths are primary based on the architecture of\n\nCassandra [8].\n\nServer\n\n@\n\nMemory cache\n\nMEMORY\n\nDISK\n\nCommit log SSTables\n\nFigure 6-19\n\n1. The write request is persisted on a commit log file.\n\n2. Data is saved in the memory cache."
    },
    {
      "page": 106,
      "text": "3. When the memory cache is full or reaches a predefined threshold, data is flushed to\nSSTable [9] on disk. Note: A sorted-string table (SSTable) is a sorted list of <key, value>\npairs. For readers interested in learning more about SStable, refer to the reference material\n\n[9].\nRead path\n\nAfter a read request is directed to a specific node, it first checks if data is in the memory\ncache. If so, the data is returned to the client as shown in Figure 6-20.\n\nServer\n\n@\n\nRead request\n\nMemory cache\n\nReturn result\n\nMEMORY\nDISK\nSSTables\nFigure 6-20\n\nIf the data is not in memory, it will be retrieved from the disk instead. We need an efficient\nway to find out which SSTable contains the key. Bloom filter [10] is commonly used to solve\nthis problem.\n\nThe read path is shown in Figure 6-21 when data is not in memory.\n\nServer\n\n@\n\nRead request\n\nMemory cache\n\nReturn result MEMORY\n\nDISK\n\nResult data\n\nBloom filter\n\nSSTables\n\nFigure 6-21\n1. The system first checks if data is in memory. If not, go to step 2.\n2. If data is not in memory, the system checks the bloom filter."
    },
    {
      "page": 107,
      "text": "3. The bloom filter is used to figure out which SSTables might contain the key.\n4. SSTables return the result of the data set.\n5. The result of the data set is returned to the client."
    },
    {
      "page": 108,
      "text": "Summary\n\nThis chapter covers many concepts and techniques. To refresh your memory, the following\ntable summarizes features and corresponding techniques used for a distributed key-value\nstore.\n\nGoal/Problems Technique\n\nAbility to store big data Use consistent hashing to spread the load\nacross servers\n\nHigh availability reads Data replication\nMulti-data center setup\n\nHighly available writes Versioning and conflict resolution with\nvector clocks\n\nDataset partition Consistent Hashing\n\nIncremental scalability Consistent Hashing\n\nHeterogeneity Consistent Hashing\n\nTunable consistency Quorum consensus\n\nHandling temporary failures Sloppy quorum and hinted handoff\nHandling permanent failures Merkle tree\n\nHandling data center outage Cross-data center replication\n\nTable 6-2"
    },
    {
      "page": 109,
      "text": "Reference materials\n\n[1] Amazon DynamoDB: https://aws.amazon.com/dynamodb/\n\n[2] memcached: https://memcached.org/\n\n[3] Redis: https://redis.io/\n[4] Dynamo: Amazon’s Highly Available Key-value Store:\nhttps://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\n\n[5] Cassandra: https://cassandra.apache.org/\n\n[6] Bigtable: A Distributed Storage System for Structured Data:\nhttps://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-\nosdi06.pdf\n\n[7] Merkle tree: https://en.wikipedia.org/wiki/Merkle tree\n\n[8] Cassandra architecture: https://cassandra.apache.org/doc/latest/architecture/\n[9] SStable: https://www.igvita.com/2012/02/06/sstable-and-log-structured-storage-leveldb/\n[10] Bloom filter https://en.wikipedia.org/wiki/Bloom filter"
    },
    {
      "page": 110,
      "text": "CHAPTER 7: DESIGN A UNIQUE ID GENERATOR IN\nDISTRIBUTED SYSTEMS\nIn this chapter, you are asked to design a unique ID generator in distributed systems. Your\nfirst thought might be to use a primary key with the auto_increment attribute in a traditional\ndatabase. However, auto_increment does not work in a distributed environment because a\nsingle database server is not large enough and generating unique IDs across multiple\ndatabases with minimal delay is challenging.\n\nHere are a few examples of unique IDs:\n\nhas == +\n| user_id |\nRRR PERS RR BRAS RES SESS +\n| 1227238262110117894 |\nhn wm +\n| 1241107244890099715 |\nhn so im +\n| 1243643959492173824 |\nqpe eee soseeoeos aoe esoess +\n| 1247686501489692673 |\npee BB OS oS SSeS Sehes= +\n| 156798176607545344e |\n+---------------------- +\n\nFigure 7-1"
    },
    {
      "page": 111,
      "text": "Step 1 - Understand the problem and establish design scope\nAsking clarification questions is the first step to tackle any system design interview question.\nHere is an example of candidate-interviewer interaction:\n\nCandidate: What are the characteristics of unique IDs?\nInterviewer: [Ds must be unique and sortable.\n\nCandidate: For each new record, does ID increment by 1?\nInterviewer: The ID increments by time but not necessarily only increments by 1. IDs\ncreated in the evening are larger than those created in the morning on the same day.\n\nCandidate: Do IDs only contain numerical values?\nInterviewer: Yes, that is correct.\n\nCandidate: What is the ID length requirement?\nInterviewer: IDs should fit into 64-bit.\n\nCandidate: What is the scale of the system?\nInterviewer: The system should be able to generate 10,000 IDs per second.\n\nAbove are some of the sample questions that you can ask your interviewer. It is important to\nunderstand the requirements and clarify ambiguities. For this interview question, the\nrequirements are listed as follows:\n\n¢ IDs must be unique.\n\n¢ IDs are numerical values only.\n\n+ IDs fit into 64-bit.\n\n¢ IDs are ordered by date.\n\nAbility to generate over 10,000 unique IDs per second."
    },
    {
      "page": 112,
      "text": "Step 2 - Propose high-level design and get buy-in\n\nMultiple options can be used to generate unique IDs in distributed systems. The options we\nconsidered are:\n\n* Multi-master replication\n\n* Universally unique identifier (UUID)\n* Ticket server\n\n* Twitter snowflake approach\n\nLet us look at each of them, how they work, and the pros/cons of each option.\n\nMulti-master replication\nAs shown in Figure 7-2, the first approach is multi-master replication.\n\n“\n¢\n\nWeb servers\n\nese e eee eee eee\n\nFigure 7-2\n\nThis approach uses the databases’ auto_increment feature. Instead of increasing the next ID\nby 1, we increase it by k, where k is the number of database servers in use. As illustrated in\nFigure 7-2, next ID to be generated is equal to the previous ID in the same server plus 2. This\nsolves some scalability issues because IDs can scale with the number of database servers.\nHowever, this strategy has some major drawbacks:\n\n* Hard to scale with multiple data centers\n* IDs do not go up with time across multiple servers.\n* It does not scale well when a server is added or removed.\n\nUUID\n\nA UUID is another easy way to obtain unique IDs. UUID is a 128-bit number used to identify\ninformation in computer systems. UUID has a very low probability of getting collusion.\nQuoted from Wikipedia, “after generating 1 billion UUIDs every second for approximately\n100 years would the probability of creating a single duplicate reach 50%” [1].\n\nHere is an example of UUID: 09c93e62-50b4-468d-bf8a-c07e1040bfb2. UUIDs can be\ngenerated independently without coordination between servers. Figure 7-3 presents the\nUUIDs design."
    },
    {
      "page": 113,
      "text": "ie)\n@Q\n©\n=\n\nFigure 7-3\nIn this design, each web server contains an ID generator, and a web server is responsible for\ngenerating IDs independently.\nPros:\n\n* Generating UUID is simple. No coordination between servers is needed so there will not\nbe any synchronization issues.\n\n* The system is easy to scale because each web server is responsible for generating IDs\nthey consume. ID generator can easily scale with web servers.\n\nCons:\n¢ IDs are 128 bits long, but our requirement is 64 bits.\n* IDs do not go up with time.\n¢ IDs could be non-numeric.\n\nTicket Server\n\nTicket servers are another interesting way to generate unique IDs. Flicker developed ticket\nservers to generate distributed primary keys [2]. It is worth mentioning how the system\nworks.\n\nWeb Sever Web Sever Web Sever Web Sever\n\nTicket Server\n\nFigure 7-4\nThe idea is to use a centralized auto_increment feature in a single database server (Ticket\nServer). To learn more about this, refer to flicker’s engineering blog article [2].\nPros:\n¢ Numeric IDs.\n* It is easy to implement, and it works for small to medium-scale applications.\n\nCons:"
    },
    {
      "page": 114,
      "text": "* Single point of failure. Single ticket server means if the ticket server goes down, all\nsystems that depend on it will face issues. To avoid a single point of failure, we can set up\nmultiple ticket servers. However, this will introduce new challenges such as data\nsynchronization.\n\nTwitter snowflake approach\n\nApproaches mentioned above give us some ideas about how different ID generation systems\nwork. However, none of them meet our specific requirements; thus, we need another\napproach. Twitter’s unique ID generation system called “snowflake” [3] is inspiring and can\nsatisfy our requirements.\n\nDivide and conquer is our friend. Instead of generating an ID directly, we divide an ID into\ndifferent sections. Figure 7-5 shows the layout of a 64-bit ID.\n\n1 bit 41 bits 5 bits 5 bits 12 bits\n(e) timestamp datacenter ID) machine ID sequence number\nFigure 7-5\n\nEach section is explained below.\n\n* Sign bit: 1 bit. It will always be 0. This is reserved for future uses. It can potentially be\nused to distinguish between signed and unsigned numbers.\n\n« Timestamp: 41 bits. Milliseconds since the epoch or custom epoch. We use Twitter\nsnowflake default epoch 1288834974657, equivalent to Nov 04, 2010, 01:42:54 UTC.\n\n* Datacenter ID: 5 bits, which gives us 2 \\5 = 32 datacenters.\n* Machine ID: 5 bits, which gives us 2 \\5 = 32 machines per datacenter.\n\n* Sequence number: 12 bits. For every ID generated on that machine/process, the sequence\nnumber is incremented by 1. The number is reset to 0 every millisecond."
    },
    {
      "page": 115,
      "text": "Step 3 - Design deep dive\n\nIn the high-level design, we discussed various options to design a unique ID generator in\ndistributed systems. We settle on an approach that is based on the Twitter snowflake ID\ngenerator. Let us dive deep into the design. To refresh our memory, the design diagram is\nrelisted below.\n\n1 bit 41 bits 5 bits 5 bits 12 bits\n(e) timestamp datacenter ID) machine ID sequence number\nFigure 7-6\n\nDatacenter IDs and machine IDs are chosen at the startup time, generally fixed once the\nsystem is up running. Any changes in datacenter IDs and machine IDs require careful review\nsince an accidental change in those values can lead to ID conflicts. Timestamp and sequence\nnumbers are generated when the ID generator is running.\n\nTimestamp\n\nThe most important 41 bits make up the timestamp section. As timestamps grow with time,\nIDs are sortable by time. Figure 7-7 shows an example of how binary representation is\nconverted to UTC. You can also convert UTC back to binary representation using a similar\nmethod.\n\n0-00100010101001011010011011000101101011000-01010-01100-000000000000\n\nto decimal\n\nv\n297616116568\n\n+ Twitter epoch 1288834974657\n\nv\n1586451091225\n\nconvert milliseconds to UTC time\n\nv\nApr 09 2020 16:51:31UTC\n\nFigure 7-7"
    },
    {
      "page": 116,
      "text": "The maximum timestamp that can be represented in 41 bits is\n2 N41 - 1 = 2199023255551 milliseconds (ms), which gives us: ~ 69 years =\n\n2199023255551 ms / 1000 seconds / 365 days / 24 hours/ 3600 seconds. This means the ID\ngenerator will work for 69 years and having a custom epoch time close to today’s date delays\nthe overflow time. After 69 years, we will need a new epoch time or adopt other techniques\nto migrate IDs.\n\nSequence number\n\nSequence number is 12 bits, which give us 2 4 12 = 4096 combinations. This field is 0 unless\nmore than one ID is generated in a millisecond on the same server. In theory, a machine can\nsupport a maximum of 4096 new IDs per millisecond."
    },
    {
      "page": 117,
      "text": "Step 4 - Wrap up\n\nIn this chapter, we discussed different approaches to design a unique ID generator: multi-\nmaster replication, UUID, ticket server, and Twitter snowflake-like unique ID generator. We\nsettle on snowflake as it supports all our use cases and is scalable in a distributed\nenvironment.\n\nIf there is extra time at the end of the interview, here are a few additional talking points:\n* Clock synchronization. In our design, we assume ID generation servers have the same\nclock. This assumption might not be true when a server is running on multiple cores. The\nsame challenge exists in multi-machine scenarios. Solutions to clock synchronization are\nout of the scope of this book; however, it is important to understand the problem exists.\nNetwork Time Protocol is the most popular solution to this problem. For interested\nreaders, refer to the reference material [4].\n* Section length tuning. For example, fewer sequence numbers but more timestamp bits\nare effective for low concurrency and long-term applications.\n\n* High availability. Since an ID generator is a mission-critical system, it must be highly\navailable.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 118,
      "text": "Reference materials\n\n[1] Universally unique identifier: https://en.wikipedia.org/wiki/Universally unique identifier\n\n[2] Ticket Servers: Distributed Unique Primary Keys on the Cheap:\nhttps://code. flickr.net/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-\ncheap/\n\n[3] Announcing Snowflake: https://blog.twitter.com/engineering/en_us/a/2010/announcing-\nsnowflake.html\n\n[4] Network time protocol: https://en.wikipedia.org/wiki/Network Time Protocol"
    },
    {
      "page": 119,
      "text": "CHAPTER 8: DESIGN A URL SHORTENER\n\nIn this chapter, we will tackle an interesting and classic system design interview question:\ndesigning a URL shortening service like tinyurl."
    },
    {
      "page": 120,
      "text": "Step 1 - Understand the problem and establish design scope\nSystem design interview questions are intentionally left open-ended. To design a well-crafted\nsystem, it is critical to ask clarification questions.\n\nCandidate: Can you give an example of how a URL shortener work?\n\nInterviewer: Assume URL\nhttps://www.systeminterview.com/q=chatsystem&c=loggedin&v=v3&l=long is the original\nURL. Your service creates an alias with shorter length: https://tinyurl.com/ y7keocwj. If you\nclick the alias, it redirects you to the original URL.\n\nCandidate: What is the traffic volume?\n\nInterviewer: 100 million URLs are generated per day.\nCandidate: How long is the shortened URL?\nInterviewer: As short as possible.\n\nCandidate: What characters are allowed in the shortened URL?\nInterviewer: Shortened URL can be a combination of numbers (0-9) and characters (a-z, A-\nZ).\nCandidate: Can shortened URLs be deleted or updated?\nInterviewer: For simplicity, let us assume shortened URLs cannot be deleted or updated.\nHere are the basic use cases:\n1.URL shortening: given a long URL => return a much shorter URL\n2.URL redirecting: given a shorter URL => redirect to the original URL\n3.High availability, scalability, and fault tolerance considerations\nBack of the envelope estimation\n¢ Write operation: 100 million URLs are generated per day.\n\n« Write operation per second: 100 million / 24 /3600 = 1160\n\n* Read operation: Assuming ratio of read operation to write operation is 10:1, read\noperation per second: 1160 * 10 = 11,600\n\n* Assuming the URL shortener service will run for 10 years, this means we must support\n100 million * 365 * 10 = 365 billion records.\n\n« Assume average URL length is 100.\n* Storage requirement over 10 years: 365 billion * 100 bytes * 10 years = 365 TB\n\nIt is important for you to walk through the assumptions and calculations with your\ninterviewer so that both of you are on the same page."
    },
    {
      "page": 121,
      "text": "Step 2 - Propose high-level design and get buy-in\nIn this section, we discuss the API endpoints, URL redirecting, and URL shortening flows.\n\nAPI Endpoints\n\nAPI endpoints facilitate the communication between clients and servers. We will design the\nAPIs REST-style. If you are unfamiliar with restful API, you can consult external materials,\nsuch as the one in the reference material [1]. A URL shortener primary needs two API\nendpoints.\n\n1.URL shortening. To create a new short URL, a client sends a POST request, which contains\none parameter: the original long URL. The API looks like this:\n\nPOST api/v1/data/shorten\n* request parameter: {longUrl: longURLString}\n* return shortURL\n2.URL redirecting. To redirect a short URL to the corresponding long URL, a client sends a\nGET request. The API looks like this:\nGET api/v1/shortUrl\n* Return longURL for HTTP redirection\n\nURL redirecting\n\nFigure 8-1 shows what happens when you enter a tinyurl onto the browser. Once the server\nreceives a tinyurl request, it changes the short URL to the long URL with 301 redirect.\n\nRequest URL: https://tinyurl.com/qtj5opu\n\nRequest Method: GET\nStatus Code: © 3@1\nRemote Address: [2606:4700:10: :6814:391e]:443\n\nReferrer Policy: no-referrer-when-downgrade\n\nv Response Headers\nalt-sve: h3-27=\":443\"; ma=86400, h3-25=\":443\"; ma=8640@, h3-24=\":443\"; ma=86400, h3-23=\":443\"; ma=86400\ncache-control: max-age=0, no-cache, private\ncf-cache-status: DYNAMIC\ncf-ray: 581fbd8ac986ed33-SIC\ncontent-type: text/html; charset=UTF-8\ndate: Fri, 10 Apr 202 22:00:23 GMT\n\nexpect-ct: max-age=604800, report-uri=\"https://report-uri.cloudflare.com/cdn-cgi/beacon/expect-ct\"\n\n: https: //www.amazon.com/dp/B@17V4NTFA? pLink=63eaef76-979c-4d&ref=adblp13nvvxx_@_2_im\n\nFigure 8-1\n\nThe detailed communication between clients and servers is shown in Figure 8-2."
    },
    {
      "page": 122,
      "text": "short URL: https://tinyurl.com/qtj5opu\n\nlong URL: https:/Avww.amazon.com/dp/B017V4NTFA?pLink=63eaef76-979c-4d&\nref=adblp13nvvxx_0_2_im\n\nO\n\nClient\n\nVisit Short URL\n\nstatus code: 304 tinyurl server\n\nJocation: !ong URL\n\nVisit\nOn\ng\n\nAmazon server\n\nFigure 8-2\nOne thing worth discussing here is 301 redirect vs 302 redirect.\n\n301 redirect. A 301 redirect shows that the requested URL is “permanently” moved to the\nlong URL. Since it is permanently redirected, the browser caches the response, and\nsubsequent requests for the same URL will not be sent to the URL shortening service.\nInstead, requests are redirected to the long URL server directly.\n\n302 redirect. A 302 redirect means that the URL is “temporarily” moved to the long URL,\nmeaning that subsequent requests for the same URL will be sent to the URL shortening\nservice first. Then, they are redirected to the long URL server.\n\nEach redirection method has its pros and cons. If the priority is to reduce the server load,\nusing 301 redirect makes sense as only the first request of the same URL is sent to URL\nshortening servers. However, if analytics is important, 302 redirect is a better choice as it can\ntrack click rate and source of the click more easily.\n\nThe most intuitive way to implement URL redirecting is to use hash tables. Assuming the\nhash table stores <shortURL, longURL> pairs, URL redirecting can be implemented by the\nfollowing:\n\n* Get longURL: longURL = hashTable.get(shortURL)\n\n* Once you get the longURL, perform the URL redirect.\n\nURL shortening\n\nLet us assume the short URL looks like this: www.tinyurl.com/{hashValue}. To support the\nURL shortening use case, we must find a hash function fx that maps a long URL to the\nhashValue, as shown in Figure 8-3."
    },
    {
      "page": 123,
      "text": "oem} 1 fi\n\nhash\n\nhttps://tinyurl.com/|qtj5opu\n\nFigure 8-3\nThe hash function must satisfy the following requirements:\n* Each longURL must be hashed to one hashValue.\n* Each hashValue can be mapped back to the longURL.\n\nDetailed design for the hash function is discussed in deep dive."
    },
    {
      "page": 124,
      "text": "Step 3 - Design deep dive\n\nUp until now, we have discussed the high-level design of URL shortening and URL\nredirecting. In this section, we dive deep into the following: data model, hash function, URL\nshortening and URL redirecting.\n\nData model\n\nIn the high-level design, everything is stored in a hash table. This is a good starting point;\nhowever, this approach is not feasible for real-world systems as memory resources are limited\nand expensive. A better option is to store <shortURL, longURL> mapping in a relational\ndatabase. Figure 8-4 shows a simple database table design. The simplified version of the table\ncontains 3 columns: id, shortURL, longURL.\n\nid (auto increment)\nshorttURL\nlongURL\n\nFigure 8-4\n\nHash function\nHash function is used to hash a long URL to a short URL, also known as hashValue.\n\nHash value length\n\nThe hashValue consists of characters from [0-9, a-z, A-Z], containing 10 + 26 + 26 = 62\npossible characters. To figure out the length of hashValue, find the smallest n such that 62/n\n> 365 billion. The system must support up to 365 billion URLs based on the back of the\nenvelope estimation. Table 8-1 shows the length of hashValue and the corresponding\nmaximal number of URLs it can support."
    },
    {
      "page": 125,
      "text": "N Maximal number of URLs\n\n1 62/1 = 62\n\n2 6242 = 3,844\n\n8 6243 = 238,328\n\n4 624 4 = 14,776,336\n\n5 6245 = 916,132,832\n\n6 6246 = 56,800,235,584\n\naA 6247 = 3,521,614,606,208 = ~3.5 trillion\n\nTable 8-1\n\nWhen n = 7, 62 \\n = ~3.5 trillion, 3.5 trillion is more than enough to hold 365 billion URLs,\nso the length of hashValue is 7.\n\nWe will explore two types of hash functions for a URL shortener. The first one is “hash +\ncollision resolution”, and the second one is “base 62 conversion.” Let us look at them one by\none.\n\nHash + collision resolution\n\nTo shorten a long URL, we should implement a hash function that hashes a long URL to a 7-\ncharacter string. A straightforward solution is to use well-known hash functions like CRC32,\nMDS, or SHA-1. The following table compares the hash results after applying different hash\nfunctions on this URL: https://en.wikipedia.org/wiki/Systems_design.\n\nHash function Hash value (Hexadecimal)\n\nCRC32 Scb54054\n\nMDS 5a62509a84df9ee03fe1 2Z30b9df8b84e\n\nSHA-1 Qeeae7916c0685390 1d9ccbefbfcaf4de57ed85b\nTable 8-2\n\nAs shown in Table 8-2, even the shortest hash value (from CRC32) is too long (more than 7\ncharacters). How can we make it shorter?\n\nThe first approach is to collect the first 7 characters of a hash value; however, this method\ncan lead to hash collisions. To resolve hash collisions, we can recursively append a new\npredefined string until no more collision is discovered. This process is explained in Figure 8-"
    },
    {
      "page": 126,
      "text": "input: longURL\n\nexist in DB?\n\nlongURL +\npredefined string\n\nhas collision\n\nno\n\nFigure 8-5\n\nThis method can eliminate collision; however, it is expensive to query the database to check\nif a shortURL exists for every request. A technique called bloom filters [2] can improve\nperformance. A bloom filter is a space-efficient probabilistic technique to test if an element is\na member of a set. Refer to the reference material [2] for more details.\n\nBase 62 conversion\n\nBase conversion is another approach commonly used for URL shorteners. Base conversion\nhelps to convert the same number between its different number representation systems. Base\n62 conversion is used as there are 62 possible characters for hashValue. Let us use an\nexample to explain how the conversion works: convert 11157,, to base 62 representation\n\n(11157,, represents 11157 in a base 10 system).\n\n+ From its name, base 62 is a way of using 62 characters for encoding. The mappings are:\n0-0, ..., 9-9, 10-a, 11-b, ..., 35-z, 36-A, ..., 61-Z, where ‘a’ stands for 10, ‘Z’ stands for 61,\netc.\n\n° 11157, =2 x 62°+ 55 x 62'+ 59 x 62° = [2, 55, 59] -> [2, T, X] in base 62\n\nrepresentation. Figure 8-6 shows the conversation process."
    },
    {
      "page": 127,
      "text": "\\ 8\n\nR\n\nemainder\n\n62 11157 59\n62 | 179 55\n62 | 2 2\n\nRepresentation in base 62\n\nXx\n\nFigure 8-6\n¢ Thus, the short URL is https://tinyurl.com /2TX\n\nComparison of the two approaches\n\nTable 8-3 shows the differences of the two approaches.\n\nHash + collision resolution\n\nBase 62 conversion\n\nFixed short URL length.\n\nThe short URL length is not fixed. It goes\nup with the ID.\n\nIt does not need a unique ID generator.\n\nThis option depends on a unique ID\ngenerator.\n\nCollision is possible and must be\nresolved.\n\nCollision is impossible because ID is\nunique.\n\nIt is impossible to figure out the next\navailable short URL because it does not\ndepend on ID.\n\nIt is easy to figure out the next available\nshort URL if ID increments by 1 for a new\nentry. This can be a security concern.\n\nTable 8-3\n\nURL shortening deep dive\n\nAs one of the core pieces of the system, we want the URL shortening flow to be logically\nsimple and functional. Base 62 conversion is used in our design. We build the following\n\ndiagram (Figure 8-7) to demonstrate the flow."
    },
    {
      "page": 128,
      "text": "2. longURL in DB?\n\n1. input: longURL\n\nye 3. return shortURL\n\n4. Generate a\nnew ID\n\n5. Convert ID to\nshortURL\n\n6. Save ID,\nshortURL,\nlongURL in DB\n\nFigure 8-7\n1. longURL is the input.\n2. The system checks if the longURL is in the database.\n3. If it is, it means the longURL was converted to shortURL before. In this case, fetch the\nshortURL from the database and return it to the client.\n4. If not, the longURL is new. A new unique ID (primary key) Is generated by the unique\nID generator.\n5. Convert the ID to shortURL with base 62 conversion.\n6. Create a new database row with the ID, shortURL, and longURL.\nTo make the flow easier to understand, let us look at a concrete example.\n« Assuming the input longURL is: https://en.wikipedia.org/wiki/Systems_design\n¢ Unique ID generator returns ID: 2009215674938.\n* Convert the ID to shortURL using the base 62 conversion. ID (2009215674938) is\n\nconverted to “zn9edcu”.\n* Save ID, shortURL, and longURL to the database as shown in Table 8-4.\n\nid shortURL longURL\n\n2009215674938 zn9edcu https://en.wikipedia.org/wiki/Systems_design\n\nTable 8-4\n\nThe distributed unique ID generator is worth mentioning. Its primary function is to generate\nglobally unique IDs, which are used for creating shortURLs. In a highly distributed"
    },
    {
      "page": 129,
      "text": "environment, implementing a unique ID generator is challenging. Luckily, we have already\ndiscussed a few solutions in “Chapter 7: Design A Unique ID Generator in Distributed\nSystems”. You can refer back to it to refresh your memory.\n\nURL redirecting deep dive\n\nFigure 8-8 shows the detailed design of the URL redirecting. As there are more reads than\nwrites, <shortURL, longURL> mapping is stored in a cache to improve performance.\n\n@ GET https://tinyurl.com/zn9edcu z\n\n“5 moma E\n\nReturn long URL: ‘ Web servers +\n\n® https://en.wikipedia.org/wiki/Systems_design Load balancer\n\nDatabase\n\nFigure 8-8\nThe flow of URL redirecting is summarized as follows:\n1. A user clicks a short URL link: https://tinyurl.com/zn9edcu\n2. The load balancer forwards the request to web servers.\n3. If a shortURL is already in the cache, return the longURL directly.\n\n4. If a shortURL is not in the cache, fetch the longURL from the database. If it is not in the\ndatabase, it is likely a user entered an invalid shortURL.\n\n5. The longURL is returned to the user."
    },
    {
      "page": 130,
      "text": "Step 4 - Wrap up\nIn this chapter, we talked about the API design, data model, hash function, URL shortening,\nand URL redirecting.\n\nIf there is extra time at the end of the interview, here are a few additional talking points.\n\n* Rate limiter: A potential security problem we could face is that malicious users send an\noverwhelmingly large number of URL shortening requests. Rate limiter helps to filter out\nrequests based on IP address or other filtering rules. If you want to refresh your memory\nabout rate limiting, refer to “Chapter 4: Design a rate limiter”.\n\n« Web server scaling: Since the web tier is stateless, it is easy to scale the web tier by\nadding or removing web servers.\n\n* Database scaling: Database replication and sharding are common techniques.\n\n* Analytics: Data is increasingly important for business success. Integrating an analytics\nsolution to the URL shortener could help to answer important questions like how many\npeople click on a link? When do they click the link? etc.\n\n+ Availability, consistency, and reliability. These concepts are at the core of any large\nsystem’s success. We discussed them in detail in Chapter 1, please refresh your memory\non these topics.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 131,
      "text": "Reference materials\n\n[1] A RESTful Tutorial: https://www.restapitutorial.com/index.html\n[2] Bloom filter: https://en.wikipedia.org/wiki/Bloom_filter"
    },
    {
      "page": 132,
      "text": "CHAPTER 9: DESIGN A WEB CRAWLER\n\nIn this chapter, we focus on web crawler design: an interesting and classic system design\ninterview question.\n\nA web crawler is known as a robot or spider. It is widely used by search engines to discover\nnew or updated content on the web. Content can be a web page, an image, a video, a PDF\nfile, etc. A web crawler starts by collecting a few web pages and then follows links on those\npages to collect new content. Figure 9-1 shows a visual example of the crawl process.\n\nlime.com page\n\nwww.lime.com\nwww.peach.com\nWww.mango.com\n\npeach.com page\n\n=\n\nmango.com page\n\na.com page\n\nWwww.banana.com\n—__ >\n\nbanana.com page\n\nwww.a.com\n\nwww.b.com\n\nwww.c.com\n\nb.com page\n\neee\nwww.orange.com\nWww.plum.com\n\norange.com page\n\nc.com page\n\nplum.com page\n\nFigure 9-1\n\nA crawler is used for many purposes:\n\n* Search engine indexing: This is the most common use case. A crawler collects web\n\npages to create a local index for search engines. For example, Googlebot is the web\ncrawler behind the Google search engine.\n\n* Web archiving: This is the process of collecting information from the web to preserve\ndata for future uses. For instance, many national libraries run crawlers to archive web\nsites. Notable examples are the US Library of Congress [1] and the EU web archive [2].\n\n* Web mining: The explosive growth of the web presents an unprecedented opportunity for"
    },
    {
      "page": 133,
      "text": "data mining. Web mining helps to discover useful knowledge from the internet. For\nexample, top financial firms use crawlers to download shareholder meetings and annual\nreports to learn key company initiatives.\n\n« Web monitoring. The crawlers help to monitor copyright and trademark infringements\nover the Internet. For example, Digimarc [3] utilizes crawlers to discover pirated works\nand reports.\n\nThe complexity of developing a web crawler depends on the scale we intend to support. It\ncould be either a small school project, which takes only a few hours to complete or a gigantic\nproject that requires continuous improvement from a dedicated engineering team. Thus, we\nwill explore the scale and features to support below."
    },
    {
      "page": 134,
      "text": "Step 1 - Understand the problem and establish design scope\n\nThe basic algorithm of a web crawler is simple:\n1. Given a set of URLs, download all the web pages addressed by the URLs.\n2. Extract URLs from these web pages\n3. Add new URLs to the list of URLs to be downloaded. Repeat these 3 steps.\n\nDoes a web crawler work truly as simple as this basic algorithm? Not exactly. Designing a\nvastly scalable web crawler is an extremely complex task. It is unlikely for anyone to design\na massive web crawler within the interview duration. Before jumping into the design, we\nmust ask questions to understand the requirements and establish design scope:\n\nCandidate: What is the main purpose of the crawler? Is it used for search engine indexing,\ndata mining, or something else?\nInterviewer: Search engine indexing.\n\nCandidate: How many web pages does the web crawler collect per month?\nInterviewer: 1 billion pages.\n\nCandidate: What content types are included? HTML only or other content types such as\nPDFs and images as well?\nInterviewer: HTML only.\n\nCandidate: Shall we consider newly added or edited web pages?\nInterviewer: Yes, we should consider the newly added or edited web pages.\n\nCandidate: Do we need to store HTML pages crawled from the web?\nInterviewer: Yes, up to 5 years\n\nCandidate: How do we handle web pages with duplicate content?\nInterviewer: Pages with duplicate content should be ignored.\n\nAbove are some of the sample questions that you can ask your interviewer. It is important to\nunderstand the requirements and clarify ambiguities. Even if you are asked to design a\nstraightforward product like a web crawler, you and your interviewer might not have the\nsame assumptions.\nBeside functionalities to clarify with your interviewer, it is also important to note down the\nfollowing characteristics of a good web crawler:\n* Scalability: The web is very large. There are billions of web pages out there. Web\ncrawling should be extremely efficient using parallelization.\n* Robustness: The web is full of traps. Bad HTML, unresponsive servers, crashes,\nmalicious links, etc. are all common. The crawler must handle all those edge cases.\n\n* Politeness: The crawler should not make too many requests to a website within a short\ntime interval.\n\n+ Extensibility: The system is flexible so that minimal changes are needed to support new\ncontent types. For example, if we want to crawl image files in the future, we should not\nneed to redesign the entire system.\n\nBack of the envelope estimation\n\nThe following estimations are based on many assumptions, and it is important to\ncommunicate with the interviewer to be on the same page.\n\n« Assume 1 billion web pages are downloaded every month."
    },
    {
      "page": 135,
      "text": "* QPS: 1,000,000,000 / 30 days / 24 hours / 3600 seconds = ~400 pages per second.\n\n* Peak QPS = 2 * QPS = 800\n\n« Assume the average web page size is 500k.\n\n* 1-billion-page x 500k = 500 TB storage per month. If you are unclear about digital\nstorage units, go through “Power of 2” section in Chapter 2 again.\n\n« Assuming data are stored for five years, 500 TB * 12 months * 5 years = 30 PB. A 30 PB\nstorage is needed to store five-year content."
    },
    {
      "page": 136,
      "text": "Step 2 - Propose high-level design and get buy-in\n\nOnce the requirements are clear, we move on to the high-level design. Inspired by previous\nstudies on web crawling [4] [5], we propose a high-level design as shown in Figure 9-2.\n\nDNS Resolver\n\nContent\nStorage\n| seed URLs ie URL Frontier ml HTML Downloader 4 Content Parser 4 Content Seen?\nLy\nLink Extractor\nURL Filter\nURL Seen?\n\nURL\nStorage\nFigure 9-2\n\nFirst, we explore each design component to understand their functionalities. Then, we\nexamine the crawler workflow step-by-step.\nSeed URLs\n\nA web crawler uses seed URLs as a starting point for the crawl process. For example, to\ncrawl all web pages from a university’s website, an intuitive way to select seed URLs is to\nuse the university’s domain name.\n\nv\n\nTo crawl the entire web, we need to be creative in selecting seed URLs. A good seed URL\nserves as a good starting point that a crawler can utilize to traverse as many links as possible.\nThe general strategy is to divide the entire URL space into smaller ones. The first proposed\napproach is based on locality as different countries may have different popular websites.\nAnother way is to choose seed URLs based on topics; for example, we can divide URL space\ninto shopping, sports, healthcare, etc. Seed URL selection is an open-ended question. You are\nnot expected to give the perfect answer. Just think out loud.\n\nURL Frontier\n\nMost modern web crawlers split the crawl state into two: to be downloaded and already\ndownloaded. The component that stores URLs to be downloaded is called the URL Frontier.\nYou can refer to this as a First-in-First-out (FIFO) queue. For detailed information about the\nURL Frontier, refer to the deep dive."
    },
    {
      "page": 137,
      "text": "HTML Downloader\n\nThe HTML downloader downloads web pages from the internet. Those URLs are provided\nby the URL Frontier.\n\nDNS Resolver\nTo download a web page, a URL must be translated into an IP address. The HTML\n\nDownloader calls the DNS Resolver to get the corresponding IP address for the URL. For\ninstance, URL www.wikipedia.org is converted to IP address 198.35.26.96 as of 3/5/2019.\n\nContent Parser\n\nAfter a web page is downloaded, it must be parsed and validated because malformed web\npages could provoke problems and waste storage space. Implementing a content parser in a\ncrawl server will slow down the crawling process. Thus, the content parser is a separate\ncomponent.\n\nContent Seen?\n\nOnline research [6] reveals that 29% of the web pages are duplicated contents, which may\ncause the same content to be stored multiple times. We introduce the “Content Seen?” data\nstructure to eliminate data redundancy and shorten processing time. It helps to detect new\ncontent previously stored in the system. To compare two HTML documents, we can compare\nthem character by character. However, this method is slow and time-consuming, especially\nwhen billions of web pages are involved. An efficient way to accomplish this task is to\ncompare the hash values of the two web pages [7].\n\nContent Storage\n\nIt is a storage system for storing HTML content. The choice of storage system depends on\nfactors such as data type, data size, access frequency, life span, etc. Both disk and memory\nare used.\n\n* Most of the content is stored on disk because the data set is too big to fit in memory.\n* Popular content is kept in memory to reduce latency.\nURL Extractor\n\nURL Extractor parses and extracts links from HTML pages. Figure 9-3 shows an example of\na link extraction process. Relative paths are converted to absolute URLs by adding the\n“https://en.wikipedia.org” prefix."
    },
    {
      "page": 138,
      "text": "<html class=\"client-nojs\" lang=\"en\" dirs\"1ltr\">\n<head>\n<meta charset=\"UTF-8\"/>\n<title>Wikipedia, the free encyclopedia</title>\n</head>\n<body>\n<li><a href=\"/wiki/Cong_Weixi\" title=\"Cong Weixi\">Cong Weixi</a></li>\n<li><a href=\"/wiki/Kay_Hagan\" title=\"Kay Hagan\">Kay Hagan</a></li>\n<li><a href=\"/wiki/Vladimir_Bukovsky\" title=\"Vladimir Bukovsky\">Vladimir Bukovsky</a></li>\n<li><a href=\"/wiki/John_Conyers\" title=\"John Conyers\">John Conyers</a></li>\n</body>\n</html>\n\nhttps://en.wikipedia.org/wiki/Cong_Weixi\n, [https://en.wikipedia.org/wiki/Kay_Hagan\nExtracted Links: me\nhttps://en.wikipedia.org/wiki/Vladimir_Bukovsky\nhttps://en.wikipedia.org/wiki/John_Conyers\n\nFigure 9-3\nURL Filter\nThe URL filter excludes certain content types, file extensions, error links and URLs in\n“blacklisted” sites.\nURL Seen?\n\n“URL Seen?” is a data structure that keeps track of URLs that are visited before or already in\nthe Frontier. “URL Seen?” helps to avoid adding the same URL multiple times as this can\nincrease server load and cause potential infinite loops.\n\nBloom filter and hash table are common techniques to implement the “URL Seen?”\ncomponent. We will not cover the detailed implementation of the bloom filter and hash table\nhere. For more information, refer to the reference materials [4] [8].\n\nURL Storage\n\nURL Storage stores already visited URLs.\n\nSo far, we have discussed every system component. Next, we put them together to explain the\nworkflow.\n\nWeb crawler workflow\n\nTo better explain the workflow step-by-step, sequence numbers are added in the design\ndiagram as shown in Figure 9-4."
    },
    {
      "page": 139,
      "text": "Content\n\nStorage\n\nDNS Resolver\n\n® ©\n\nVv\n\n| seed URLs 2, URL Frontier © HTML Downloader oJ Content Parser oJ Content Seen?\n\nA\n\nLink Extractor\n\nURL Filter\n\n@\n\nURL Seen?\n\ni\n\n=>\nStorage\n\nFigure 9-4\nStep 1: Add seed URLs to the URL Frontier\nStep 2: HTML Downloader fetches a list of URLs from URL Frontier.\n\nStep 3: HTML Downloader gets IP addresses of URLs from DNS resolver and starts\ndownloading.\n\nStep 4: Content Parser parses HTML pages and checks if pages are malformed.\nStep 5: After content is parsed and validated, it is passed to the “Content Seen?” component.\nStep 6: “Content Seen” component checks if a HTML page is already in the storage.\n\n+ If it is in the storage, this means the same content in a different URL has already been\nprocessed. In this case, the HTML page is discarded.\n\n+ If it is not in the storage, the system has not processed the same content before. The\ncontent is passed to Link Extractor.\n\nStep 7: Link extractor extracts links from HTML pages.\nStep 8: Extracted links are passed to the URL filter.\nStep 9: After links are filtered, they are passed to the “URL Seen?” component.\n\nStep 10: “URL Seen” component checks if a URL is already in the storage, if yes, it is\nprocessed before, and nothing needs to be done.\n\nStep 11: If a URL has not been processed before, it is added to the URL Frontier."
    },
    {
      "page": 140,
      "text": "Step 3 - Design deep dive\nUp until now, we have discussed the high-level design. Next, we will discuss the most\nimportant building components and techniques in depth:\n\n* Depth-first search (DFS) vs Breadth-first search (BFS)\n\n* URL frontier\n\n* HTML Downloader\n\n* Robustness\n\n¢ Extensibility\n\n* Detect and avoid problematic content\n\nDES vs BFS\n\nYou can think of the web as a directed graph where web pages serve as nodes and hyperlinks\n(URLs) as edges. The crawl process can be seen as traversing a directed graph from one web\npage to others. Two common graph traversal algorithms are DFS and BFS. However, DFS is\nusually not a good choice because the depth of DFS can be very deep.\n\nBFS is commonly used by web crawlers and is implemented by a first-in-first-out (FIFO)\nqueue. In a FIFO queue, URLs are dequeued in the order they are enqueued. However, this\nimplementation has two problems:\n* Most links from the same web page are linked back to the same host. In Figure 9-5, all\nthe links in wikipedia.com are internal links, making the crawler busy processing URLs\nfrom the same host (wikipedia.com). When the crawler tries to download web pages in\nparallel, Wikipedia servers will be flooded with requests. This is considered as “impolite”.\n\nwikipedia.com/page1/1\n\nwikipedia.com/page1\n\nikipedia.com/page1/2\n\nES\n\nwikipedia.com/page2/1\n\nwikipedia.com/page2\n\nwikipedia.com/page2/2\n\nwikipedia.com/pageN/1\n\nwikipedia.com/pageN\n\nwikipedia.com/pageN/2\n\nwikipedia.com/pageN/N\n\nFigure 9-5"
    },
    {
      "page": 141,
      "text": "* Standard BFS does not take the priority of a URL into consideration. The web is large\nand not every page has the same level of quality and importance. Therefore, we may want\nto prioritize URLs according to their page ranks, web traffic, update frequency, etc.\n\nURL frontier\n\nURL frontier helps to address these problems. A URL frontier is a data structure that stores\nURLs to be downloaded. The URL frontier is an important component to ensure politeness,\nURL prioritization, and freshness. A few noteworthy papers on URL frontier are mentioned\nin the reference materials [5] [9]. The findings from these papers are as follows:\n\nPoliteness\n\nGenerally, a web crawler should avoid sending too many requests to the same hosting server\nwithin a short period. Sending too many requests is considered as “impolite” or even treated\nas denial-of-service (DOS) attack. For example, without any constraint, the crawler can send\nthousands of requests every second to the same website. This can overwhelm the web\nservers.\n\nThe general idea of enforcing politeness is to download one page at a time from the same\nhost. A delay can be added between two download tasks. The politeness constraint is\nimplemented by maintain a mapping from website hostnames to download (worker) threads.\nEach downloader thread has a separate FIFO queue and only downloads URLs obtained from\nthat queue. Figure 9-6 shows the design that manages politeness.\n\nQueue router\n\nb1\n\nQueue selector\n\nWorker thread 1\n\nWorker thread 2\n\nFigure 9-6"
    },
    {
      "page": 142,
      "text": "* Queue router: It ensures that each queue (b1, b2, ... bn) only contains URLs from the\nsame host.\n\n* Mapping table: It maps each host to a queue.\n\nHost Queue\nwikipedia.com b1\napple.com b2\nnike.com bn\nTable 9-1\n\n* FIFO queues b1, b2 to bn: Each queue contains URLs from the same host.\n* Queue selector: Each worker thread is mapped to a FIFO queue, and it only downloads\nURLs from that queue. The queue selection logic is done by the Queue selector.\n¢ Worker thread 1 to N. A worker thread downloads web pages one by one from the same\nhost. A delay can be added between two download tasks.\nPriority\nA random post from a discussion forum about Apple products carries very different weight\nthan posts on the Apple home page. Even though they both have the “Apple” keyword, it is\nsensible for a crawler to crawl the Apple home page first.\n\nWe prioritize URLs based on usefulness, which can be measured by PageRank [10], website\ntraffic, update frequency, etc. “Prioritizer” is the component that handles URL prioritization.\nRefer to the reference materials [5] [10] for in-depth information about this concept.\n\nFigure 9-7 shows the design that manages URL priority."
    },
    {
      "page": 143,
      "text": "input URLs\n\nPrioritizer\n\nf1\n\nQueue selector\n\noutput URLs\nv\n\nFigure 9-7\n¢ Prioritizer: It takes URLs as input and computes the priorities.\n\n* Queue f1 to fn: Each queue has an assigned priority. Queues with high priority are\nselected with higher probability.\n\n* Queue selector: Randomly choose a queue with a bias towards queues with higher\npriority.\nFigure 9-8 presents the URL frontier design, and it contains two modules:\n¢ Front queues: manage prioritization\n* Back queues: manage politeness"
    },
    {
      "page": 144,
      "text": "| input URLs\n\nf1\n\nFront queue selector\n\noutput URLs\n\nBack queue router\n\nMapping Table\n\nBack queue selector\n\nWorker thread 1 Worker thread 2 Worker thread 3\n\nFigure 9-8\n\nFreshness\nWeb pages are constantly being added, deleted, and edited. A web crawler must periodically\n\nrecrawl downloaded pages to keep our data set fresh. Recrawl all the URLs is time-\nconsuming and resource intensive. Few strategies to optimize freshness are listed as follows:"
    },
    {
      "page": 145,
      "text": "* Recrawl based on web pages’ update history.\n* Prioritize URLs and recrawl important pages first and more frequently.\n\nStorage for URL Frontier\n\nIn real-world crawl for search engines, the number of URLs in the frontier could be hundreds\nof millions [4]. Putting everything in memory is neither durable nor scalable. Keeping\neverything in the disk is undesirable neither because the disk is slow; and it can easily\nbecome a bottleneck for the crawl.\n\nWe adopted a hybrid approach. The majority of URLs are stored on disk, so the storage space\nis not a problem. To reduce the cost of reading from the disk and writing to the disk, we\nmaintain buffers in memory for enqueue/dequeue operations. Data in the buffer is\nperiodically written to the disk.\n\nHTML Downloader\n\nThe HTML Downloader downloads web pages from the internet using the HTTP protocol.\nBefore discussing the HTML Downloader, we look at Robots Exclusion Protocol first.\n\nRobots.txt\n\nRobots.txt, called Robots Exclusion Protocol, is a standard used by websites to communicate\nwith crawlers. It specifies what pages crawlers are allowed to download. Before attempting to\ncrawl a web site, a crawler should check its corresponding robots.txt first and follow its rules.\n\nTo avoid repeat downloads of robots.txt file, we cache the results of the file. The file is\ndownloaded and saved to cache periodically. Here is a piece of robots.txt file taken from\nhttps://www.amazon.com/robots.txt. Some of the directories like creatorhub are disallowed\nfor Google bot.\n\nUser-agent: Googlebot\n\nDisallow: /creatorhub/*\n\nDisallow: /rss/people/*/reviews\nDisallow: /gp/pdp/rss/*/reviews\nDisallow: /gp/cdp/member-reviews/\nDisallow: /gp/aw/cr/\n\nBesides robots.txt, performance optimization is another important concept we will cover for\nthe HTML downloader.\n\nPerformance optimization\n\nBelow is a list of performance optimizations for HTML downloader.\n\n1. Distributed crawl\n\nTo achieve high performance, crawl jobs are distributed into multiple servers, and each server\nruns multiple threads. The URL space is partitioned into smaller pieces; so, each downloader\nis responsible for a subset of the URLs. Figure 9-9 shows an example of a distributed crawl."
    },
    {
      "page": 146,
      "text": "URL Frontier\n\ndistribute URLs\n\nOo;\nSoy, uy\nOR,\n<s\n\nHTML Downloaders\nFigure 9-9\n2. Cache DNS Resolver\nDNS Resolver is a bottleneck for crawlers because DNS requests might take time due to the\nsynchronous nature of many DNS interfaces. DNS response time ranges from 10ms to\n200ms. Once a request to DNS is carried out by a crawler thread, other threads are blocked\nuntil the first request is completed. Maintaining our DNS cache to avoid calling DNS\n\nfrequently is an effective technique for speed optimization. Our DNS cache keeps the domain\nname to IP address mapping and is updated periodically by cron jobs.\n\n3. Locality\n\nDistribute crawl servers geographically. When crawl servers are closer to website hosts,\ncrawlers experience faster download time. Design locality applies to most of the system\ncomponents: crawl servers, cache, queue, storage, etc.\n\n4. Short timeout\n\nSome web servers respond slowly or may not respond at all. To avoid long wait time, a\nmaximal wait time is specified. If a host does not respond within a predefined time, the\ncrawler will stop the job and crawl some other pages.\n\nRobustness\n\nBesides performance optimization, robustness is also an important consideration. We present\n\na few approaches to improve the system robustness:\n* Consistent hashing: This helps to distribute loads among downloaders. A new\ndownloader server can be added or removed using consistent hashing. Refer to Chapter 5:\nDesign consistent hashing for more details.\n* Save crawl states and data: To guard against failures, crawl states and data are written to\na storage system. A disrupted crawl can be restarted easily by loading saved states and\ndata.\n\n¢ Exception handling: Errors are inevitable and common in a large-scale system. The"
    },
    {
      "page": 147,
      "text": "crawler must handle exceptions gracefully without crashing the system.\n* Data validation: This is an important measure to prevent system errors.\n\nExtensibility\n\nAs almost every system evolves, one of the design goals is to make the system flexible\nenough to support new content types. The crawler can be extended by plugging in new\nmodules. Figure 9-10 shows how to add new modules.\n\nDNS Resolver Content\nStorage\nv\n\n| seed URLs 1 URL Frontier 4 HTML Downloader 4 Content Parser + Content Seen?\nrN —\n\n| Web Monitor |\n\n|\n\nPNG\n\n| Downloade? | Link Extractor\n\nExtension Module\n\nURL Filter\n\nURL\nStorage\n\n=\nP]\niad\no\n®\no\ns\n2\n\nFigure 9-10\n\n* PNG Downloader module is plugged-in to download PNG files.\n* Web Monitor module is added to monitor the web and prevent copyright and trademark\ninfringements.\n\nDetect and avoid problematic content\nThis section discusses the detection and prevention of redundant, meaningless, or harmful\ncontent.\n\n1. Redundant content\nAs discussed previously, nearly 30% of the web pages are duplicates. Hashes or checksums\nhelp to detect duplication [11].\n\n2. Spider traps\n\nA spider trap is a web page that causes a crawler in an infinite loop. For instance, an infinite\ndeep directory structure is listed as follows:\nwww.spidertrapexample.com/foo/bar/foo/bar/foo/bar/...\n\nSuch spider traps can be avoided by setting a maximal length for URLs. However, no one-\nsize-fits-all solution exists to detect spider traps. Websites containing spider traps are easy to\nidentify due to an unusually large number of web pages discovered on such websites. It is\nhard to develop automatic algorithms to avoid spider traps; however, a user can manually"
    },
    {
      "page": 148,
      "text": "verify and identify a spider trap, and either exclude those websites from the crawler or apply\nsome customized URL filters.\n\n3. Data noise\n\nSome of the contents have little or no value, such as advertisements, code snippets, spam\nURLs, etc. Those contents are not useful for crawlers and should be excluded if possible."
    },
    {
      "page": 149,
      "text": "Step 4 - Wrap up\nIn this chapter, we first discussed the characteristics of a good crawler: scalability, politeness,\nextensibility, and robustness. Then, we proposed a design and discussed key components.\nBuilding a scalable web crawler is not a trivial task because the web is enormously large and\nfull of traps. Even though we have covered many topics, we still miss many relevant talking\npoints:\n* Server-side rendering: Numerous websites use scripts like JavaScript, AJAX, etc to\ngenerate links on the fly. If we download and parse web pages directly, we will not be able\nto retrieve dynamically generated links. To solve this problem, we perform server-side\nrendering (also called dynamic rendering) first before parsing a page [12].\n+ Filter out unwanted pages: With finite storage capacity and crawl resources, an anti-spam\ncomponent is beneficial in filtering out low quality and spam pages [13] [14].\n* Database replication and sharding: Techniques like replication and sharding are used to\nimprove the data layer availability, scalability, and reliability.\n* Horizontal scaling: For large scale crawl, hundreds or even thousands of servers are\nneeded to perform download tasks. The key is to keep servers stateless.\n* Availability, consistency, and reliability: These concepts are at the core of any large\nsystem’s success. We discussed these concepts in detail in Chapter 1. Refresh your\nmemory on these topics.\n* Analytics: Collecting and analyzing data are important parts of any system because data\nis key ingredient for fine-tuning.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 150,
      "text": "Reference materials\n\n[1] US Library of Congress: https://www.loc.gov/websites/\n\n[2] EU Web Archive: http://data.europa.eu/webarchive\n\n[3] Digimarc: https://www.digimarc.com/products/digimarc-services/piracy-intelligence\n\n[4] Heydon A., Najork M. Mercator: A scalable, extensible web crawler World Wide Web, 2\n(4) (1999), pp. 219-229\n\n[5] By Christopher Olston, Marc Najork: Web Crawling.\nhttp://infolab.stanford.edu/~olston/publications/crawling survey.pdf\n\n[6] 29% Of Sites Face Duplicate Content Issues: https://tinyurl.com/y6tmh55y\n\n[7] Rabin M.O., et al. Fingerprinting by random polynomials Center for Research in\nComputing Techn., Aiken Computation Laboratory, Univ. (1981)\n\n[8] B. H. Bloom, “Space/time trade-offs in hash coding with allowable errors,”\nCommunications of the ACM, vol. 13, no. 7, pp. 422-426, 1970.\n\n[9] Donald J. Patterson, Web Crawling:\nhttps://www.ics.uci.edu/~lopes/teaching/cs221W12/slides/Lecture05.pdf\n\n[10] L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank citation\nranking: Bringing order to the web,” Technical Report, Stanford University,\n1998.\n\n[11] Burton Bloom. Space/time trade-offs in hash coding with allowable errors.\nCommunications of the ACM, 13(7), pages 422--426, July 1970.\n\n[12] Google Dynamic Rendering:\nhttps://developers.google.com/search/docs/guides/dynamic-rendering\n\n[13] T. Urvoy, T. Lavergne, and P. Filoche, “Tracking web spam with hidden style\nsimilarity,” in Proceedings of the 2nd International Workshop on Adversarial Information\nRetrieval on the Web, 2006.\n\n[14] H.-T. Lee, D. Leonard, X. Wang, and D. Loguinov, “IRLbot: Scaling to 6 billion pages\nand beyond,” in Proceedings of the 17th International World Wide Web Conference, 2008."
    },
    {
      "page": 151,
      "text": "CHAPTER 10: DESIGN A NOTIFICATION SYSTEM\n\nA notification system has already become a very popular feature for many applications in\nrecent years. A notification alerts a user with important information like breaking news,\nproduct updates, events, offerings, etc. It has become an indispensable part of our daily life.\nIn this chapter, you are asked to design a notification system.\n\nA notification is more than just mobile push notification. Three types of notification formats\nare: mobile push notification, SMS message, and Email. Figure 10-1 shows an example of\neach of these notifications.\n\nVerizon\n\n1434 oo:\n< Bue\n\nMake your screen time count\nand #Make2020. inbox\n\na\nIi43\n\nWednesday, January 8 : q e\n\nGoDaddy Jan 6 &\n\nNotification Center ; i aes\n\n: ‘  GeDaddy Customer Number:\n\n234997112\n\nHey, it's time\n\nSes '\n\n| Seance Yesterday, 401PM Tue, Dec 10, 2:92PM i for New\n\n| Hi Alex, your Amazon package wes delivered. : Saas RS\n\n| 3 more notifications 1 Se phone outage in San H 7, ’\n: Mateo County Y ears\n{At&t seven digit phone calls to : resolutions\n\n: San Mateo County Dispatch\n\n: are currently down. If you need\n\n| to reach San Mateo County\n\n: Dispatch for any reason;\nMedical, Police or Fire call\n9-1-1.\n\nfom) o\n\nain!\n\nPush\n\nnotification SMS i Email\n\nFigure 10-1"
    },
    {
      "page": 152,
      "text": "Step 1 - Understand the problem and establish design scope\n\nBuilding a scalable system that sends out millions of notifications a day is not an easy task. It\nrequires a deep understanding of the notification ecosystem. The interview question is\npurposely designed to be open-ended and ambiguous, and it is your responsibility to ask\nquestions to clarify the requirements.\n\nCandidate: What types of notifications does the system support?\nInterviewer: Push notification, SMS message, and email.\n\nCandidate: Is it a real-time system?\n\nInterviewer: Let us say it is a soft real-time system. We want a user to receive notifications\nas soon as possible. However, if the system is under a high workload, a slight delay is\nacceptable.\n\nCandidate: What are the supported devices?\nInterviewer: iOS devices, android devices, and laptop/desktop.\n\nCandidate: What triggers notifications?\nInterviewer: Notifications can be triggered by client applications. They can also be\nscheduled on the server-side.\n\nCandidate: Will users be able to opt-out?\nInterviewer: Yes, users who choose to opt-out will no longer receive notifications.\n\nCandidate: How many notifications are sent out each day?\nInterviewer: 10 million mobile push notifications, 1 million SMS messages, and 5 million\nemails."
    },
    {
      "page": 153,
      "text": "Step 2 - Propose high-level design and get buy-in\n\nThis section shows the high-level design that supports various notification types: iOS push\nnotification, Android push notification, SMS message, and Email. It is structured as follows:\n\n* Different types of notifications\n* Contact info gathering flow\n+ Notification sending/receiving flow\n\nDifferent types of notifications\nWe start by looking at how each notification type works at a high level.\n\niOS push notification\n\niOS\n\nFigure 10-2\nWe primary need three components to send an iOS push notification:\n\n* Provider. A provider builds and sends notification requests to Apple Push Notification\nService (APNS). To construct a push notification, the provider provides the following\ndata:\n\n* Device token: This is a unique identifier used for sending push notifications.\n\n* Payload: This is a JSON dictionary that contains a notification’s payload. Here is an\n\nexample:\n{\n\"aps\": {\n\"alert\" : {\n\"title\" : \"Game Request\",\n\"body\" : \"Bob wants to play chess\",\n\"action-loc-key\" : \"PLAY\"\n3,\n\"badge\": 5\n#\n}\n\n* APNS: This is a remote service provided by Apple to propagate push notifications to iOS\ndevices.\n* iOS Device: It is the end client, which receives push notifications.\n\nAndroid push notification\n\nAndroid adopts a similar notification flow. Instead of using APNs, Firebase Cloud Messaging\n(FCM) is commonly used to send push notifications to android devices."
    },
    {
      "page": 154,
      "text": "FCM\n\nAndroid\nFigure 10-3\n\nSMS message\n\nFor SMS messages, third party SMS services like Twilio [1], Nexmo [2], and many others are\ncommonly used. Most of them are commercial services.\n\nSMS ~~\n\nService\n\nFigure 10-4\n\nEmail\n\nAlthough companies can set up their own email servers, many of them opt for commercial\nemail services. Sendgrid [3] and Mailchimp [4] are among the most popular email services,\nwhich offer a better delivery rate and data analytics.\n\ncacsa\nt.Gy1\n\nProvider ——_ >| S1__,|\n\nEmail Email\nService\n\nFigure 10-5\n\nFigure 10-6 shows the design after including all the third-party services."
    },
    {
      "page": 155,
      "text": "wo\n\nAndroid\nSMS\n' ae\n\nne oe\n, Cacs : Email\nEmail\nService\n\nThird Party\nServices\n\nFigure 10-6\n\nContact info gathering flow\n\nTo send notifications, we need to gather mobile device tokens, phone numbers, or email\naddresses. As shown in Figure 10-7, when a user installs our app or signs up for the first time,\nAPI servers collect user contact info and store it in the database.\n\n4 BA\n\ni ‘ store contact\n\n, API servers »\n\non app install\nor sign up\n\nLoad balancer\n\nFigure 10-7\n\nFigure 10-8 shows simplified database tables to store contact info. Email addresses and phone\nnumbers are stored in the user table, whereas device tokens are stored in the device table. A"
    },
    {
      "page": 156,
      "text": "user can have multiple devices, indicating that a push notification can be sent to all the user\ndevices.\n\n;\nuser_id bigint\n\nid bigint\n\nemail varchar 6\n: device_token varchar\n\ncountry_code integer\n\nuser_id bigint\nphone_number integer ~ 8\n\nlast_logged_in_at timestam\ncreated_at timestamp ste soeLle £\n\nFigure 10-8\nNotification sending/receiving flow\nWe will first present the initial design; then, propose some optimizations.\nHigh-level design\n\nFigure 10-9 shows the design, and each system component is explained below.\n\nFCM i Android\n\nNotification System 4 :\ni SMS\n: SMS :\nService :\nc3c3\nino\n\ncac3 i Email\n\nThird Party\nServices\n\nFigure 10-9\n\nService 1 to N: A service can be a micro-service, a cron job, or a distributed system that\ntriggers notification sending events. For example, a billing service sends emails to remind\ncustomers of their due payment or a shopping website tells customers that their packages will\nbe delivered tomorrow via SMS messages."
    },
    {
      "page": 157,
      "text": "Notification system: The notification system is the centerpiece of sending/receiving\nnotifications. Starting with something simple, only one notification server is used. It provides\nAPIs for services 1 to N, and builds notification payloads for third party services.\n\nThird-party services: Third party services are responsible for delivering notifications to\nusers. While integrating with third-party services, we need to pay extra attention to\nextensibility. Good extensibility means a flexible system that can easily plugging or\nunplugging of a third-party service. Another important consideration is that a third-party\nservice might be unavailable in new markets or in the future. For instance, FCM is\nunavailable in China. Thus, alternative third-party services such as Jpush, PushY, etc are used\nthere.\n\niOS, Android, SMS, Email: Users receive notifications on their devices.\n\nThree problems are identified in this design:\n* Single point of failure (SPOF): A single notification server means SPOF.\n\n* Hard to scale: The notification system handles everything related to push notifications in\none server. It is challenging to scale databases, caches, and different notification\nprocessing components independently.\n\n* Performance bottleneck: Processing and sending notifications can be resource intensive.\nFor example, constructing HTML pages and waiting for responses from third party\nservices could take time. Handling everything in one system can result in the system\noverload, especially during peak hours.\n\nHigh-level design (improved)\n\nAfter enumerating challenges in the initial design, we improve the design as listed below:\n* Move the database and cache out of the notification server.\n« Add more notification servers and set up automatic horizontal scaling.\n* Introduce message queues to decouple the system components.\n\nFigure 10-10 shows the improved high-level design."
    },
    {
      "page": 158,
      "text": "retry on error\n\noe! :\n© ime ©:\n| f 1 >\nWorkers | H / ios\nService 1 H i\n@ a\n' : : i\nMem) — i. @ —_-0\nService 2 es” ‘Android PN | Workers | S eom Android\nen —|—|— ee . q :\n~—~+ Hi\ni H i\niNotification! Sn 1 :\n‘sewers! es: H :\n~ im _. ] 0\nMuu)—~ sae —: CJ :\n‘ 1 : i\n‘SMS queue | Workers | i sms! SMS\nek ! Service |\nService N : '\niin ioc |\n' 1 i hol: m\n> inlala — ih : >\n1 ‘ : : Email\nEmail queue 4 Workers Email |\nScat ! Service\nThird Party |\n1 Services}\n\nFigure 10-10\nThe best way to go through the above diagram is from left to right:\n\nService 1 to N: They represent different services that send notifications via APIs provided by\nnotification servers.\nNotification servers: They provide the following functionalities:\n\n+ Provide APIs for services to send notifications. Those APIs are only accessible internally\nor by verified clients to prevent spams.\n\n* Carry out basic validations to verify emails, phone numbers, etc.\n* Query the database or cache to fetch data needed to render a notification.\n+ Put notification data to message queues for parallel processing.\n\nHere is an example of the API to send an email:\n\nPOST https://api.example.com/v/sms/send\nRequest body"
    },
    {
      "page": 159,
      "text": "\"to\": [\n{\n\"user_id\": 123456\n}\nle\n\"from\": {\n\"email\": \"from_address@example.com\"\n\n},\n\"subject\": \"Hello, World!\",\n\n\"content\": [\n\n{\n\"type\": \"text/plain\",\n\"value\": \"Hello, World!\"\n\n}\nCache: User info, device info, notification templates are cached.\nDB: It stores data about user, notification, settings, etc.\n\nMessage queues: They remove dependencies between components. Message queues serve as\nbuffers when high volumes of notifications are to be sent out. Each notification type is\nassigned with a distinct message queue so an outage in one third-party service will not affect\nother notification types.\n\nWorkers: Workers are a list of servers that pull notification events from message queues and\nsend them to the corresponding third-party services.\nThird-party services: Already explained in the initial design.\niOS, Android, SMS, Email: Already explained in the initial design.\nNext, let us examine how every component works together to send a notification:\n1. A service calls APIs provided by notification servers to send notifications.\n\n2. Notification servers fetch metadata such as user info, device token, and notification\nsetting from the cache or database.\n\n3. A notification event is sent to the corresponding queue for processing. For instance, an\niOS push notification event is sent to the iOS PN queue.\n\n4. Workers pull notification events from message queues.\n5. Workers send notifications to third party services.\n6. Third-party services send notifications to user devices."
    },
    {
      "page": 160,
      "text": "Step 3 - Design deep dive\nIn the high-level design, we discussed different types of notifications, contact info gathering\nflow, and notification sending/receiving flow. We will explore the following in deep dive:\n* Reliability.\n* Additional component and considerations: notification template, notification settings,\nrate limiting, retry mechanism, security in push notifications, monitor queued notifications\nand event tracking.\n\n* Updated design.\nReliability\nWe must answer a few important reliability questions when designing a notification system in\ndistributed environments.\n\nHow to prevent data loss?\n\nOne of the most important requirements in a notification system is that it cannot lose data.\nNotifications can usually be delayed or re-ordered, but never lost. To satisfy this requirement,\nthe notification system persists notification data in a database and implements a retry\nmechanism. The notification log database is included for data persistence, as shown in Figure\n\nGEE\n\niOS PN , Workers 1\n\nsen =?\n\nNotification log\n\nFigure 10-11\n\nWill recipients receive a notification exactly once?\n\nThe short answer is no. Although notification is delivered exactly once most of the time, the\ndistributed nature could result in duplicate notifications. To reduce the duplication\noccurrence, we introduce a dedupe mechanism and handle each failure case carefully. Here is\na simple dedupe logic:\n\nWhen a notification event first arrives, we check if it is seen before by checking the event ID.\nIf it is seen before, it is discarded. Otherwise, we will send out the notification. For interested\nreaders to explore why we cannot have exactly once delivery, refer to the reference material\n[5].\n\nAdditional components and considerations\n\nWe have discussed how to collect user contact info, send, and receive a notification. A\nnotification system is a lot more than that. Here we discuss additional components including\ntemplate reusing, notification settings, event tracking, system monitoring, rate limiting, etc.\n\nNotification template"
    },
    {
      "page": 161,
      "text": "A large notification system sends out millions of notifications per day, and many of these\nnotifications follow a similar format. Notification templates are introduced to avoid building\nevery notification from scratch. A notification template is a preformatted notification to\ncreate your unique notification by customizing parameters, styling, tracking links, etc. Here is\nan example template of push notifications.\n\nBODY:\n\nYou dreamed of it. We dared it. ITEM NAME] is back — only until [DATE].\n\nCTA:\n\nOrder Now. Or, Save My [ITEM NAME]\n\nThe benefits of using notification templates include maintaining a consistent format, reducing\nthe margin error, and saving time.\n\nNotification setting\n\nUsers generally receive way too many notifications daily and they can easily feel\noverwhelmed. Thus, many websites and apps give users fine-grained control over notification\nsettings. This information is stored in the notification setting table, with the following fields:\nuser_id bigInt\n\nchannel varchar # push notification, email or SMS\n\nopt_in boolean # opt-in to receive notification\n\nBefore any notification is sent to a user, we first check if a user is opted-in to receive this type\nof notification.\n\nRate limiting\n\nTo avoid overwhelming users with too many notifications, we can limit the number of\nnotifications a user can receive. This is important because receivers could turn off\nnotifications completely if we send too often.\n\nRetry mechanism\n\nWhen a third-party service fails to send a notification, the notification will be added to the\nmessage queue for retrying. If the problem persists, an alert will be sent out to developers.\nSecurity in push notifications\n\nFor iOS or Android apps, appKey and appSecret are used to secure push notification APIs\n[6]. Only authenticated or verified clients are allowed to send push notifications using our\nAPIs. Interested users should refer to the reference material [6].\n\nMonitor queued notifications\n\nA key metric to monitor is the total number of queued notifications. If the number is large,\nthe notification events are not processed fast enough by workers. To avoid delay in the\nnotification delivery, more workers are needed. Figure 10-12 (credit to [7]) shows an\nexample of queued messages to be processed."
    },
    {
      "page": 162,
      "text": "Queued messages\n\n0\n200\n0\nc\n\nFigure 10-12\n\nEvents tracking\n\nNotification metrics, such as open rate, click rate, and engagement are important in\nunderstanding customer behaviors. Analytics service implements events tracking. Integration\nbetween the notification system and the analytics service is usually required. Figure 10-13\nshows an example of events that might be tracked for analytics purposes.\n\npending\n\nFigure 10-13\n\nUpdated design\nPutting everything together, Figure 10-14 shows the updated notification system design."
    },
    {
      "page": 163,
      "text": ">——send pending click tracking\n\nservice —\n\nsent\n\nretry on error\n\n‘\n\nae “oe\nService N m |__| Bulheniieation ( aaa t fxens)—>\n| Notification Rate limit d i ios\n\nWorkers +\n\\ servers\n\nNotification\ntemplate\n\nsetting\nuser\n\nFigure 10-14\n\nIn this design, many new components are added in comparison with the previous design.\n+ The notification servers are equipped with two more critical features: authentication and\nrate-limiting.\n* We also add a retry mechanism to handle notification failures. If the system fails to send\nnotifications, they are put back in the messaging queue and the workers will retry for a\npredefined number of times.\n\n* Furthermore, notification templates provide a consistent and efficient notification\ncreation process.\n\n* Finally, monitoring and tracking systems are added for system health checks and future\nimprovements."
    },
    {
      "page": 164,
      "text": "Step 4 - Wrap up\n\nNotifications are indispensable because they keep us posted with important information. It\ncould be a push notification about your favorite movie on Netflix, an email about discounts\non new products, or a message about your online shopping payment confirmation.\n\nIn this chapter, we described the design of a scalable notification system that supports\nmultiple notification formats: push notification, SMS message, and email. We adopted\nmessage queues to decouple system components.\nBesides the high-level design, we dug deep into more components and optimizations.\n* Reliability: We proposed a robust retry mechanism to minimize the failure rate.\n* Security: AppKey/appSecret pair is used to ensure only verified clients can send\nnotifications.\n* Tracking and monitoring: These are implemented in any stage of a notification flow to\ncapture important stats.\n+ Respect user settings: Users may opt-out of receiving notifications. Our system checks\nuser settings first before sending notifications.\n* Rate limiting: Users will appreciate a frequency capping on the number of notifications\nthey receive.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 165,
      "text": "Reference materials\n[1] Twilio SMS: https://www.twilio.com/sms\n[2] Nexmo SMS: https://www.nexmo.com/products/sms\n\n[3] Sendgrid: https://sendgrid.com/\n[4] Mailchimp: https://mailchimp.com/\n\n[5] You Cannot Have Exactly-Once Delivery: https://bravenewgeek.com/you-cannot-have-\nexactly-once-delivery/\n\n[6] Security in Push Notifications: https://cloud.ibm.com/docs/services/mobilepush?\ntopic=mobile-pushnotification-security-in-push-notifications\n\n[7] RadditMQ: https://bit.ly/2sotla6"
    },
    {
      "page": 166,
      "text": "CHAPTER 11: DESIGN A NEWS FEED SYSTEM\n\nIn this chapter, you are asked to design a news feed system. What is news feed? According to\nthe Facebook help page, “News feed is the constantly updating list of stories in the middle of\nyour home page. News Feed includes status updates, photos, videos, links, app activity, and\nlikes from people, pages, and groups that you follow on Facebook” [1]. This is a popular\ninterview question. Similar questions commonly asked are: design Facebook news feed,\nInstagram feed, Twitter timeline, etc.\n\nFigure 11-1 (source : https://bit.ly/2Vv9JCm)"
    },
    {
      "page": 167,
      "text": "Step 1 - Understand the problem and establish design scope\n\nThe first set of clarification questions are to understand what the interviewer has in mind\nwhen she asks you to design a news feed system. At the very least, you should figure out\nwhat features to support. Here is an example of candidate-interviewer interaction:\n\nCandidate: Is this a mobile app? Or a web app? Or both?\nInterviewer: Both\n\nCandidate: What are the important features?\nInterview: A user can publish a post and see her friends’ posts on the news feed page.\n\nCandidate: Is the news feed sorted by reverse chronological order or any particular order\nsuch as topic scores? For instance, posts from your close friends have higher scores.\nInterviewer: To keep things simple, let us assume the feed is sorted by reverse chronological\norder.\n\nCandidate: How many friends can a user have?\nInterviewer: 5000\n\nCandidate: What is the traffic volume?\nInterviewer: 10 million DAU\n\nCandidate: Can feed contain images, videos, or just text?\nInterviewer: It can contain media files, including both images and videos.\n\nNow you have gathered the requirements, we focus on designing the system."
    },
    {
      "page": 168,
      "text": "Step 2 - Propose high-level design and get buy-in\nThe design is divided into two flows: feed publishing and news feed building.\n\n¢ Feed publishing: when a user publishes a post, corresponding data is written into cache\nand database. A post is populated to her friends’ news feed.\n\n* Newsfeed building: for simplicity, let us assume the news feed is built by aggregating\nfriends’ posts in reverse chronological order.\nNewsfeed APIs\n\nThe news feed APIs are the primary ways for clients to communicate with servers. Those\nAPIs are HTTP based that allow clients to perform actions, which include posting a status,\nretrieving news feed, adding friends, etc. We discuss two most important APIs: feed\npublishing API and news feed retrieval API.\n\nFeed publishing API\n\nTo publish a post, a HTTP POST request will be sent to the server. The API is shown below:\nPOST W1/me/feed\n\nParams:\n* content: content is the text of the post.\n\n* auth_token: it is used to authenticate API requests.\nNewsfeed retrieval API\nThe API to retrieve news feed is shown below:\n\nGET /v1/me/feed\nParams:\n* auth_token: it is used to authenticate API requests.\nFeed publishing\nFigure 11-2 shows the high-level design of the feed publishing flow."
    },
    {
      "page": 169,
      "text": "i) User a 0\n\nSO, Web browser Mobile app\n\nvilme/feed?\ncontent=Hello&\nauth_token={auth_token}\n\nLoad balancer\n\n- s\n\nt ’\n' 1\n1 1\n' '\n' 1\n' fl\n\\ Web servers }\na | =,\n| Post Service | Fanout Service Notification |\nService\n\nNews Feed\n\nPost Cache\nose acne Cache\n\nFigure 11-2\n+ User: a user can view news feeds on a browser or mobile app. A user makes a post with\ncontent “Hello” through API:\n/v1/me/feed 2content=Hello&auth_token={auth_token}\n+ Load balancer: distribute traffic to web servers.\n* Web servers: web servers redirect traffic to different internal services.\n* Post service: persist post in the database and cache.\n* Fanout service: push new content to friends’ news feed. Newsfeed data is stored in the\ncache for fast retrieval.\n* Notification service: inform friends that new content is available and send out push\nnotifications.\n\nNewsfeed building\nIn this section, we discuss how news feed is built behind the scenes. Figure 11-3 shows the\nhigh-level design:"
    },
    {
      "page": 170,
      "text": "User oO 6\n\nWeb browser Mobile app\n\nvi/me/feed\n\nLoad balancer\n\ngsc\n_—|—\n1\n\n1\n\n1\n\n1\n\n1\n\n'\n\nt\n1\n1\n1\n1\n1\n1\n\\\n\n.\n\nNews Feed Service\n\nNews Feed\nCache\n\nFigure 11-3\n+ User: a user sends a request to retrieve her news feed. The request looks like this:\n/ v1/me/feed.\n* Load balancer: load balancer redirects traffic to web servers.\n* Web servers: web servers route requests to newsfeed service.\n* Newsfeed service: news feed service fetches news feed from the cache.\n* Newsfeed cache: store news feed IDs needed to render the news feed."
    },
    {
      "page": 171,
      "text": "Step 3 - Design deep dive\n\nThe high-level design briefly covered two flows: feed publishing and news feed building.\nHere, we discuss those topics in more depth.\n\nFeed publishing deep dive\n\nFigure 11-4 outlines the detailed design for feed publishing. We have discussed most of\ncomponents in high-level design, and we will focus on two components: web servers and\nfanout service.\n\nUser ia]\nQW Web browser Mobile app\nvifme/feed?\n\ncontent=Hello&\nauth_token={auth_token}\n\nLoad balancer\n\n1\ni Authentication Notification\nPost Service ‘\n! Rate Limiting Service\n\nPost Cache\n\nPost DB User Cache User DB\n\nNews Feed\nCache\n\nFigure 11-4\n\nWeb servers\nBesides communicating with clients, web servers enforce authentication and rate-limiting."
    },
    {
      "page": 172,
      "text": "Only users signed in with valid auth_token are allowed to make posts. The system limits the\nnumber of posts a user can make within a certain period, vital to prevent spam and abusive\ncontent.\n\nFanout service\n\nFanout is the process of delivering a post to all friends. Two types of fanout models are:\nfanout on write (also called push model) and fanout on read (also called pull model). Both\nmodels have pros and cons. We explain their workflows and explore the best approach to\nsupport our system.\n\nFanout on write. With this approach, news feed is pre-computed during write time. A new\npost is delivered to friends’ cache immediately after it is published.\nPros:\n\n« The news feed is generated in real-time and can be pushed to friends immediately.\n\n* Fetching news feed is fast because the news feed is pre-computed during write time.\n\nCons:\n\n* If auser has many friends, fetching the friend list and generating news feeds for all of\nthem are slow and time consuming. It is called hotkey problem.\n\n« For inactive users or those rarely log in, pre-computing news feeds waste computing\nresources.\n\nFanout on read. The news feed is generated during read time. This is an on-demand model.\nRecent posts are pulled when a user loads her home page.\nPros:\n\n* For inactive users or those who rarely log in, fanout on read works better because it will\nnot waste computing resources on them.\n\n* Data is not pushed to friends so there is no hotkey problem.\nCons:\n\n+ Fetching the news feed is slow as the news feed is not pre-computed.\nWe adopt a hybrid approach to get benefits of both approaches and avoid pitfalls in them.\nSince fetching the news feed fast is crucial, we use a push model for the majority of users.\nFor celebrities or users who have many friends/followers, we let followers pull news content\n\non-demand to avoid system overload. Consistent hashing is a useful technique to mitigate the\nhotkey problem as it helps to distribute requests/data more evenly.\n\nLet us take a close look at the fanout service as shown in Figure 11-5."
    },
    {
      "page": 173,
      "text": "User Cache User DB\n\n'\n1\n1\n1\n1\n1\n1\n\nFanout Workers\n. =\n\nNews Feed\nCache\n\nFigure 11-5\nThe fanout service works as follows:\n\n1. Fetch friend IDs from the graph database. Graph databases are suited for managing\nfriend relationship and friend recommendations. Interested readers wishing to learn more\nabout this concept should refer to the reference material [2].\n\n2. Get friends info from the user cache. The system then filters out friends based on user\nsettings. For example, if you mute someone, her posts will not show up on your news feed\neven though you are still friends. Another reason why posts may not show is that a user\ncould selectively share information with specific friends or hide it from other people.\n\n3. Send friends list and new post ID to the message queue.\n\n4. Fanout workers fetch data from the message queue and store news feed data in the news\nfeed cache. You can think of the news feed cache as a <post_id, user_id> mapping table.\nWhenever a new post is made, it will be appended to the news feed table as shown in\nFigure 11-6. The memory consumption can become very large if we store the entire user\nand post objects in the cache. Thus, only IDs are stored. To keep the memory size small,\nwe set a configurable limit. The chance of a user scrolling through thousands of posts in\nnews feed is slim. Most users are only interested in the latest content, so the cache miss\nrate is low.\n\n5. Store <post_id, user_id > in news feed cache. Figure 11-6 shows an example of what\nthe news feed looks like in cache."
    },
    {
      "page": 174,
      "text": "post_id user_id\npost_id user_id\npost_id user_id\npost_id user_id\npost_id user_id\npost_id user_id\npost_id user_id\npost_id user_id\nFigure 11-6\n\nNewsfeed retrieval deep dive\nFigure 11-7 illustrates the detailed design for news feed retrieval.\n\nUser a 5\n\nWeb browser Mobile app\n\nCDN\n\nivilme/feed\n\n©\non —l— :\n; Authentication ;\n' E Rate Limiting ;\n7 Web servers i\n\nUser Cache\n\nNews Feed\nCache Post Cache Post DB\n\nFigure 11-7\n\nAs shown in Figure 11-7, media content (images, videos, etc.) are stored in CDN for fast\nretrieval. Let us look at how a client retrieves news feed."
    },
    {
      "page": 175,
      "text": "1. A user sends a request to retrieve her news feed. The request looks like this: /v1/me/feed\n2. The load balancer redistributes requests to web servers.\n\n3. Web servers call the news feed service to fetch news feeds.\n\n4. News feed service gets a list post IDs from the news feed cache.\n\n5. A user’s news feed is more than just a list of feed IDs. It contains username, profile\npicture, post content, post image, etc. Thus, the news feed service fetches the complete\nuser and post objects from caches (user cache and post cache) to construct the fully\nhydrated news feed.\n\n6. The fully hydrated news feed is returned in JSON format back to the client for\nrendering.\n\nCache architecture\n\nCache is extremely important for a news feed system. We divide the cache tier into 5 layers\nas shown in Figure 11-8.\n\n7 1\n1\nNews Feed 1 news feed :\n' 1\nLS so ern\n1\nContent | hot cache normal ;\n\\ 1\noe  ———O——™O—eeeeDTe\n1\nSocial Graph ; follower following ;\n' i\n0 SSS ee Se ==:\n1\nAction liked replied | others ] ;\n' 1\nSee ae\n1\nCounters | like counter reply counter other counters ;\n' 1\nFigure 11-8\n\n* News Feed: It stores IDs of news feeds.\n* Content: It stores every post data. Popular content is stored in hot cache.\n* Social Graph: It stores user relationship data.\n\n+ Action: It stores info about whether a user liked a post, replied a post, or took other\nactions on a post.\n\n* Counters: It stores counters for like, reply, follower, following, etc."
    },
    {
      "page": 176,
      "text": "Step 4 - Wrap up\nIn this chapter, we designed a news feed system. Our design contains two flows: feed\npublishing and news feed retrieval.\n\nLike any system design interview questions, there is no perfect way to design a system. Every\ncompany has its unique constraints, and you must design a system to fit those constraints.\nUnderstanding the tradeoffs of your design and technology choices are important. If there are\na few minutes left, you can talk about scalability issues. To avoid duplicated discussion, only\nhigh-level talking points are listed below.\nScaling the database:\n\n+ Vertical scaling vs Horizontal scaling\n\n* SQL vs NoSQL\n\n* Master-slave replication\n\n* Read replicas\n\n* Consistency models\n\n* Database sharding\nOther talking points:\n\n* Keep web tier stateless\n\n* Cache data as much as you can\n\n* Support multiple data centers\n\n+ Lose couple components with message queues\n\n* Monitor key metrics. For instance, QPS during peak hours and latency while users\n\nrefreshing their news feed are interesting to monitor.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 177,
      "text": "Reference materials\n\n[1] How News Feed Works:\nhttps://www.facebook.com/help/327131014036297/\n\n[2] Friend of Friend recommendations Neo4j and SQL Sever:\n\nhttp://geekswithblogs.net/brendonpage/archive/2015/10/26/friend-of-friend-\nrecommendations-with-neo4j.aspx"
    },
    {
      "page": 178,
      "text": "CHAPTER 12: DESIGN A CHAT SYSTEM\n\nIn this chapter we explore the design of a chat system. Almost everyone uses a chat app.\nFigure 12-1 shows some of the most popular apps in the marketplace.\n\n@ Whatsapp 2 Facebook messenger % Wechat\ny- 6g\n\nA chat app performs different functions for different people. It is extremely important to nail\ndown the exact requirements. For example, you do not want to design a system that focuses\n\non group chat when the interviewer has one-on-one chat in mind. It is important to explore\nthe feature requirements.\n\nGoogle Hangout Discord\n\nFigure 12-1"
    },
    {
      "page": 179,
      "text": "Step 1 - Understand the problem and establish design scope\n\nIt is vital to agree on the type of chat app to design. In the marketplace, there are one-on-one\nchat apps like Facebook Messenger, WeChat, and WhatsApp, office chat apps that focus on\ngroup chat like Slack, or game chat apps, like Discord, that focus on large group interaction\nand low voice chat latency.\n\nThe first set of clarification questions should nail down what the interviewer has in mind\nexactly when she asks you to design a chat system. At the very least, figure out if you should\nfocus on a one-on-one chat or group chat app. Some questions you might ask are as follows:\n\nCandidate: What kind of chat app shall we design? 1 on 1 or group based?\nInterviewer: It should support both 1 on 1 and group chat.\n\nCandidate: Is this a mobile app? Or a web app? Or both?\nInterviewer: Both.\n\nCandidate: What is the scale of this app? A startup app or massive scale?\nInterviewer: It should support 50 million daily active users (DAU).\n\nCandidate: For group chat, what is the group member limit?\nInterviewer: A maximum of 100 people\n\nCandidate: What features are important for the chat app? Can it support attachment?\nInterviewer: 1 on 1 chat, group chat, online indicator. The system only supports text\nmessages.\n\nCandidate: Is there a message size limit?\nInterviewer: Yes, text length should be less than 100,000 characters long.\n\nCandidate: Is end-to-end encryption required?\nInterviewer: Not required for now but we will discuss that if time allows.\n\nCandidate: How long shall we store the chat history?\nInterviewer: Forever.\n\nIn the chapter, we focus on designing a chat app like Facebook messenger, with an emphasis\non the following features:\n\n« A one-on-one chat with low delivery latency\n\n* Small group chat (max of 100 people)\n\n* Online presence\n\n* Multiple device support. The same account can be logged in to multiple accounts at the\nsame time.\n\n¢ Push notifications\n\nIt is also important to agree on the design scale. We will design a system that supports 50\nmillion DAU."
    },
    {
      "page": 180,
      "text": "Step 2 - Propose high-level design and get buy-in\n\nTo develop a high-quality design, we should have a basic knowledge of how clients and\nservers communicate. In a chat system, clients can be either mobile applications or web\napplications. Clients do not communicate directly with each other. Instead, each client\nconnects to a chat service, which supports all the features mentioned above. Let us focus on\nfundamental operations. The chat service must support the following functions:\n\n* Receive messages from other clients.\n\n* Find the right recipients for each message and relay the message to the recipients.\n\n« If a recipient is not online, hold the messages for that recipient on the server until she is\n\nonline.\n\nFigure 12-2 shows the relationships between clients (sender and receiver) and the chat\n\nservice.\n| } Chat service: CI\n— message 1. store message message\nsender 2. relay message receiver\n\nFigure 12-2\n\nWhen a client intends to start a chat, it connects the chats service using one or more network\nprotocols. For a chat service, the choice of network protocols is important. Let us discuss this\nwith the interviewer.\n\nRequests are initiated by the client for most client/server applications. This is also true for the\nsender side of a chat application. In Figure 12-2, when the sender sends a message to the\nreceiver via the chat service, it uses the time-tested HTTP protocol, which is the most\ncommon web protocol. In this scenario, the client opens a HTTP connection with the chat\nservice and sends the message, informing the service to send the message to the receiver. The\nkeep-alive is efficient for this because the keep-alive header allows a client to maintain a\npersistent connection with the chat service. It also reduces the number of TCP handshakes.\nHTTP is a fine option on the sender side, and many popular chat applications such as\nFacebook [1] used HTTP initially to send messages.\n\nHowever, the receiver side is a bit more complicated. Since HTTP is client-initiated, it is not\ntrivial to send messages from the server. Over the years, many techniques are used to\nsimulate a server-initiated connection: polling, long polling, and WebSocket. Those are\nimportant techniques widely used in system design interviews so let us examine each of\nthem.\n\nPolling\n\nAs shown in Figure 12-3, polling is a technique that the client periodically asks the server if\nthere are messages available. Depending on polling frequency, polling could be costly. It\ncould consume precious server resources to answer a question that offers no as an answer\nmost of the time."
    },
    {
      "page": 181,
      "text": "New messages?\n\nNew messages?\n\nNew messages?\n\nYes. Return\nnew messages\n\nNew messages?\n\nNO\n\nFigure 12-3\nLong polling\n\nBecause polling could be inefficient, the next progression is long polling (Figure 12-4)."
    },
    {
      "page": 182,
      "text": "New messages? |———_——\n\nWait for new messages\n\n+ ————____$_$_$_________—_—_—————_|_ Yes. Return new messages\n\nNew messages?, |———_—_—___\n\nTimeout\n\nNew messages?\n\nFigure 12-4\n\nIn long polling, a client holds the connection open until there are actually new messages\navailable or a timeout threshold has been reached. Once the client receives new messages, it\nimmediately sends another request to the server, restarting the process. Long polling has a\nfew drawbacks:\n* Sender and receiver may not connect to the same chat server. HTTP based servers are\nusually stateless. If you use round robin for load balancing, the server that receives the\nmessage might not have a long-polling connection with the client who receives the\nmessage.\n+ A server has no good way to tell if a client is disconnected.\n\n* It is inefficient. If a user does not chat much, long polling still makes periodic\nconnections after timeouts.\n\nWebSocket\n\nWebSocket is the most common solution for sending asynchronous updates from server to\nclient. Figure 12-5 shows how it works."
    },
    {
      "page": 183,
      "text": "GET /ws ov\n\ngemen\n\nacknowes\n\nBidirectional messages\n\nFigure 12-5\n\nWebSocket connection is initiated by the client. It is bi-directional and persistent. It starts its\nlife as a HTTP connection and could be “upgraded” via some well-defined handshake to a\nWebSocket connection. Through this persistent connection, a server could send updates to a\nclient. WebSocket connections generally work even if a firewall is in place. This is because\nthey use port 80 or 443 which are also used by HTTP/HTTPS connections.\n\nEarlier we said that on the sender side HTTP is a fine protocol to use, but since WebSocket is\nbidirectional, there is no strong technical reason not to use it also for sending. Figure 12-6\nshows how WebSockets (ws) is used for both sender and receiver sides.\n\nsender\n\n[\n\nChat service\n\nreceiver\n\nFigure 12-6\n\nBy using WebSocket for both sending and receiving, it simplifies the design and makes\nimplementation on both client and server more straightforward. Since WebSocket\nconnections are persistent, efficient connection management is critical on the server-side."
    },
    {
      "page": 184,
      "text": "High-level design\n\nJust now we mentioned that WebSocket was chosen as the main communication protocol\nbetween the client and server for its bidirectional communication, it is important to note that\neverything else does not have to be WebSocket. In fact, most features (sign up, login, user\nprofile, etc) of a chat application could use the traditional request/response method over\nHTTP. Let us drill in a bit and look at the high-level components of the system.\n\nAs shown in Figure 12-7, the chat system is broken down into three major categories:\nstateless services, stateful services, and third-party integration.\n\nLoad balancer\n\nAuthe tion\nservice\n\nService\ndiscovery\n\nFigure 12-7\n\nStateless Services\nStateless services are traditional public-facing request/response services, used to manage the\nlogin, signup, user profile, etc. These are common features among many websites and apps.\n\nStateless services sit behind a load balancer whose job is to route requests to the correct\nservices based on the request paths. These services can be monolithic or individual"
    },
    {
      "page": 185,
      "text": "microservices. We do not need to build many of these stateless services by ourselves as there\nare services in the market that can be integrated easily. The one service that we will discuss\nmore in deep dive is the service discovery. Its primary job is to give the client a list of DNS\nhost names of chat servers that the client could connect to.\n\nStateful Service\n\nThe only stateful service is the chat service. The service is stateful because each client\nmaintains a persistent network connection to a chat server. In this service, a client normally\ndoes not switch to another chat server as long as the server is still available. The service\ndiscovery coordinates closely with the chat service to avoid server overloading. We will go\ninto detail in deep dive.\n\nThird-party integration\nFor a chat app, push notification is the most important third-party integration. It is a way to\ninform users when new messages have arrived, even when the app is not running. Proper\n\nintegration of push notification is crucial. Refer to Chapter 10 Design a notification system\nfor more information.\n\nScalability\n\nOn a small scale, all services listed above could fit in one server. Even at the scale we design\nfor, it is in theory possible to fit all user connections in one modern cloud server. The number\nof concurrent connections that a server can handle will most likely be the limiting factor. In\nour scenario, at 1M concurrent users, assuming each user connection needs 10K of memory\non the server (this is a very rough figure and very dependent on the language choice), it only\nneeds about 10GB of memory to hold all the connections on one box.\n\nIf we propose a design where everything fits in one server, this may raise a big red flag in the\ninterviewer’s mind. No technologist would design such a scale in a single server. Single\nserver design is a deal breaker due to many factors. The single point of failure is the biggest\namong them.\n\nHowever, it is perfectly fine to start with a single server design. Just make sure the\ninterviewer knows this is a starting point. Putting everything we mentioned together, Figure\n12-8 shows the adjusted high-level design."
    },
    {
      "page": 186,
      "text": "Real time service\n\nLoad balancer\n\nPresence\n\nChat servers servers\n\nBag |\nNotification\nservers\n\nAPI servers\n\nKV store KV store\n\nKV store\n\nFigure 12-8\n\nIn Figure 12-8, the client maintains a persistent WebSocket connection to a chat server for\nreal-time messaging.\n\n+ Chat servers facilitate message sending/receiving.\n\n+ Presence servers manage online/offline status.\n\n* API servers handle everything including user login, signup, change profile, etc.\n\n+ Notification servers send push notifications.\n\n+ Finally, the key-value store is used to store chat history. When an offline user comes\n\nonline, she will see all her previous chat history.\nStorage\n\nAt this point, we have servers ready, services up running and third-party integrations\ncomplete. Deep down the technical stack is the data layer. Data layer usually requires some\neffort to get it correct. An important decision we must make is to decide on the right type of\ndatabase to use: relational databases or NoSQL databases? To make an informed decision, we"
    },
    {
      "page": 187,
      "text": "will examine the data types and read/write patterns.\n\nTwo types of data exist in a typical chat system. The first is generic data, such as user profile,\nsetting, user friends list. These data are stored in robust and reliable relational databases.\nReplication and sharding are common techniques to satisfy availability and scalability\nrequirements.\n\nThe second is unique to chat systems: chat history data. It is important to understand the\nread/write pattern.\n\n¢ The amount of data is enormous for chat systems. A previous study [2] reveals that\nFacebook messenger and Whatsapp process 60 billion messages a day.\n\n* Only recent chats are accessed frequently. Users do not usually look up for old chats.\n\n¢ Although very recent chat history is viewed in most cases, users might use features that\nrequire random access of data, such as search, view your mentions, jump to specific\nmessages, etc. These cases should be supported by the data access layer.\n\n* The read to write ratio is about 1:1 for 1 on 1 chat apps.\nSelecting the correct storage system that supports all of our use cases is crucial. We\nrecommend key-value stores for the following reasons:\n\n* Key-value stores allow easy horizontal scaling.\n\n* Key-value stores provide very low latency to access data.\n\n+ Relational databases do not handle long tail [3] of data well. When the indexes grow\nlarge, random access is expensive.\n\n* Key-value stores are adopted by other proven reliable chat applications. For example,\nboth Facebook messenger and Discord use key-value stores. Facebook messenger uses\nHBase [4], and Discord uses Cassandra [5].\n\nData models\n\nJust now, we talked about using key-value stores as our storage layer. The most important\ndata is message data. Let us take a close look.\n\nMessage table for 1 on 1 chat\n\nFigure 12-9 shows the message table for 1 on 1 chat. The primary key is message_id, which\nhelps to decide message sequence. We cannot rely on created_at to decide the message\nsequence because two messages can be created at the same time."
    },
    {
      "page": 188,
      "text": "message\n\nmessage _id bigint\n\nmessage _from bigint\n\nmessage_to bitint\n\ncontent text\n\ncreated_at timestamp\nFigure 12-9\n\nMessage table for group chat\n\nFigure 12-10 shows the message table for group chat. The composite primary key is\n(channel_id, message_id). Channel and group represent the same meaning here. channel_id\nis the partition key because all queries in a group chat operate in a channel.\n\ngroup_message\n\nchannel_id bigint\n\nmessage _id bigint\n\nuser_id bigint\n\ncontent text\n\ncreated_at timestamp\nFigure 12-10\n\nMessage ID"
    },
    {
      "page": 189,
      "text": "How to generate message_id is an interesting topic worth exploring. Message_id carries the\nresponsibility of ensuring the order of messages. To ascertain the order of messages,\nmessage_id must satisfy the following two requirements:\n\n¢ IDs must be unique.\n* IDs should be sortable by time, meaning new rows have higher IDs than old ones.\n\nHow can we achieve those two guarantees? The first idea that comes to mind is the\n“auto_increment” keyword in MySql. However, NoSQL databases usually do not provide\nsuch a feature.\n\nThe second approach is to use a global 64-bit sequence number generator like Snowflake [6].\nThis is discussed in “Chapter 7: Design a unique ID generator in a distributed system”.\n\nThe final approach is to use local sequence number generator. Local means IDs are only\nunique within a group. The reason why local IDs work is that maintaining message sequence\nwithin one-on-one channel or a group channel is sufficient. This approach is easier to\nimplement in comparison to the global ID implementation."
    },
    {
      "page": 190,
      "text": "Step 3 - Design deep dive\nIn a system design interview, usually you are expected to dive deep into some of the\n\ncomponents in the high-level design. For the chat system, service discovery, messaging\nflows, and online/offline indicators worth deeper exploration.\n\nService discovery\n\nThe primary role of service discovery is to recommend the best chat server for a client based\non the criteria like geographical location, server capacity, etc. Apache Zookeeper [7] is a\npopular open-source solution for service discovery. It registers all the available chat servers\nand picks the best chat server for a client based on predefined criteria.\n\nFigure 12-11 shows how service discovery (Zookeeper) works.\n\nLoad balancer\n\nChat server 2\n\nAPI servers\n\nService discovery (Zookeeper)\n\nChat server 1 Chat server 2 Chat server N\n\nFigure 12-11\n\n1. User A tries to log in to the app.\n2. The load balancer sends the login request to API servers.\n\n3. After the backend authenticates the user, service discovery finds the best chat server for\nUser A. In this example, server 2 is chosen and the server info is returned back to User A.\n\n4. User A connects to chat server 2 through WebSocket.\n\nMessage flows\nIt is interesting to understand the end-to-end flow of a chat system. In this section, we will"
    },
    {
      "page": 191,
      "text": "explore 1 on 1 chat flow, message synchronization across multiple devices and group chat\nflow.\n\n1 on 1 chat flow\nFigure 12-12 explains what happens when User A sends a message to User B.\n\nChat server 2\n\n5.a | online\n\nMessage sync queue\n\noffline\n\nPN servers\n\nFigure 12-12\n1. User A sends a chat message to Chat server 1.\n2. Chat server 1 obtains a message ID from the ID generator.\n3. Chat server 1 sends the message to the message sync queue.\n4. The message is stored in a key-value store.\n\n5.a. If User B is online, the message is forwarded to Chat server 2 where User B is\nconnected.\n\n5.b. If User B is offline, a push notification is sent from push notification (PN) servers.\n6. Chat server 2 forwards the message to User B. There is a persistent WebSocket\nconnection between User B and Chat server 2.\n\nMessage synchronization across multiple devices\n\nMany users have multiple devices. We will explain how to sync messages across multiple\ndevices. Figure 12-13 shows an example of message synchronization."
    },
    {
      "page": 192,
      "text": "cur_max_message_id = 653 cur_max_message_id = 842\n\nal Session for User A's laptop\n\nSession for User A's phone\nChat server 1\n\nFigure 12-13\n\nIn Figure 12-13, user A has two devices: a phone and a laptop. When User A logs in to the\nchat app with her phone, it establishes a WebSocket connection with Chat server 1. Similarly,\nthere is a connection between the laptop and Chat server 1.\n\nEach device maintains a variable called cur_max_message_id, which keeps track of the latest\nmessage ID on the device. Messages that satisfy the following two conditions are considered\nas News messages:\n\n* The recipient ID is equal to the currently logged-in user ID.\n* Message ID in the key-value store is larger than cur_max_message_id .\n\nWith distinct cur_max_message_id on each device, message synchronization is easy as each\ndevice can get new messages from the KV store.\n\nSmall group chat flow\n\nIn comparison to the one-on-one chat, the logic of group chat is more complicated. Figures\n12-14 and 12-15 explain the flow."
    },
    {
      "page": 193,
      "text": "Chat server 1\n\n(MMM CI\nUser B\nMessage sync queue\n\nAa)a]al 0\n\nUser C\nMessage sync queue\n\nFigure 12-14\n\nFigure 12-14 explains what happens when User A sends a message in a group chat. Assume\nthere are 3 members in the group (User A, User B and user C). First, the message from User\nA is copied to each group member’s message sync queue: one for User B and the second for\nUser C. You can think of the message sync queue as an inbox for a recipient. This design\nchoice is good for small group chat because:\n\n* it simplifies message sync flow as each client only needs to check its own inbox to get\nnew messages.\n* when the group number is small, storing a copy in each recipient’s inbox is not too\nexpensive.\nWeChat uses a similar approach, and it limits a group to 500 members [8]. However, for\ngroups with a lot of users, storing a message copy for each member is not acceptable.\nOn the recipient side, a recipient can receive messages from multiple users. Each recipient\n\nhas an inbox (message sync queue) which contains messages from different senders. Figure\n12-15 illustrates the design."
    },
    {
      "page": 194,
      "text": "Chat server 2\n\nChat server 1\n\nMessage sync queue\nFigure 12-15\nOnline presence\n\nAn online presence indicator is an essential feature of many chat applications. Usually, you\ncan see a green dot next to a user’s profile picture or username. This section explains what\nhappens behind the scenes.\n\nIn the high-level design, presence servers are responsible for managing online status and\ncommunicating with clients through WebSocket. There are a few flows that will trigger\nonline status change. Let us examine each of them.\n\nUser login\n\nThe user login flow is explained in the “Service Discovery” section. After a WebSocket\nconnection is built between the client and the real-time service, user A’s online status and\nlast_active_at timestamp are saved in the KV store. Presence indicator shows the user is\nonline after she logs in.\n\nC4 ws connection\nPresence servers\n\nUser A\n\nUser A: {status: online,\nlast_active_at: timestamp\n\nFigure 12-16"
    },
    {
      "page": 195,
      "text": "User logout\n\nWhen a user logs out, it goes through the user logout flow as shown in Figure 12-17. The\nonline status is changed to offline in the KV store. The presence indicator shows a user is\noffline.\n\n= .\n=>\nie\nO tin\nPresence servers |\ni\nUser A\n\nKV store\n\nUser A: {status:offline}\n\nFigure 12-17\n\nUser disconnection\n\nWe all wish our internet connection is consistent and reliable. However, that is not always the\ncase; thus, we must address this issue in our design. When a user disconnects from the\ninternet, the persistent connection between the client and server is lost. A naive way to handle\nuser disconnection is to mark the user as offline and change the status to online when the\nconnection re-establishes. However, this approach has a major flaw. It is common for users to\ndisconnect and reconnect to the internet frequently in a short time. For example, network\nconnections can be on and off while a user goes through a tunnel. Updating online status on\nevery disconnect/reconnect would make the presence indicator change too often, resulting in\npoor user experience.\n\nWe introduce a heartbeat mechanism to solve this problem. Periodically, an online client\nsends a heartbeat event to presence servers. If presence servers receive a heartbeat event\nwithin a certain time, say x seconds from the client, a user is considered as online. Otherwise,\nit is offline.\n\nIn Figure 12-18, the client sends a heartbeat event to the server every 5 seconds. After\nsending 3 heartbeat events, the client is disconnected and does not reconnect within x = 30\nseconds (This number is arbitrarily chosen to demonstrate the logic). The online status is\nchanged to offline."
    },
    {
      "page": 196,
      "text": "heartbeat\nr ) Heartbeat received. Status is online\n\nheartbeat\n@ Heartbeat received. Status is online\n\nheartbeat\n[ ) Heartbeat received. Status is online\n\nr ) No heartbeat after 30 seconds\nChange status to offline\n\nFigure 12-18\nOnline status fanout\nHow do user A’s friends know about the status changes? Figure 12-19 explains how it works.\nPresence servers use a publish-subscribe model, in which each friend pair maintains a\nchannel. When User A’s online status changes, it publishes the event to three channels,\nchannel A-B, A-C, and A-D. Those three channels are subscribed by User B, C, and D,\nrespectively. Thus, it is easy for friends to get online status updates. The communication\nbetween clients and servers is through real-time WebSocket.\n\n« ‘subscribe C\n\nUser B\n\nChannel A-B\n\nUser Channel A-C User C\n\n< subscribe. |\n\nUser D\n\nMeh | « subscribe a\n\nFigure 12-19"
    },
    {
      "page": 197,
      "text": "The above design is effective for a small user group. For instance, WeChat uses a similar\napproach because its user group is capped to 500. For larger groups, informing all members\nabout online status is expensive and time consuming. Assume a group has 100,000 members.\nEach status change will generate 100,000 events. To solve the performance bottleneck, a\npossible solution is to fetch online status only when a user enters a group or manually\nrefreshes the friend list."
    },
    {
      "page": 198,
      "text": "Step 4 - Wrap up\n\nIn this chapter, we presented a chat system architecture that supports both 1-to-1 chat and\nsmall group chat. WebSocket is used for real-time communication between the client and\nserver. The chat system contains the following components: chat servers for real-time\nmessaging, presence servers for managing online presence, push notification servers for\nsending push notifications, key-value stores for chat history persistence and API servers for\nother functionalities.\n\nIf you have extra time at the end of the interview, here are additional talking points:\n* Extend the chat app to support media files such as photos and videos. Media files are\nsignificantly larger than text in size. Compression, cloud storage, and thumbnails are\ninteresting topics to talk about.\n¢ End-to-end encryption. Whatsapp supports end-to-end encryption for messages. Only the\nsender and the recipient can read messages. Interested readers should refer to the article in\nthe reference materials [9].\n* Caching messages on the client-side is effective to reduce the data transfer between the\nclient and server.\n¢ Improve load time. Slack built a geographically distributed network to cache users’ data,\nchannels, etc. for better load time [10].\n¢ Error handling.\n* The chat server error. There might be hundreds of thousands, or even more persistent\nconnections to a chat server. If a chat server goes offline, service discovery\n(Zookeeper) will provide a new chat server for clients to establish new connections\nwith.\n\n* Message resent mechanism. Retry and queueing are common techniques for\nresending messages.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 199,
      "text": "Reference materials\n\n[1] Erlang at Facebook: https://www.erlang-\nfactory.com/upload/presentations/31/EugeneLetuchy-ErlangatFacebook.pdf\n\n[2] Messenger and WhatsApp process 60 billion messages a day:\nhttps://www.theverge.com/2016/4/12/11415198/facebook-messenger-whatsapp-number-\nmessages-vs-sms-f8-2016\n\n[3] Long tail: https://en.wikipedia.org/wiki/Long tail\n[4] The Underlying Technology of Messages: https://www.facebook.com/notes/facebook-\nengineering/the-underlying-technology-of-messages/454991608919/\n\n[5] How Discord Stores Billions of Messages: https://blog.discordapp.com/how-discord-\nstores-billions-of-messages-7fa6ec7ee4c7\n\n[6] Announcing Snowflake: https://blog.twitter.com/engineering/en_us/a/2010/announcing-\nsnowflake.html\n\n[7] Apache ZooKeeper: https://zookeeper.apache.org/\n\n[8] From nothing: the evolution of WeChat background system (Article in Chinese):\nhttps://www.infog.cn/article/the-road-of-the-growth-weixin-background\n\n[9] End-to-end encryption: https://fag.whatsapp.com/en/android/28030015/\n\n[10] Flannel: An Application-Level Edge Cache to Make Slack Scale:\nhttps://slack.engineering/flannel-an-application-level-edge-cache-to-make-slack-scale-\nb8a6400e2f6b"
    },
    {
      "page": 200,
      "text": "CHAPTER 13: DESIGN A SEARCH AUTOCOMPLETE\nSYSTEM\n\nWhen searching on Google or shopping at Amazon, as you type in the search box, one or\nmore matches for the search term are presented to you. This feature is referred to as\nautocomplete, typeahead, search-as-you-type, or incremental search. Figure 13-1 presents an\nexample of a Google search showing a list of autocompleted results when “dinner” is typed\ninto the search box. Search autocomplete is an important feature of many products. This leads\nus to the interview question: design a search autocomplete system, also called “design top k”\nor “design top k most searched queries”.\n\nGoogle\n\ndinner\n\nfC\n\ndinner ideas\ndinner recipes\ndinner near me\ndinner\n\ndinnerly\n\ndinner in spanish\ndinner nearby\ndinner tonight\ndinner rolls\n\ndinnerware sets\n\nGoogle Search I'm Feeling Lucky\n\nReport ina|\n\nFigure 13-1"
    },
    {
      "page": 201,
      "text": "Step 1 - Understand the problem and establish design scope\nThe first step to tackle any system design interview question is to ask enough questions to\nclarify requirements. Here is an example of candidate-interviewer interaction:\n\nCandidate: Is the matching only supported at the beginning of a search query or in the\nmiddle as well?\nInterviewer: Only at the beginning of a search query.\n\nCandidate: How many autocomplete suggestions should the system return?\nInterviewer: 5\n\nCandidate: How does the system know which 5 suggestions to return?\nInterviewer: This is determined by popularity, decided by the historical query frequency.\n\nCandidate: Does the system support spell check?\nInterviewer: No, spell check or autocorrect is not supported.\n\nCandidate: Are search queries in English?\nInterviewer: Yes. If time allows at the end, we can discuss multi-language support.\n\nCandidate: Do we allow capitalization and special characters?\nInterviewer: No, we assume all search queries have lowercase alphabetic characters.\n\nCandidate: How many users use the product?\nInterviewer: 10 million DAU.\nRequirements\nHere is a summary of the requirements:\nFast response time: As a user types a search query, autocomplete suggestions must show\nup fast enough. An article about Facebook’s autocomplete system [1] reveals that the\nsystem needs to return results within 100 milliseconds. Otherwise it will cause stuttering.\n* Relevant: Autocomplete suggestions should be relevant to the search term.\n* Sorted: Results returned by the system must be sorted by popularity or other ranking\nmodels.\n* Scalable: The system can handle high traffic volume.\n* Highly available: The system should remain available and accessible when part of the\nsystem is offline, slows down, or experiences unexpected network errors.\nBack of the envelope estimation\n« Assume 10 million daily active users (DAU).\n« An average person performs 10 searches per day.\n+ 20 bytes of data per query string:\n« Assume we use ASCII character encoding. 1 character = 1 byte\n« Assume a query contains 4 words, and each word contains 5 characters on average.\n¢ That is 4 x 5 = 20 bytes per query.\n¢ For every character entered into the search box, a client sends a request to the backend\nfor autocomplete suggestions. On average, 20 requests are sent for each search query. For\n\nexample, the following 6 requests are sent to the backend by the time you finish typing\n“dinner”.\n\nsearch?q=d"
    },
    {
      "page": 202,
      "text": "search?q=di\nsearch?q=din\nsearch?q=dinn\nsearch?q=dinne\nsearch?q=dinner\n\n« ~24,000 query per second (QPS) = 10,000,000 users * 10 queries / day * 20 characters /\n24 hours / 3600 seconds.\n\n* Peak QPS = QPS * 2 = ~48,000\n« Assume 20% of the daily queries are new. 10 million * 10 queries / day * 20 byte per\nquery * 20% = 0.4 GB. This means 0.4GB of new data is added to storage daily."
    },
    {
      "page": 203,
      "text": "Step 2 - Propose high-level design and get buy-in\nAt the high-level, the system is broken down into two:\n\n* Data gathering service: It gathers user input queries and aggregates them in real-time.\nReal-time processing is not practical for large data sets; however, it is a good starting\npoint. We will explore a more realistic solution in deep dive.\n\n* Query service: Given a search query or prefix, return 5 most frequently searched terms.\n\nData gathering service\n\nLet us use a simplified example to see how data gathering service works. Assume we have a\nfrequency table that stores the query string and its frequency as shown in Figure 13-2. In the\nbeginning, the frequency table is empty. Later, users enter queries “twitch”, “twitter”,\n“twitter,” and “twillo” sequentially. Figure 13-2 shows how the frequency table is updated.\n\nquery: twitch : query: twitter 4 query: twitter i query: twillo\nQuery | Frequency : Query | Frequency ; Query) Frequency q Query| Frequency i Query | Frequency\ntwitch 1 + | twitch 1 : ; .\n' ‘| twitch 1 1 | twitch 1\ntwitter 1 : '\ntwitter 2 twitter 2\nFigure 13-2\n\nQuery service\n\nAssume we have a frequency table as shown in Table 13-1. It has two fields.\n* Query: it stores the query string.\n* Frequency: it represents the number of times a query has been searched."
    },
    {
      "page": 204,
      "text": "When a user types “tw\n\nQuery Frequency\nTwitter 35\n\ntwitch 29\n\ntwilight 25\n\ntwin peak 21\n\ntwitch prime 18\n\ntwitter search 14\n\ntwillo 10\n\ntwin peak sf 8\n\nTable 13-1\n\n(Figure 13-3), assuming the frequency table is based on Table 13-1.\n\nTo get top 5 frequently searched queries, execute the following SQL query:\n\nSELECT * FROM frequency_table\n\ntw\ntwitter\ntwitch\ntwilight\ntwin peak\n\ntwitch prime\n\nFigure 13-3\n\nWHERE query Like ~prefix%\nORDER BY frequency DESC\nLIMIT 5\n\nFigure 13-4\n\n” in the search box, the following top 5 searched queries are displayed"
    },
    {
      "page": 205,
      "text": "This is an acceptable solution when the data set is small. When it is large, accessing the\ndatabase becomes a bottleneck. We will explore optimizations in deep dive."
    },
    {
      "page": 206,
      "text": "Step 3 - Design deep dive\nIn the high-level design, we discussed data gathering service and query service. The high-\nlevel design is not optimal, but it serves as a good starting point. In this section, we will dive\ndeep into a few components and explore optimizations as follows:\n\n+ Trie data structure\n\n* Data gathering service\n\n* Query service\n\n* Scale the storage\n\n¢ Trie operations\n\nTrie data structure\n\nRelational databases are used for storage in the high-level design. However, fetching the top\n5 search queries from a relational database is inefficient. The data structure trie (prefix tree) is\nused to overcome the problem. As trie data structure is crucial for the system, we will\ndedicate significant time to design a customized trie. Please note that some of the ideas are\nfrom articles [2] and [3].\n\nUnderstanding the basic trie data structure is essential for this interview question. However,\nthis is more of a data structure question than a system design question. Besides, many online\nmaterials explain this concept. In this chapter, we will only discuss an overview of the trie\ndata structure and focus on how to optimize the basic trie to improve response time.\n\nTrie (pronounced “try”) is a tree-like data structure that can compactly store strings. The\nname comes from the word retrieval, which indicates it is designed for string retrieval\noperations. The main idea of trie consists of the following:\n\n+ A trie is a tree-like data structure.\n\n¢ The root represents an empty string.\n\n* Each node stores a character and has 26 children, one for each possible character. To\nsave space, we do not draw empty links.\n\n* Each tree node represents a single word or a prefix string.\n\n99 ep 99 66, 29 66, 0&6,\n\nFigure 13-5 shows a trie with search queries “tree”, “try”, “true”, “toy”,\nSearch queries are highlighted with a thicker border.\n\nwish”, “win”."
    },
    {
      "page": 207,
      "text": "Figure 13-5\n\nBasic trie data structure stores characters in nodes. To support sorting by frequency,\nfrequency info needs to be included in nodes. Assume we have the following frequency table.\n\nQuery Frequency\ntree 10\ntry 29\ntrue 35\ntoy 14\nwish 25\nwin 50\nTable 13-2\n\nAfter adding frequency info to nodes, updated trie data structure is shown in Figure 13-6."
    },
    {
      "page": 208,
      "text": "Figure 13-6\nHow does autocomplete work with trie? Before diving into the algorithm, let us define some\nterms.\n+ p: length of a prefix\n* n: total number of nodes in a trie\n* c: number of children of a given node\n\nSteps to get top k most searched queries are listed below:\n1. Find the prefix. Time complexity: O(p).\n2. Traverse the subtree from the prefix node to get all valid children. A child is valid if it\ncan form a valid query string. Time complexity: O(c)\n3. Sort the children and get top k. Time complexity: O(clogc)\nLet us use an example as shown in Figure 13-7 to explain the algorithm. Assume k equals to\n2 and a user types “tr” in the search box. The algorithm works as follows:\n* Step 1: Find the prefix node “tr”.\n* Step 2: Traverse the subtree to get all valid children. In this case, nodes [tree: 10], [true:\n35], [try: 29] are valid.\n* Step 3: Sort the children and get top 2. [true: 35] and [try: 29] are the top 2 queries with\nprefix “tr”."
    },
    {
      "page": 209,
      "text": "| win: 50 |\n\nwish: 25\n\n¥\n@) Top 2: [best: 35, bet: 29]\n\nFigure 13-7\n\nThe time complexity of this algorithm is the sum of time spent on each step mentioned above:\nO(p) + O(c) + O(clogc)\nThe above algorithm is straightforward. However, it is too slow because we need to traverse\nthe entire trie to get top k results in the worst-case scenario. Below are two optimizations:\n\n1. Limit the max length of a prefix\n\n2. Cache top search queries at each node\nLet us look at these optimizations one by one.\n\nLimit the max length of a prefix\n\nUsers rarely type a long search query into the search box. Thus, it is safe to say p is a small\ninteger number, say 50. If we limit the length of a prefix, the time complexity for “Find the\nprefix” can be reduced from O(p) to O(small constant), aka O(1).\n\nCache top search queries at each node\n\nTo avoid traversing the whole trie, we store top k most frequently used queries at each node.\nSince 5 to 10 autocomplete suggestions are enough for users, k is a relatively small number.\nIn our specific case, only the top 5 search queries are cached.\n\nBy caching top search queries at every node, we significantly reduce the time complexity to\nretrieve the top 5 queries. However, this design requires a lot of space to store top queries at\nevery node. Trading space for time is well worth it as fast response time is very important.\n\nFigure 13-8 shows the updated trie data structure. Top 5 queries are stored on each node. For\nexample, the node with prefix “be” stores the following: [best: 35, bet: 29, bee: 20, be: 15,\nbeer: 10]."
    },
    {
      "page": 210,
      "text": "[best: 35, bet: 29, bee: 20,\nbe: 15, buy: 14]\n\n| [best: 35, bet: 29, bee:\n1 20, be: 15, beer: 10]\n\nwin: 11\n\n[bee: 20, beer, 10) bee: 20 : \\[best: 35]\n1\n\neoaens eS\n\nbeer: 10 | best: 35\n\nFigure 13-8\nLet us revisit the time complexity of the algorithm after applying those two optimizations:\n1. Find the prefix node. Time complexity: O(1)\n2. Return top k. Since top k queries are cached, the time complexity for this step is O(1).\n\nAs the time complexity for each of the steps is reduced to O(1), our algorithm takes only\nO(1) to fetch top k queries.\n\nData gathering service\n\nIn our previous design, whenever a user types a search query, data is updated in real-time.\nThis approach is not practical for the following two reasons:\n\n* Users may enter billions of queries per day. Updating the trie on every query\nsignificantly slows down the query service.\n* Top suggestions may not change much once the trie is built. Thus, it is unnecessary to\nupdate the trie frequently.\nTo design a scalable data gathering service, we examine where data comes from and how\ndata is used. Real-time applications like Twitter require up to date autocomplete suggestions.\nHowever, autocomplete suggestions for many Google keywords might not change much on a\ndaily basis.\n\nDespite the differences in use cases, the underlying foundation for data gathering service\nremains the same because data used to build the trie is usually from analytics or logging\nservices.\n\nFigure 13-9 shows the redesigned data gathering service. Each component is examined one\nby one."
    },
    {
      "page": 211,
      "text": "fini ak me: s\n—|—\n1\n\n1\n\nf\n\n'\n\nAggregators !\n\n'\n| fae\n'\n\n1\n\n1\n\n1\n\n‘\n\nLOG Update weekly\n\nAnalytics Logs Trie DB\n\nFigure 13-9\n\nAnalytics Logs. It stores raw data about search queries. Logs are append-only and are not\nindexed. Table 13-3 shows an example of the log file.\n\nquery time\ntree 2019-10-01 22:01:01\ntry 2019-10-01 22:01:05\ntree 2019-10-01 22:01:30\ntoy 2019-10-01 22:02:22\ntree 2019-10-02 22:02:42\ntry 2019-10-03 22:03:03\nTable 13-3\n\nAggregators. The size of analytics logs is usually very large, and data is not in the right\nformat. We need to aggregate data so it can be easily processed by our system.\n\nDepending on the use case, we may aggregate data differently. For real-time applications\nsuch as Twitter, we aggregate data in a shorter time interval as real-time results are important.\nOn the other hand, aggregating data less frequently, say once per week, might be good\nenough for many use cases. During an interview session, verify whether real-time results are\nimportant. We assume trie is rebuilt weekly.\n\nAggregated Data.\n\nTable 13-4 shows an example of aggregated weekly data. “time” field represents the start\ntime of a week. “frequency” field is the sum of the occurrences for the corresponding query\nin that week."
    },
    {
      "page": 212,
      "text": "query Time frequency\ntree 2019-10-01 12000\n\ntree 2019-10-08 15000\n\ntree 2019-10-15 9000\n\ntoy 2019-10-01 8500\n\ntoy 2019-10-08 6256\n\ntoy 2019-10-15 8866\n\nTable 13-4\n\nWorkers. Workers are a set of servers that perform asynchronous jobs at regular intervals.\nThey build the trie data structure and store it in Trie DB.\n\nTrie Cache. Trie Cache is a distributed cache system that keeps trie in memory for fast read.\nIt takes a weekly snapshot of the DB.\n\nTrie DB. Trie DB is the persistent storage. Two options are available to store the data:\n\n1. Document store: Since a new trie is built weekly, we can periodically take a snapshot of it,\nserialize it, and store the serialized data in the database. Document stores like MongoDB [4]\nare good fits for serialized data.\n\n2. Key-value store: A trie can be represented in a hash table form [4] by applying the\nfollowing logic:\n\n« Every prefix in the trie is mapped to a key in a hash table.\n\n* Data on each trie node is mapped to a value in a hash table.\nFigure 13-10 shows the mapping between the trie and hash table."
    },
    {
      "page": 213,
      "text": "}<— root\nLd Key Value\nptt Soe Woes b [be: 15, bee: 20,\n' [be: 15, bee: 20, b Beene nnctee estes >| beer: 10, best: 35]\n; beer: 10, best: 35] 1\nee —___ ;\nbe [be: 15, bee: 20,\nween Sees esl es ee ee cee? beer: 10, best: 35]\n1 1 oes\n[be: 15, bee: 20, . eee\nH beer: 10, best: 35] een 4\nbee [bee: 20, beer: 10]\nbes [best: 35]\nbeer [beer: 10]\nLees  ———— best [best: 35]\nbeer: 10 best: 35 Fr---70-0 7\n\nFigure 13-10\n\nIn Figure 13-10, each trie node on the left is mapped to the <key, value> pair on the right. If\nyou are unclear how key-value stores work, refer to Chapter 6: Design a key-value store.\n\nQuery service\n\nIn the high-level design, query service calls the database directly to fetch the top 5 results.\nFigure 13-11 shows the improved design as previous design is inefficient."
    },
    {
      "page": 214,
      "text": "User\n\nm\n\nWeb browser Mobile app\n\nAPI servers,\n\n‘\neee = = -\n\nTrie Cache Trie DB\n\nFigure 13-11\n\n1. A search query is sent to the load balancer.\n2. The load balancer routes the request to API servers.\n\n3. API servers get trie data from Trie Cache and construct autocomplete suggestions for\nthe client.\n\n4. In case the data is not in Trie Cache, we replenish data back to the cache. This way, all\nsubsequent requests for the same prefix are returned from the cache. A cache miss can\nhappen when a cache server is out of memory or offline.\n\nQuery service requires lightning-fast speed. We propose the following optimizations:\n\n* AJAX request. For web applications, browsers usually send AJAX requests to fetch\nautocomplete results. The main benefit of AJAX is that sending/receiving a\nrequest/response does not refresh the whole web page.\n\n* Browser caching. For many applications, autocomplete search suggestions may not\nchange much within a short time. Thus, autocomplete suggestions can be saved in browser\ncache to allow subsequent requests to get results from the cache directly. Google search\nengine uses the same cache mechanism. Figure 13-12 shows the response header when\nyou type “system design interview” on the Google search engine. As you can see, Google"
    },
    {
      "page": 215,
      "text": "caches the results in the browser for 1 hour. Please note: “private” in cache-control means\nresults are intended for a single user and must not be cached by a shared cache. “max-\nage=3600” means the cache is valid for 3600 seconds, aka, an hour.\n\nRequest URL: https: //www. google.com/complete/search?q&cp=0&client=psy-ab&xssi=t&gs_ri=gws-wiz&hl=en&authuser=e&pq=system design interview!\n\nRequest method: GET\nRemote address: [2607:f8b@:49@5: 807: :2004]:443\n\n@D ok @ _ EditandResend Raw headers\n\nVersion: HTTP/2.0\n\nStatus code:\n\nFilter headers\nResponse headers (615 B)\nalt-svc:_quic=\":443\", ma=2592000; v=\"46...00,h3-Q043=\":443\"; ma=2592000\n1600\n\ncontent-disposition: attachment, filename=\"f.t\n\ncache-control: private, max-age\n\ncontent-encoding: br\n\ncontent-type: application/json; charset=UTF-8\n\ndate: Tue, 17 Dec 2019 22:52:01 GMT\n\nexpires: Tue, 17 Dec 2019 22:52:01 GMT\n\nserver: gws\n\nstrict-transport-security: max-age=31536000\n\ntrailer: X-Google-GFE-Current-Request-Cost-From-GWS\nX-Firefox-Spdy: h2\n\nx-frame-options: SAMEORIGIN\n\nx-xss-protection: 0\n\nFigure 13-12\n\n* Data sampling: For a large-scale system, logging every search query requires a lot of\nprocessing power and storage. Data sampling is important. For instance, only 1 out of\nevery N requests is logged by the system.\n\nTrie operations\n\nTrie is a core component of the autocomplete system. Let us look at how trie operations\n(create, update, and delete) work.\n\nCreate\n\nTrie is created by workers using aggregated data. The source of data is from Analytics\nLog/DB.\n\nUpdate\n\nThere are two ways to update the trie.\n\nOption 1: Update the trie weekly. Once a new trie is created, the new trie replaces the old\none.\n\nOption 2: Update individual trie node directly. We try to avoid this operation because it is\nslow. However, if the size of the trie is small, it is an acceptable solution. When we update a\ntrie node, its ancestors all the way up to the root must be updated because ancestors store top\nqueries of children. Figure 13-13 shows an example of how the update operation works. On\nthe left side, the search query “beer” has the original value 10. On the right side, it is updated\nto 30. As you can see, the node and its ancestors have the “beer” value updated to 30."
    },
    {
      "page": 216,
      "text": "}<——-_ root }<— root\n\n‘\n' [best: 35, bee: 20,\n1\n1\n\n'\n1 [best: 35, beer: 30,\nbe: 15, beer: 10] ' }\n,\n\n'\n1\n\nbee: 20, be: 15] '\n——\n\n| best: 35, bee: 20,\n1 be: 15, beer: 10]\n‘\n\n[best: 35, beer: 30,\n1 bee: 20, be: 15]\n‘\n\nfa\ntc}\n\na\nawe #\nfom\n2\na\nvo--?\n\n‘\n\n[beer, 30, bee: 2a bee: 20 | \\[best: 35]\n1\n\na ee ie\n\n| beer: 30 best: 35\n\nFigure 13-13\nDelete\n\nWe have to remove hateful, violent, sexually explicit, or dangerous autocomplete\nsuggestions. We add a filter layer (Figure 13-14) in front of the Trie Cache to filter out\nunwanted suggestions. Having a filter layer gives us the flexibility of removing results based\non different filter rules. Unwanted suggestions are removed physically from the database\nasynchronically so the correct data set will be used to build trie in the next update cycle.\n\nFilter Layer Trie Cache\n\nFigure 13-14\n\nScale the storage\n\nNow that we have developed a system to bring autocomplete queries to users, it is time to\nsolve the scalability issue when the trie grows too large to fit in one server.\n\nSince English is the only supported language, a naive way to shard is based on the first\ncharacter. Here are some examples.\n\n+ If we need two servers for storage, we can store queries starting with ‘a’ to ‘m’ on the\nfirst server, and ‘n’ to ‘z’ on the second server.\n\n65> 689\n\n+ If we need three servers, we can split queries into ‘a’ to ‘i’, ‘j’ to ‘r’ and ‘s’ to ‘z’.\n\nFollowing this logic, we can split queries up to 26 servers because there are 26 alphabetic\ncharacters in English. Let us define sharding based on the first character as first level\nsharding. To store data beyond 26 servers, we can shard on the second or even at the third\nlevel. For example, data queries that start with ‘a’ can be split into 4 servers: ‘aa-ag’, ‘ah-\nan’, ‘ao-au’, and ‘av-az’.\n\nAt the first glance this approach seems reasonable, until you realize that there are a lot more\nwords that start with the letter ‘c’ than ‘x’. This creates uneven distribution."
    },
    {
      "page": 217,
      "text": "To mitigate the data imbalance problem, we analyze historical data distribution pattern and\napply smarter sharding logic as shown in Figure 13-15. The shard map manager maintains a\nlookup database for identifying where rows should be stored. For example, if there are a\nsimilar number of historical queries for ‘s’ and for ‘u’, ‘v’, ‘w’, ‘x’, ‘y’ and ‘z’ combined, we\ncan maintain two shards: one for ‘s’ and one for ‘u’ to ‘z’.\n\nShard map manager\n\n—-\nShard ...\n\n= =>\nShard 1 Shard 2\n\nDatabases\n\nFigure 13-15"
    },
    {
      "page": 218,
      "text": "Step 4 - Wrap up\n\nAfter you finish the deep dive, your interviewer might ask you some follow up questions.\nInterviewer: How do you extend your design to support multiple languages?\n\nTo support other non-English queries, we store Unicode characters in trie nodes. If you are\nnot familiar with Unicode, here is the definition: “an encoding standard covers all the\ncharacters for all the writing systems of the world, modern and ancient” [5].\n\nInterviewer: What if top search queries in one country are different from others?\n\nIn this case, we might build different tries for different countries. To improve the response\ntime, we can store tries in CDNs.\n\nInterviewer: How can we support the trending (real-time) search queries?\nAssuming a news event breaks out, a search query suddenly becomes popular. Our original\ndesign will not work because:\n* Offline workers are not scheduled to update the trie yet because this is scheduled to run\non weekly basis.\n+ Even if it is scheduled, it takes too long to build the trie.\nBuilding a real-time search autocomplete is complicated and is beyond the scope of this book\nso we will only give a few ideas:\n* Reduce the working data set by sharding.\n* Change the ranking model and assign more weight to recent search queries.\n\n* Data may come as streams, so we do not have access to all the data at once. Streaming\ndata means data is generated continuously. Stream processing requires a different set of\nsystems: Apache Hadoop MapReduce [6], Apache Spark Streaming [7], Apache Storm\n\n[8], Apache Kafka [9], etc. Because all those topics require specific domain knowledge,\nwe are not going into detail here.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 219,
      "text": "Reference materials\n[1] The Life of a Typeahead Query: https://www.facebook.com/notes/facebook-\nengineering/the-life-of-a-typeahead-query/389105248919/\n\n[2] How We Built Prefixy: A Scalable Prefix Search Service for Powering Autocomplete:\nhttps://medium.com/@prefixyteam/how-we-built-prefixy-a-scalable-prefix-search-service-\nfor-powering-autocomplete-c20f98e2eff1\n\n[3] Prefix Hash Tree An Indexing Data Structure over Distributed Hash Tables:\nhttps://people.eecs.berkeley.edu/~sylvia/papers/pht.pdf\n\n[4] MongoDB wikipedia: https://en.wikipedia.org/wiki/MongoDB\n\n[5] Unicode frequently asked questions: https://www.unicode.org/faq/basic_g.htm]\n[6] Apache hadoop: https://hadoop.apache.org/\n\n[7] Spark streaming: https://spark.apache.org/streaming/\n\n[8] Apache storm: https://storm.apache.org/\n[9] Apache kafka: https://kafka.apache.org/documentation/"
    },
    {
      "page": 220,
      "text": "CHAPTER 14: DESIGN YOUTUBE\n\nIn this chapter, you are asked to design YouTube. The solution to this question can be applied\nto other interview questions like designing a video sharing platform such as Netflix and Hulu.\nFigure 14-1 shows the YouTube homepage.\n\nFigure 14-1\n\n‘YouTube looks simple: content creators upload videos and viewers click play. Is it really that\nsimple? Not really. There are lots of complex technologies underneath the simplicity. Let us\nlook at some impressive statistics, demographics, and fun facts of YouTube in 2020 [1] [2].\n\n* Total number of monthly active users: 2 billion.\n\n* Number of videos watched per day: 5 billion.\n\n* 73% of US adults use YouTube.\n\n* 50 million creators on YouTube.\n\n* YouTube’s Ad revenue was $15.1 billion for the full year 2019, up 36% from 2018.\n* YouTube is responsible for 37% of all mobile internet traffic.\n\n* YouTube is available in 80 different languages.\n\nFrom these statistics, we know YouTube is enormous, global and makes a lot of money."
    },
    {
      "page": 221,
      "text": "Step 1 - Understand the problem and establish design scope\n\nAs revealed in Figure 14-1, besides watching a video, you can do a lot more on YouTube. For\nexample, comment, share, or like a video, save a video to playlists, subscribe to a channel,\netc. It is impossible to design everything within a 45- or 60-minute interview. Thus, it is\nimportant to ask questions to narrow down the scope.\n\nCandidate: What features are important?\nInterviewer: Ability to upload a video and watch a video.\n\nCandidate: What clients do we need to support?\nInterviewer: Mobile apps, web browsers, and smart TV.\n\nCandidate: How many daily active users do we have?\nInterviewer: 5 million\n\nCandidate: What is the average daily time spent on the product?\nInterviewer: 30 minutes.\n\nCandidate: Do we need to support international users?\nInterviewer: Yes, a large percentage of users are international users.\n\nCandidate: What are the supported video resolutions?\nInterviewer: The system accepts most of the video resolutions and formats.\n\nCandidate: Is encryption required?\nInterviewer: Yes\n\nCandidate: Any file size requirement for videos?\nInterviewer: Our platform focuses on small and medium-sized videos. The maximum\nallowed video size is 1GB.\n\nCandidate: Can we leverage some of the existing cloud infrastructures provided by Amazon,\nGoogle, or Microsoft?\nInterviewer: That is a great question. Building everything from scratch is unrealistic for most\ncompanies, it is recommended to leverage some of the existing cloud services.\nIn the chapter, we focus on designing a video streaming service with the following features:\n\n+ Ability to upload videos fast\n\n* Smooth video streaming\n\n« Ability to change video quality\n\n+ Low infrastructure cost\n\n* High availability, scalability, and reliability requirements\n\n* Clients supported: mobile apps, web browser, and smart TV\n\nBack of the envelope estimation\n\nThe following estimations are based on many assumptions, so it is important to communicate\nwith the interviewer to make sure she is on the same page.\n\n« Assume the product has 5 million daily active users (DAU).\n\n* Users watch 5 videos per day.\n\n* 10% of users upload 1 video per day.\n\n« Assume the average video size is 300 MB.\n\n* Total daily storage space needed: 5 million * 10% * 300 MB = 150TB"
    },
    {
      "page": 222,
      "text": "«CDN cost.\n\n* When cloud CDN serves a video, you are charged for data transferred out of the\n\nCDN.\n\n* Let us use Amazon’s CDN CloudFront for cost estimation (Figure 14-2) [3]. Assume\n100% of traffic is served from the United States. The average cost per GB is $0.02.\nFor simplicity, we only calculate the cost of video streaming.\n\n*5 million * 5 videos * 0.3GB * $0.02 = $150,000 per day.\nFrom the rough cost estimation, we know serving videos from the CDN costs lots of money.\n\nEven though cloud providers are willing to lower the CDN costs significantly for big\ncustomers, the cost is still substantial. We will discuss ways to reduce CDN costs in deep\n\ndive.\n\nPer Month\n\nFirst 10TB\nNext 40TB\nNext 100TB\nNext 350TB\nNext 524TB\nNext 4PB\n\nOver 5PB\n\nUnited\nStates &\nCanada\n\n$0.085\n$0.080\n$0.060\n$0.040\n$0.030\n$0.025\n\n$0.020\n\nEurope &\nIsrael\n\n$0.085\n$0.080\n$0.060\n$0.040\n$0.030\n$0.025\n\n$0.020\n\nSouth\nAfrica &\nMiddle East\n\n$0.110\n$0.105\n$0.090\n$0.080\n$0.060\n$0.050\n\n$0.040\n\nSouth\nAmerica\n\n0.110\n0.105\n$0.090\n$0.080\n0.060\n\n0.050\n\n0.040\n\nJapan\n\n$0.114\n$0.089\n$0.086\n$0.084\n$0.080\n$0.070\n\n$0.060\n\nAustralia\n\n$0.114\n$0.098\n$0.094\n$0.092\n$0.090\n$0.085\n\n$0.080\n\nSingapore,\nSouth Korea,\nTaiwan, Hong India\nKong, &\nPhilippines\n0.140 $0.170\n0.135 $0.130\n$0.120 $0.110\n0.100 $0.100\n0.080 $0.100\n$0.070 $0.100\n0.060 $0.100\n\nFigure 14-2 On-demand pricing for Data Transfer to the Internet (per GB)."
    },
    {
      "page": 223,
      "text": "Step 2 - Propose high-level design and get buy-in\n\nAs discussed previously, the interviewer recommended leveraging existing cloud services\ninstead of building everything from scratch. CDN and blob storage are the cloud services we\nwill leverage. Some readers might ask why not building everything by ourselves? Reasons\nare listed below:\n* System design interviews are not about building everything from scratch. Within the\nlimited time frame, choosing the right technology to do a job right is more important than\nexplaining how the technology works in detail. For instance, mentioning blob storage for\nstoring source videos is enough for the interview. Talking about the detailed design for\nblob storage could be an overkill.\n\n* Building scalable blob storage or CDN is extremely complex and costly. Even large\ncompanies like Netflix or Facebook do not build everything themselves. Netflix leverages\nAmazon’s cloud services [4], and Facebook uses Akamai’s CDN [5].\n\nAt the high-level, the system comprises three components (Figure 14-3).\n\nwees =\neee es\n\nFigure 14-3\nClient: You can watch YouTube on your computer, mobile phone, and smartTV.\nCDN: Videos are stored in CDN. When you press play, a video is streamed from the CDN.\n\nAPI servers: Everything else except video streaming goes through API servers. This includes\nfeed recommendation, generating video upload URL, updating metadata database and cache,\nuser signup, etc.\nIn the question/answer session, the interviewer showed interests in two flows:\n\n* Video uploading flow\n\n+ Video streaming flow\n\nWe will explore the high-level design for each of them.\n\nVideo uploading flow\nFigure 14-4 shows the high-level design for the video uploading."
    },
    {
      "page": 224,
      "text": "Original storage\n\ndy Load balancer\nT]\n\n, API servers\n\nMetadata Cache Metadata DB\n\n1 Completion\nCompletion queue \\ handler\n\nTranscoding H\nservers\n\nioe —-|——\n\n1\n\na |_|_| i transcoding complete\n' i\n\n1\n\n'\n\ni\n\nTranscoded\nstorage\n\nFigure 14-4\n\nIt consists of the following components:\n\n* User: A user watches YouTube on devices such as a computer, mobile phone, or smart\nTV.\n\n* Load balancer: A load balancer evenly distributes requests among API servers.\n* API servers: All user requests go through API servers except video streaming.\n\n* Metadata DB: Video metadata are stored in Metadata DB. It is sharded and replicated to\nmeet performance and high availability requirements.\n\n* Metadata cache: For better performance, video metadata and user objects are cached.\n\n* Original storage: A blob storage system is used to store original videos. A quotation in\nWikipedia regarding blob storage shows that: “A Binary Large Object (BLOB) is a\ncollection of binary data stored as a single entity in a database management system” [6].\n\n+ Transcoding servers: Video transcoding is also called video encoding. It is the process of\nconverting a video format to other formats (MPEG, HLS, etc), which provide the best\nvideo streams possible for different devices and bandwidth capabilities."
    },
    {
      "page": 225,
      "text": "* Transcoded storage: It is a blob storage that stores transcoded video files.\n\n* CDN: Videos are cached in CDN. When you click the play button, a video is streamed\nfrom the CDN.\n\n* Completion queue: It is a message queue that stores information about video transcoding\ncompletion events.\n\n* Completion handler: This consists of a list of workers that pull event data from the\ncompletion queue and update metadata cache and database.\nNow that we understand each component individually, let us examine how the video\nuploading flow works. The flow is broken down into two processes running in parallel.\na. Upload the actual video.\nb. Update video metadata. Metadata contains information about video URL, size,\nresolution, format, user info, etc.\n\nFlow a: upload the actual video"
    },
    {
      "page": 226,
      "text": "Original storage\n\naT i\nt\n1\n1\na\n\nMetadata Cache 3b.1.a\n\na a a\n\na —|— a1\na _|_|_f\n\nA , transcoding complete\n| BEE MMM) Se\n, Transcoding } - . : eee i\n‘servers i Completion queue fa andler 1\n\ns\n\nrank -\n\n2 .\n1\ni\n'\n&\nTranscoded 1 '\nstorage Sos mw CON. me\nFigure 14-5\n\nFigure 14-5 shows how to upload the actual video. The explanation is shown below:\n1. Videos are uploaded to the original storage.\n2. Transcoding servers fetch videos from the original storage and start transcoding.\n3. Once transcoding is complete, the following two steps are executed in parallel:\n3a. Transcoded videos are sent to transcoded storage.\n3b. Transcoding completion events are queued in the completion queue.\n3a.1. Transcoded videos are distributed to CDN.\n\n3b.1. Completion handler contains a bunch of workers that continuously pull event data\nfrom the queue."
    },
    {
      "page": 227,
      "text": "3b.1.a. and 3b.1.b. Completion handler updates the metadata database and cache when\nvideo transcoding is complete.\n\n4. API servers inform the client that the video is successfully uploaded and is ready for\nstreaming.\n\nFlow b: update the metadata\n\nWhile a file is being uploaded to the original storage, the client in parallel sends a request to\nupdate the video metadata as shown in Figure 14-6. The request contains video metadata,\nincluding file name, size, format, etc. API servers update the metadata cache and database.\n\nUpdate metadata\n\nLoad balancer\n\nok\n| ——\nTo\n1 !\nTt\n1 !\n\\ 1\n\nAPI servers\n\nMetadata Cache Metadata DB\n\nFigure 14-6\nVideo streaming flow\nWhenever you watch a video on YouTube, it usually starts streaming immediately and you\ndo not wait until the whole video is downloaded. Downloading means the whole video is\ncopied to your device, while streaming means your device continuously receives video\n\nstreams from remote source videos. When you watch streaming videos, your client loads a\nlittle bit of data at a time so you can watch videos immediately and continuously.\n\nBefore we discuss video streaming flow, let us look at an important concept: streaming\nprotocol. This is a standardized way to control data transfer for video streaming. Popular\nstreaming protocols are:\n\n* MPEG—DASH. MPEG stands for “Moving Picture Experts Group” and DASH stands for\n\"Dynamic Adaptive Streaming over HTTP\".\n\n* Apple HLS. HLS stands for “HTTP Live Streaming”.\n* Microsoft Smooth Streaming.\n« Adobe HTTP Dynamic Streaming (HDS)."
    },
    {
      "page": 228,
      "text": "You do not need to fully understand or even remember those streaming protocol names as\nthey are low-level details that require specific domain knowledge. The important thing here is\nto understand that different streaming protocols support different video encodings and\nplayback players. When we design a video streaming service, we have to choose the right\nstreaming protocol to support our use cases. To learn more about streaming protocols, here is\nan excellent article [7].\n\nVideos are streamed from CDN directly. The edge server closest to you will deliver the\nvideo. Thus, there is very little latency. Figure 14-7 shows a high level of design for video\nstreaming.\n\ncorr r,K\n\nFigure 14-7"
    },
    {
      "page": 229,
      "text": "Step 3 - Design deep dive\nIn the high-level design, the entire system is broken down in two parts: video uploading flow\n\nand video streaming flow. In this section, we will refine both flows with important\noptimizations and introduce error handling mechanisms.\n\nVideo transcoding\n\nWhen you record a video, the device (usually a phone or camera) gives the video file a\ncertain format. If you want the video to be played smoothly on other devices, the video must\nbe encoded into compatible bitrates and formats. Bitrate is the rate at which bits are processed\nover time. A higher bitrate generally means higher video quality. High bitrate streams need\nmore processing power and fast internet speed.\n\nVideo transcoding is important for the following reasons:\n\n* Raw video consumes large amounts of storage space. An hour-long high definition video\nrecorded at 60 frames per second can take up a few hundred GB of space.\n\n* Many devices and browsers only support certain types of video formats. Thus, it is\nimportant to encode a video to different formats for compatibility reasons.\n\n* To ensure users watch high-quality videos while maintaining smooth playback, it is a\ngood idea to deliver higher resolution video to users who have high network bandwidth\nand lower resolution video to users who have low bandwidth.\n\n* Network conditions can change, especially on mobile devices. To ensure a video is\nplayed continuously, switching video quality automatically or manually based on network\nconditions is essential for smooth user experience.\n\nMany types of encoding formats are available; however, most of them contain two parts:\n\n¢ Container: This is like a basket that contains the video file, audio, and metadata. You can\ntell the container format by the file extension, such as .avi, .mov, or .mp4.\n\n* Codecs: These are compression and decompression algorithms aim to reduce the video\nsize while preserving the video quality. The most used video codecs are H.264, VP9, and\nHEVC.\n\nDirected acyclic graph (DAG) model\n\nTranscoding a video is computationally expensive and time-consuming. Besides, different\ncontent creators may have different video processing requirements. For instance, some\ncontent creators require watermarks on top of their videos, some provide thumbnail images\nthemselves, and some upload high definition videos, whereas others do not.\n\nTo support different video processing pipelines and maintain high parallelism, it is important\nto add some level of abstraction and let client programmers define what tasks to execute. For\nexample, Facebook’s streaming video engine uses a directed acyclic graph (DAG)\nprogramming model, which defines tasks in stages so they can be executed sequentially or\nparallelly [8]. In our design, we adopt a similar DAG model to achieve flexibility and\nparallelism. Figure 14-8 represents a DAG for video transcoding."
    },
    {
      "page": 230,
      "text": "2 i Audio\nOr I vid Aud |___»\nriginal video sneoding Assemble\nMetadata\n\nIn Figure 14-8, the original video is split into video, audio, and metadata. Here are some of\nthe tasks that can be applied on a video file:\n\nFigure 14-8\n\n+ Inspection: Make sure videos have good quality and are not malformed.\n\n* Video encodings: Videos are converted to support different resolutions, codec, bitrates,\netc. Figure 14-9 shows an example of video encoded files.\n\n¢ Thumbnail. Thumbnails can either be uploaded by a user or automatically generated by\nthe system.\n\n* Watermark: An image overlay on top of your video contains identifying information\nabout your video."
    },
    {
      "page": 231,
      "text": "360p.mp4\n|____» 480p.mp4\n\nVideo\n\nencodings\n\na 1080p.mp4\n4k.mp4\n\nThe proposed video transcoding architecture that leverages the cloud services, is shown in\nFigure 14-10.\n\nFigure 14-9\n\nVideo transcoding architecture\n\nTemporary\nstorage\n\nP DAG Task k Encoded\nreprocessor scheduler ask workers video\n\nThe architecture has six main components: preprocessor, DAG scheduler, resource manager,\ntask workers, temporary storage, and encoded video as the output. Let us take a close look at\neach component.\n\nResource\nmanager\n\nFigure 14-10\n\nPreprocessor"
    },
    {
      "page": 232,
      "text": "Temporary\nstorage\n\nDAG\nscheduler\n\nThe preprocessor has 4 responsibilities:\n\nResource\nmanager\n\nFigure 14-11\n\n1. Video splitting. Video stream is split or further split into smaller Group of Pictures (GOP)\nalignment. GOP is a group/chunk of frames arranged in a specific order. Each chunk is an\nindependently playable unit, usually a few seconds in length.\n\n2. Some old mobile devices or browsers might not support video splitting. Preprocessor split\nvideos by GOP alignment for old clients.\n\n3. DAG generation. The processor generates DAG based on configuration files client\nprogrammers write. Figure 14-12 is a simplified DAG representation which has 2 nodes and\n\n1 edge:\n\nFigure 14-12\n\nThis DAG representation is generated from the two configuration files below (Figure 14-13):\n\nFigure 14-13 (source: [9])\n\n4. Cache data. The preprocessor is a cache for segmented videos. For better reliability, the\npreprocessor stores GOPs and metadata in temporary storage. If video encoding fails, the\nsystem could use persisted data for retry operations.\n\nDAG scheduler"
    },
    {
      "page": 233,
      "text": "Temporary\nstorage\n\nEncoded\nPreprocessor Task workers video\n\nThe DAG scheduler splits a DAG graph into stages of tasks and puts them in the task queue\nin the resource manager. Figure 14-15 shows an example of how the DAG scheduler works.\n\nResource\nmanager\n\nFigure 14-14\n\nDAG scheduler Stage 1 Stage 2\n\nVideo\nencoding\n\nThumbnail\n\nAudio\nencoding\n\nMetadata\n\nFigure 14-15\n\nAs shown in Figure 14-15, the original video is split into three stages: Stage 1: video, audio,\nand metadata. The video file is further split into two tasks in stage 2: video encoding and\nthumbnail. The audio file requires audio encoding as part of the stage 2 tasks.\n\nResource manager"
    },
    {
      "page": 234,
      "text": "Temporary\nstorage\n\nDAG\nscheduler\n\nEncoded\nvideo\n\nPreprocessor Task workers\n\nThe resource manager is responsible for managing the efficiency of resource allocation. It\ncontains 3 queues and a task scheduler as shown in Figure 14-17.\n\nFigure 14-16\n\n« Task queue: It is a priority queue that contains tasks to be executed.\n* Worker queue: It is a priority queue that contains worker utilization info.\n\n* Running queue: It contains info about the currently running tasks and workers running\nthe tasks.\n\n* Task scheduler: It picks the optimal task/worker, and instructs the chosen task worker to\nexecute the job.\n\nResource manager\n\nc Task workers\n\n,\nGet the hi ; cram) arene\n:\n\nx\n\nTask queue SS ec\nMMM : SEH\n\nget the optimal worker\n\n|\ngeal ls ae i\nrun task } | Encoder\n\nTI TI\n\nx\n\nfi\n1\n2]\nWorker queue ask scheduler, ‘ | _| -} BEE\ntte cece a |\ni Thumbnail Merger\n\n= M M put taskiworker info in queue\n\nRunning queue\n\nFigure 14-17\n\nThe resource manager works as follows:\n+ The task scheduler gets the highest priority task from the task queue.\n+ The task scheduler gets the optimal task worker to run the task from the worker queue.\n* The task scheduler instructs the chosen task worker to run the task.\n* The task scheduler binds the task/worker info and puts it in the running queue.\n* The task scheduler removes the job from the running queue once the job is done.\n\nTask workers"
    },
    {
      "page": 235,
      "text": "Temporary\nstorage\n\nEncoded\nvideo\n\nPreprocessor\n\nDAG\nscheduler\n\nTask workers run the tasks which are defined in the DAG. Different task workers may run\ndifferent tasks as shown in Figure 14-19.\n\nete ee ee ee ee\n\nResource\nmanager\n\nFigure 14-18\n\nrf Task workers ~ |\n\n[T= |\n\naan |\n\nEncoder |\n\n[T= — [T= !\n\nfee) | mee)\n\nThumbnail Merger | !\n\nSC 5 :\nFigure 14-19\n\nTemporary storage\n\nP DAG Task k Encoded\nreprocessor scheduler ask workers video\n\nMultiple storage systems are used here. The choice of storage system depends on factors like\ndata type, data size, access frequency, data life span, etc. For instance, metadata is frequently\n\nResource\nmanager\n\nFigure 14-20"
    },
    {
      "page": 236,
      "text": "accessed by workers, and the data size is usually small. Thus, caching metadata in memory is\na good idea. For video or audio data, we put them in blob storage. Data in temporary storage\nis freed up once the corresponding video processing is complete.\n\nEncoded video\n\nTemporary\nstorage\n\nDAG\nscheduler\n\nResource\nmanager\n\nPreprocessor\n\nEncoded video is the final output of the encoding pipeline. Here is an example of the output:\nfunny_720p.mp4 .\n\nFigure 14-21\n\nSystem optimizations\n\nAt this point, you ought to have good understanding about the video uploading flow, video\nstreaming flow and video transcoding. Next, we will refine the system with optimizations,\nincluding speed, safety, and cost-saving.\n\nSpeed optimization: parallelize video uploading\n\nUploading a video as a whole unit is inefficient. We can split a video into smaller chunks by\nGOP alignment as shown in Figure 14-22.\n\nSplit by GOP\nalignment\n(XK original video >| GOP 1 | GOP 2 .. |GOPN\n\nFigure 14-22\n\nThis allows fast resumable uploads when the previous upload failed. The job of splitting a\nvideo file by GOP can be implemented by the client to improve the upload speed as shown in\nFigure 14-23.\n\nBS\n\nOriginal storage\n\nFigure 14-23\n\nSpeed optimization: place upload centers close to users\nAnother way to improve the upload speed is by setting up multiple upload centers across the"
    },
    {
      "page": 237,
      "text": "globe (Figure 14-24). People in the United States can upload videos to the North America\n\nupload center, and people in China can upload videos to the Asian upload center. To achieve\nthis, we use CDN as upload centers.\n\nNorth America upload center\n\nAsian upload center\n\nEurope upload center\n\nSouth America upload center\n\nFigure 14-24\n\nSpeed optimization: parallelism everywhere\n\nAchieving low latency requires serious efforts. Another optimization is to build a loosely\ncoupled system and enable high parallelism.\n\nOur design needs some modifications to achieve high parallelism. Let us zoom in to the flow\nof how a video is transferred from original storage to the CDN. The flow is shown in Figure\n\n14-25, revealing that the output depends on the input of the previous step. This dependency\nmakes parallelism difficult.\n\n™\n——— ic\ndownload original + original 1 |\nP segmented videos ' \\_ Segmented videos, videos|! ) encoded videos\nt = ae\n\n| Download\n\nOriginal storage ' module\n.\n\n1\nt Encoding!\n‘module }\n.\n\n\\ preteens\n. : | upload upload i '\n\nEncoded videos pea ;___encoded videos éhcoded videos i :\n1 1 1\n\nUpload ' Encoded 1 Fi\n\n\\ medule i storage <_ CDN\n\nFigure 14-25\n\nTo make the system more loosely coupled, we introduced message queues as shown in Figure"
    },
    {
      "page": 238,
      "text": "14-26. Let us use an example to explain how message queues make the system more loosely\ncoupled.\n\n* Before the message queue is introduced, the encoding module must wait for the output of\nthe download module.\n\n« After the message queue is introduced, the encoding module does not need to wait for the\noutput of the download module anymore. If there are events in the message queue, the\nencoding module can execute those jobs in parallel.\n\nMessage queue Message queue\n\npoocte-s\nHae\n| Download\nOriginal storage\n\nmodule\n\nproche ns\n= I=]\n1\na _|_ |_|\n'\n» Encoding\n9 1 module\n.\n\na] aa\n\nMessage queue\n\nupload 1 H\nencoded videos_, ' 1\nr\n\n‘ 1\n\n’\n\nEncoded\nstorage\n\nprrrte-\n\nFigure 14-26\nSafety optimization: pre-signed upload URL\n\nSafety is one of the most important aspects of any product. To ensure only authorized users\nupload videos to the right location, we introduce pre-signed URLs as shown in Figure 14-27."
    },
    {
      "page": 239,
      "text": "Original storage\n\n1. POST /upload 2. <pre-signed URL>\n\n—)\n\n4\n\noe —— | ——| I\n1 !\n1 1\n, APl servers !\n\nFigure 14-27\n\nThe upload flow is updated as follows:\n1. The client makes a HTTP request to API servers to fetch the pre-signed URL, which\ngives the access permission to the object identified in the URL. The term pre-signed URL\nis used by uploading files to Amazon S3. Other cloud service providers might use a\ndifferent name. For instance, Microsoft Azure blob storage supports the same feature, but\ncall it “Shared Access Signature” [10].\n2. API servers respond with a pre-signed URL.\n3. Once the client receives the response, it uploads the video using the pre-signed URL.\n\nSafety optimization: protect your videos\n\nMany content makers are reluctant to post videos online because they fear their original\nvideos will be stolen. To protect copyrighted videos, we can adopt one of the following three\nsafety options:\n* Digital rights management (DRM) systems: Three major DRM systems are Apple\nFairPlay, Google Widevine, and Microsoft PlayReady.\n* AES encryption: You can encrypt a video and configure an authorization policy. The\nencrypted video will be decrypted upon playback. This ensures that only authorized users\ncan watch an encrypted video.\n* Visual watermarking: This is an image overlay on top of your video that contains\nidentifying information for your video. It can be your company logo or company name.\n\nCost-saving optimization\nCDN is a crucial component of our system. It ensures fast video delivery on a global scale.\n\nHowever, from the back of the envelope calculation, we know CDN is expensive, especially\nwhen the data size is large. How can we reduce the cost?\n\nPrevious research shows that YouTube video streams follow long-tail distribution [11] [12].\nIt means a few popular videos are accessed frequently but many others have few or no\nviewers. Based on this observation, we implement a few optimizations:\n\n1. Only serve the most popular videos from CDN and other videos from our high capacity"
    },
    {
      "page": 240,
      "text": "storage video servers (Figure 14-28).\n\n\\\n1\n1\ni\n1\n1\n1\n\n,\n\n‘Video servers\n\nFigure 14-28\n\n2. For less popular content, we may not need to store many encoded video versions. Short\nvideos can be encoded on-demand.\n\n3. Some videos are popular only in certain regions. There is no need to distribute these\nvideos to other regions.\n\n4. Build your own CDN like Netflix and partner with Internet Service Providers (ISPs).\nBuilding your CDN is a giant project; however, this could make sense for large streaming\ncompanies. An ISP can be Comcast, AT&T, Verizon, or other internet providers. ISPs are\nlocated all around the world and are close to users. By partnering with ISPs, you can\nimprove the viewing experience and reduce the bandwidth charges.\n\nAll those optimizations are based on content popularity, user access pattern, video size, etc. It\nis important to analyze historical viewing patterns before doing any optimization. Here are\nsome of the interesting articles on this topic: [12] [13].\n\nError handling\n\nFor a large-scale system, system errors are unavoidable. To build a highly fault-tolerant\nsystem, we must handle errors gracefully and recover from them fast. Two types of errors\nexist:\n\n* Recoverable error. For recoverable errors such as video segment fails to transcode, the\ngeneral idea is to retry the operation a few times. If the task continues to fail and the\nsystem believes it is not recoverable, it returns a proper error code to the client.\n\n¢ Non-recoverable error. For non-recoverable errors such as malformed video format, the\n\nsystem stops the running tasks associated with the video and returns the proper error code\nto the client.\n\nTypical errors for each system component are covered by the following playbook:\n* Upload error: retry a few times."
    },
    {
      "page": 241,
      "text": "* Split video error: if older versions of clients cannot split videos by GOP alignment, the\nentire video is passed to the server. The job of splitting videos is done on the server-side.\n\n¢ Transcoding error: retry.\n* Preprocessor error: regenerate DAG diagram.\n* DAG scheduler error: reschedule a task.\n* Resource manager queue down: use a replica.\n¢ Task worker down: retry the task on a new worker.\n« API server down: API servers are stateless so requests will be directed to a different API\nserver.\n* Metadata cache server down: data is replicated multiple times. If one node goes down,\nyou can still access other nodes to fetch data. We can bring up a new cache server to\nreplace the dead one.\n* Metadata DB server down:\n* Master is down. If the master is down, promote one of the slaves to act as the new\nmaster.\n* Slave is down. If a slave goes down, you can use another slave for reads and bring\nup another database server to replace the dead one."
    },
    {
      "page": 242,
      "text": "Step 4 - Wrap up\nIn this chapter, we presented the architecture design for video streaming services like\nYouTube. If there is extra time at the end of the interview, here are a few additional points:\n* Scale the API tier: Because API servers are stateless, it is easy to scale API tier\nhorizontally.\n* Scale the database: You can talk about database replication and sharding.\n+ Live streaming: It refers to the process of how a video is recorded and broadcasted in real\ntime. Although our system is not designed specifically for live streaming, live streaming\nand non-live streaming have some similarities: both require uploading, encoding, and\nstreaming. The notable differences are:\n+ Live streaming has a higher latency requirement, so it might need a different\nstreaming protocol.\n+ Live streaming has a lower requirement for parallelism because small chunks of data\nare already processed in real-time.\n+ Live streaming requires different sets of error handling. Any error handling that\ntakes too much time is not acceptable.\n* Video takedowns: Videos that violate copyrights, pornography, or other illegal acts shall\nbe removed. Some can be discovered by the system during the upload process, while\nothers might be discovered through user flagging.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 243,
      "text": "Reference materials\n\n[1] YouTube by the numbers: https://www.omnicoreagency.com/youtube-statistics/\n[2] 2019 YouTube Demographics:\nhttps://blog.hubspot.com/marketing/youtube-demographics\n\n[3] Cloudfront Pricing: https://aws.amazon.com/cloudfront/pricing/\n\n[4] Netflix on AWS: https://aws.amazon.com/solutions/case-studies/netflix/\n[5] Akamai homepage: https://www.akamai.com/\n\n[6] Binary large object: https://en.wikipedia.org/wiki/Binary large object\n[7] Here’s What You Need to Know About Streaming Protocols:\nhttps://www.dacast.com/blog/streaming-protocols/\n\n[8] SVE: Distributed Video Processing at Facebook Scale:\nhttps://www.cs.princeton.edu/~wlloyd/papers/sve-sosp17.pdf\n\n[9] Weibo video processing architecture (in Chinese):\nhttps://www.upyun.com/opentalk/399.html\n\n[10] Delegate access with a shared access signature:\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/delegate-access-with-shared-access-\nsignature\n\n[11] YouTube scalability talk by early YouTube employee: https://www.youtube.com/watch?\nv=w5WVu624fY8\n\n[12] Understanding the characteristics of internet short video sharing: A youtube-based\nmeasurement study. https://arxiv.org/pdf/0707.3670.pdf\n\n[13] Content Popularity for Open Connect:\nhttps://netflixtechblog.com/content-popularity-for-open-connect-b86d56f613b"
    },
    {
      "page": 244,
      "text": "CHAPTER 15: DESIGN GOOGLE DRIVE\n\nIn recent years, cloud storage services such as Google Drive, Dropbox, Microsoft OneDrive,\nand Apple iCloud have become very popular. In this chapter, you are asked to design Google\nDrive.\n\nLet us take a moment to understand Google Drive before jumping into the design. Google\nDrive is a file storage and synchronization service that helps you store documents, photos,\nvideos, and other files in the cloud. You can access your files from any computer,\nsmartphone, and tablet. You can easily share those files with friends, family, and coworkers\n[1]. Figure 15-1 and 15-2 show what Google drive looks like on a browser and mobile\napplication, respectively.\n\n€ C 0 @ drivegoogle.com Yr © incognito\ny ay Drive Q_ Search Drive < ®\nMy Drive ~ oe # ©\n+ New\nName 4 Owner Last modified\n> 2B) MyDrive\nBa Recipes Alex 41 PM Alex\n2, — Shared with me\nSalad Alex 1:31 PM Alex\n© _ Recent a A\nXv Starred Bs Soup Alex PM Alex\nTW Trash Apple cake fits 1:52 PM me\n(E Backups & seq 18 55 PM me\n\nFigure 15-1"
    },
    {
      "page": 245,
      "text": "1587 “\n\n=} _ Search Drive\n\nMy Drive Computers\n\nName“\n\nws Recipes\n+ Modified Doe 23, 2019\n\nI\nws Salad ee\n\n+ Modified Dec 23, 2019\n\n@ Soup es\n\nModified Dec 23, 2019\n\nfp Cetting started\n\nModified Nov 12, 2019\n\nSe Lemon shortbread\nModified Dec 23, 2019\n\neS Pasta. JPG\nModified Dec 23, 2019\n\nBB OPizza z\n\nModified Dec 23, 2019 |\n\nPotato latkes\n\na w a\n\nFigure 15-2"
    },
    {
      "page": 246,
      "text": "Step 1 - Understand the problem and establish design scope\nDesigning a Google drive is a big project, so it is important to ask questions to narrow down\nthe scope.\n\nCandidate: What are the most important features?\nInterviewer: Upload and download files, file sync, and notifications.\n\nCandidate: Is this a mobile app, a web app, or both?\nInterviewer: Both.\n\nCandidate: What are the supported file formats?\nInterviewer: Any file type.\n\nCandidate: Do files need to be encrypted?\nInterview: Yes, files in the storage must be encrypted.\n\nCandidate: Is there a file size limit?\nInterview: Yes, files must be 10 GB or smaller.\n\nCandidate: How many users does the product have?\n\nInterviewer: 10M DAU.\n\nIn this chapter, we focus on the following features:\n« Add files. The easiest way to add a file is to drag and drop a file into Google drive.\n* Download files.\n\n+ Sync files across multiple devices. When a file is added to one device, it is automatically\nsynced to other devices.\n\n* See file revisions.\n* Share files with your friends, family, and coworkers\n* Send a notification when a file is edited, deleted, or shared with you.\n\nFeatures not discussed in this chapter include:\n* Google doc editing and collaboration. Google doc allows multiple people to edit the\nsame document simultaneously. This is out of our design scope.\n\nOther than clarifying requirements, it is important to understand non-functional requirements:\n\n* Reliability. Reliability is extremely important for a storage system. Data loss is\nunacceptable.\n\n+ Fast sync speed. If file sync takes too much time, users will become impatient and\nabandon the product.\n\n* Bandwidth usage. If a product takes a lot of unnecessary network bandwidth, users will\nbe unhappy, especially when they are on a mobile data plan.\n\n* Scalability. The system should be able to handle high volumes of traffic.\n* High availability. Users should still be able to use the system when some servers are\noffline, slowed down, or have unexpected network errors.\nBack of the envelope estimation\n« Assume the application has 50 million signed up users and 10 million DAU.\n* Users get 10 GB free space.\n* Assume users upload 2 files per day. The average file size is 500 KB.\n* 1:1 read to write ratio."
    },
    {
      "page": 247,
      "text": "* Total space allocated: 50 million * 10 GB = 500 Petabyte\n* QPS for upload API: 10 million * 2 uploads / 24 hours / 3600 seconds = ~ 240\n* Peak QPS = QPS * 2 = 480"
    },
    {
      "page": 248,
      "text": "Step 2 - Propose high-level design and get buy-in\n\nInstead of showing the high-level design diagram from the beginning, we will use a slightly\ndifferent approach. We will start with something simple: build everything in a single server.\nThen, gradually scale it up to support millions of users. By doing this exercise, it will refresh\nyour memory about some important topics covered in the book.\n\nLet us start with a single server setup as listed below:\n* A web server to upload and download files.\n+ A database to keep track of metadata like user data, login info, files info, etc.\n+ A storage system to store files. We allocate 1TB of storage space to store files.\n\nWe spend a few hours setting up an Apache web server, a MySq| database, and a directory\ncalled drive/ as the root directory to store uploaded files. Under drive/ directory, there is a list\nof directories, known as namespaces. Each namespace contains all the uploaded files for that\nuser. The filename on the server is kept the same as the original file name. Each file or folder\ncan be uniquely identified by joining the namespace and the relative path.\n\nFigure 15-3 shows an example of how the /drive directory looks like on the left side and its\nexpanded view on the right side.\n\n; drive\ndrive\nuser1\nuser] expand ;\ncorcesssead > recipes\nuser2 .\n= chicken_soup.txt\nuser3\nuser2\n= football.mov\n= sports.txt\nuser3\n\n= best_pic_ever.png\n\nFigure 15-3\nAPIs\nWhat do the APIs look like? We primary need 3 APIs: upload a file, download a file, and get\nfile revisions.\n1. Upload a file to Google Drive\nTwo types of uploads are supported:\n* Simple upload. Use this upload type when the file size is small.\n* Resumable upload. Use this upload type when the file size is large and there is high\nchance of network interruption.\nHere is an example of resumable upload API:\nhttps://api.example.com/files/upload ?uploadType=resumable\nParams:\n* uploadType=resumable\n+ data: Local file to be uploaded."
    },
    {
      "page": 249,
      "text": "A resumable upload is achieved by the following 3 steps [2]:\n¢ Send the initial request to retrieve the resumable URL.\n* Upload the data and monitor upload state.\nIf upload is disturbed, resume the upload.\n2. Download a file from Google Drive\nExample API: https://api.example.com/files/download\nParams:\n* path: download file path.\nExample params:\n\n{\n\n\"path\": \"/recipes/soup/best_soup.txt\"\n}\n3. Get file revisions\nExample API: https://api.example.com/files/list_revisions\nParams:\n* path: The path to the file you want to get the revision history.\n¢ limit: The maximum number of revisions to return.\nExample params:\n\n{\n\n\"path\": \"/recipes/soup/best_soup.txt\",\n\"limit\": 20\n}\nAll the APIs require user authentication and use HTTPS. Secure Sockets Layer (SSL)\nprotects data transfer between the client and backend servers.\nMove away from single server\nAs more files are uploaded, eventually you get the space full alert as shown in Figure 15-4.\n\n/drive\n\nVE\n\n10 MB free of 1 TB\nFigure 15-4\n\nOnly 10 MB of storage space is left! This is an emergency as users cannot upload files\nanymore. The first solution comes to mind is to shard the data, so it is stored on multiple\nstorage servers. Figure 15-5 shows an example of sharding based on user_id ."
    },
    {
      "page": 250,
      "text": "user_id % 4\n\nServer1 Server2 Server3 Server4\nFigure 15-5\n\nYou pull an all-nighter to set up database sharding and monitor it closely. Everything works\nsmoothly again. You have stopped the fire, but you are still worried about potential data\nlosses in case of storage server outage. You ask around and your backend guru friend Frank\ntold you that many leading companies like Netflix and Airbnb use Amazon S3 for storage.\n“Amazon Simple Storage Service (Amazon S3) is an object storage service that offers\nindustry-leading scalability, data availability, security, and performance” [3]. You decide to\ndo some research to see if it is a good fit.\n\nAfter a lot of reading, you gain a good understanding of the S3 storage system and decide to\nstore files in S3. Amazon S3 supports same-region and cross-region replication. A region is a\ngeographic area where Amazon web services (AWS) have data centers. As shown in Figure\n15-6, data can be replicated on the same-region (left side) and cross-region (right side).\nRedundant files are stored in multiple regions to guard against data loss and ensure\navailability. A bucket is like a folder in file systems.\n\nreplication\n\nreplication\n\nreplication\n\npaesis\n\nSame-region replication Cross-region replication\nFigure 15-6\n\nAfter putting files in S3, you can finally have a good night's sleep without worrying about\ndata losses. To stop similar problems from happening in the future, you decide to do further\nresearch on areas you can improve. Here are a few areas you find:\n\n* Load balancer: Add a load balancer to distribute network traffic. A load balancer ensures"
    },
    {
      "page": 251,
      "text": "evenly distributed traffic, and if a web server goes down, it will redistribute the traffic.\n¢ Web servers: After a load balancer is added, more web servers can be added/removed\neasily, depending on the traffic load.\n\n* Metadata database: Move the database out of the server to avoid single point of failure.\nIn the meantime, set up data replication and sharding to meet the availability and\nscalability requirements.\n\n+ File storage: Amazon S3 is used for file storage. To ensure availability and durability,\nfiles are replicated in two separate geographical regions.\n\nAfter applying the above improvements, you have successfully decoupled web servers,\nmetadata database, and file storage from a single server. The updated design is shown in\n\nFigure 15-7.\n\nWeb browser Mobile app\n\nUser\n\nLoad balancer\n\neee eee eee\n\nAPI servers '\n\nSS aie\n\nMetadata DB File storage\n\nFigure 15-7\n\nSync conflicts\n\nFor a large storage system like Google Drive, sync conflicts happen from time to time. When\ntwo users modify the same file or folder at the same time, a conflict happens. How can we\nresolve the conflict? Here is our strategy: the first version that gets processed wins, and the\nversion that gets processed later receives a conflict. Figure 15-8 shows an example of a sync\nconflict."
    },
    {
      "page": 252,
      "text": "I\nUser 1 User 1 incay User 1 ‘incaz\nOur system Our system Our system\n(6)\n4 h\nge\n\nUser 2 User 2 User 2\nFigure 15-8\n\nIn Figure 15-8, user 1 and user 2 tries to update the same file at the same time, but user 1’s\nfile is processed by our system first. User 1’s update operation goes through, but, user 2 gets\na sync conflict. How can we resolve the conflict for user 2? Our system presents both copies\nof the same file: user 2’s local copy and the latest version from the server (Figure 15-9). User\n2 has the option to merge both files or override one version with the other.\n\nEJ SystemDesigninterview\n\n=n SystemDesignInterview_user 2_conflicted_copy_2019-05-01\n\nFigure 15-9\nWhile multiple users are editing the same document at the same, it is challenging to keep the\n\ndocument synchronized. Interested readers should refer to the reference material [4] [5].\n\nHigh-level design\n\nFigure 15-10 illustrates the proposed high-level design. Let us examine each component of\nthe system."
    },
    {
      "page": 253,
      "text": "lon A\n\nLoad balancer long polling\n\nfer]\nie}\nis}\nro\na\no\n2\noD\na\n\nJ Ra]ealal\n\nNotification\nService Offline backup Queue\n\nCold storage Metadata DB\n\nMetadata Cache\n\nFigure 15-10\nUser: A user uses the application either through a browser or mobile app.\n\nBlock servers: Block servers upload blocks to cloud storage. Block storage, referred to as\nblock-level storage, is a technology to store data files on cloud-based environments. A file\ncan be split into several blocks, each with a unique hash value, stored in our metadata\ndatabase. Each block is treated as an independent object and stored in our storage system\n(S3). To reconstruct a file, blocks are joined in a particular order. As for the block size, we\nuse Dropbox as a reference: it sets the maximal size of a block to 4MB [6].\n\nCloud storage: A file is split into smaller blocks and stored in cloud storage.\n\nCold storage: Cold storage is a computer system designed for storing inactive data, meaning\nfiles are not accessed for a long time.\n\nLoad balancer: A load balancer evenly distributes requests among API servers.\n\nAPI servers: These are responsible for almost everything other than the uploading flow. API\nservers are used for user authentication, managing user profile, updating file metadata, etc.\n\nMetadata database: It stores metadata of users, files, blocks, versions, etc. Please note that\nfiles are stored in the cloud and the metadata database only contains metadata.\n\nMetadata cache: Some of the metadata are cached for fast retrieval.\n\nNotification service: It is a publisher/subscriber system that allows data to be transferred\nfrom notification service to clients as certain events happen. In our specific case, notification\nservice notifies relevant clients when a file is added/edited/removed elsewhere so they can\npull the latest changes.\n\nOffline backup queue: If a client is offline and cannot pull the latest file changes, the offline"
    },
    {
      "page": 254,
      "text": "backup queue stores the info so changes will be synced when the client is online.\n\nWe have discussed the design of Google Drive at the high-level. Some of the components are\ncomplicated and worth careful examination; we will discuss these in detail in the deep dive."
    },
    {
      "page": 255,
      "text": "Step 3 - Design deep dive\nIn this section, we will take a close look at the following: block servers, metadata database,\nupload flow, download flow, notification service, save storage space and failure handling.\n\nBlock servers\n\nFor large files that are updated regularly, sending the whole file on each update consumes a\nlot of bandwidth. Two optimizations are proposed to minimize the amount of network traffic\nbeing transmitted:\n* Delta sync. When a file is modified, only modified blocks are synced instead of the\nwhole file using a sync algorithm [7] [8].\n* Compression. Applying compression on blocks can significantly reduce the data size.\nThus, blocks are compressed using compression algorithms depending on file types. For\nexample, gzip and bzip2 are used to compress text files. Different compression algorithms\nare needed to compress images and videos.\n\nIn our system, block servers do the heavy lifting work for uploading files. Block servers\nprocess files passed from clients by splitting a file into blocks, compressing each block, and\nencrypting them. Instead of uploading the whole file to the storage system, only modified\nblocks are transferred.\n\nFigure 15-11 shows how a block server works when a new file is added.\n\nF encrypt\nsplit Block 1. Poompress\n= split Block 2 compress | encrypt\n\nCloud storage\n\nf compress encrypt\nsplit} Block N\n\nBlock servers\n\nFigure 15-11\n« A file is split into smaller blocks.\n* Each block is compressed using compression algorithms.\n* To ensure security, each block is encrypted before it is sent to cloud storage.\n* Blocks are uploaded to the cloud storage.\nFigure 15-12 illustrates delta sync, meaning only modified blocks are transferred to cloud\n\nstorage. Highlighted blocks “block 2” and “block 5” represent changed blocks. Using delta\nsync, only those two blocks are uploaded to the cloud storage."
    },
    {
      "page": 256,
      "text": "Block 1 Block 2\n\nBlock 2\n\nBlock 3 Block 4\n\nchanged only\n\nBlock 5 Block 6\nBlock 7 Block 8\n\n5)\n\nBlock 9 Block 10\n\nBlock servers\n\nFigure 15-12\nBlock servers allow us to save network traffic by providing delta sync and compression.\n\nHigh consistency requirement\n\nOur system requires strong consistency by default. It is unacceptable for a file to be shown\ndifferently by different clients at the same time. The system needs to provide strong\nconsistency for metadata cache and database layers.\n\nMemory caches adopt an eventual consistency model by default, which means different\nreplicas might have different data. To achieve strong consistency, we must ensure the\nfollowing:\n\n* Data in cache replicas and the master is consistent.\n\n+ Invalidate caches on database write to ensure cache and database hold the same value.\nAchieving strong consistency in a relational database is easy because it maintains the ACID\n(Atomicity, Consistency, Isolation, Durability) properties [9]. However, NoSQL databases do\nnot support ACID properties by default. ACID properties must be programmatically\nincorporated in synchronization logic. In our design, we choose relational databases because\nthe ACID is natively supported.\n\nMetadata database\n\nFigure 15-13 shows the database schema design. Please note this is a highly simplified\nversion as it only includes the most important tables and interesting fields."
    },
    {
      "page": 257,
      "text": "1\n\n1\n\nid bigint ig isis\nerect bigint file_name varchar\nuser Bane boolean relative_path varchar\nwesille bigint 1 poe is_directory boolean\ncreated_at timestamp\nease jut vercher lastest_version bigint\ncreated_at timestamp checkeura pele\n“= __workspace_id bigint\nJ block | ——- Beaty\nblock _id bigint last_modified timestamp\n*\nfile_version_id bigint\nblock_order int file_version\nid bigint\ndevice oat Gg\ndevice_id uuid device_id uuid\nversion_number bigint\nuser_id bigint =? a 6\nlast_logged_in_at timestamp last_modified timestamp\n\nFigure 15-13\n\nUser: The user table contains basic information about the user such as username, email,\nprofile photo, etc.\n\nDevice: Device table stores device info. Push_id is used for sending and receiving mobile\npush notifications. Please note a user can have multiple devices.\n\nNamespace: A namespace is the root directory of a user.\nFile: File table stores everything related to the latest file.\n\nFile_version: It stores version history of a file. Existing rows are read-only to keep the\nintegrity of the file revision history.\n\nBlock: It stores everything related to a file block. A file of any version can be reconstructed\nby joining all the blocks in the correct order.\nUpload flow\n\nLet us discuss what happens when a client uploads a file. To better understand the flow, we\ndraw the sequence diagram as shown in Figure 15-14."
    },
    {
      "page": 258,
      "text": "| :API Servers | :Metadata DB |\n\n| Notification\n\n[:cous storage 2\nservice\n\n:Client 1 | | :Client 2 | | ‘Block servers\n\n| 4, add file\n\npa _, metadata i b a\n: : +f] 2.upload status:\nt pending 2\nbending, _.\n3. notify change}\naver\n4. notify changes\nese sessuvawsnuseer vesteundtanvasrresseastuaneemmmeendsrsueliene en cael\n2.1 upload fil i\nupload file =i\n\n22 upload file\neee |\n| 2.3 update file\n\nmetadata\n\n2.4 upload status: :\nuploaded\n\n2.5 notify change. !\n\nFigure 15-14\n\nIn Figure 15-14, two requests are sent in parallel: add file metadata and upload the file to\ncloud storage. Both requests originate from client 1.\n\n+ Add file metadata.\n1. Client 1 sends a request to add the metadata of the new file.\n\n2. Store the new file metadata in metadata DB and change the file upload status to\n“pending.”\n\n3. Notify the notification service that a new file is being added.\n\n4. The notification service notifies relevant clients (client 2) that a file is being\nuploaded.\n\n* Upload files to cloud storage.\n2.1 Client 1 uploads the content of the file to block servers.\n\n2.2 Block servers chunk the files into blocks, compress, encrypt the blocks, and\nupload them to cloud storage.\n\n2.3 Once the file is uploaded, cloud storage triggers upload completion callback. The\nrequest is sent to API servers.\n\n2.4 File status changed to “uploaded” in Metadata DB.\n2.5 Notify the notification service that a file status is changed to “uploaded.”\n\n2.6 The notification service notifies relevant clients (client 2) that a file is fully\nuploaded.\n\nWhen a file is edited, the flow is similar, so we will not repeat it.\n\nDownload flow\n\nDownload flow is triggered when a file is added or edited elsewhere. How does a client know\nif a file is added or edited by another client? There are two ways a client can know:\n\n* If client A is online while a file is changed by another client, notification service will\ninform client A that changes are made somewhere so it needs to pull the latest data."
    },
    {
      "page": 259,
      "text": "« If client A is offline while a file is changed by another client, data will be saved to the\ncache. When the offline client is online again, it pulls the latest changes.\n\nOnce a client knows a file is changed, it first requests metadata via API servers, then\ndownloads blocks to construct the file. Figure 15-15 shows the detailed flow. Note, only the\nmost important components are shown in the diagram due to space constraint.\n\n‘Notification\nservice\n\n| ‘API Servers | :Metadata DB | |\n\n| :Client 2 | | ‘Block servers | :Cloud storage\n\n2. get changes\n\n3. get changes\n\n4. return changes\n<\n\n: 5. return chang\nGSS secseaeebeds owen ese sees ee Uae ele\n\n6. download blocks i '\n\n7. download blocks\nEee\n\n8. blocks\nKGrennnneberons nee enene ;\n\n9. blocks\n\nFigure 15-15\n\n1. Notification service informs client 2 that a file is changed somewhere else.\n\n2. Once client 2 knows that new updates are available, it sends a request to fetch metadata.\n3. API servers call metadata DB to fetch metadata of the changes.\n\n4. Metadata is returned to the API servers.\n\n5. Client 2 gets the metadata.\n\n6. Once the client receives the metadata, it sends requests to block servers to download\nblocks.\n\n7. Block servers first download blocks from cloud storage.\n8. Cloud storage returns blocks to the block servers.\n9. Client 2 downloads all the new blocks to reconstruct the file.\n\nNotification service\n\nTo maintain file consistency, any mutation of a file performed locally needs to be informed to\nother clients to reduce conflicts. Notification service is built to serve this purpose. At the\nhigh-level, notification service allows data to be transferred to clients as events happen. Here\nare a few options:\n\n* Long polling. Dropbox uses long polling [10].\n* WebSocket. WebSocket provides a persistent connection between the client and the\nserver. Communication is bi-directional.\n\nEven though both options work well, we opt for long polling for the following two reasons:\n\n* Communication for notification service is not bi-directional. The server sends\ninformation about file changes to the client, but not vice versa.\n\n* WebSocket is suited for real-time bi-directional communication such as a chat app. For"
    },
    {
      "page": 260,
      "text": "Google Drive, notifications are sent infrequently with no burst of data.\n\nWith long polling, each client establishes a long poll connection to the notification service. If\nchanges to a file are detected, the client will close the long poll connection. Closing the\nconnection means a client must connect to the metadata server to download the latest\nchanges. After a response is received or connection timeout is reached, a client immediately\nsends a new request to keep the connection open.\n\nSave storage space\n\nTo support file version history and ensure reliability, multiple versions of the same file are\nstored across multiple data centers. Storage space can be filled up quickly with frequent\nbackups of all file revisions. Three techniques are proposed to reduce storage costs:\n\n* De-duplicate data blocks. Eliminating redundant blocks at the account level is an easy\nway to save space. Two blocks are identical if they have the same hash value.\n\n« Adopt an intelligent data backup strategy. Two optimization strategies can be applied:\n\n¢ Set a limit: We can set a limit for the number of versions to store. If the limit is\nreached, the oldest version will be replaced with the new version.\n\n¢ Keep valuable versions only: Some files might be edited frequently. For example,\nsaving every edited version for a heavily modified document could mean the file is\nsaved over 1000 times within a short period. To avoid unnecessary copies, we could\nlimit the number of saved versions. We give more weight to recent versions.\nExperimentation is helpful to figure out the optimal number of versions to save.\n* Moving infrequently used data to cold storage. Cold data is the data that has not been\nactive for months or years. Cold storage like Amazon S3 glacier [11] is much cheaper than\nS3.\n\nFailure Handling\n\nFailures can occur in a large-scale system and we must adopt design strategies to address\nthese failures. Your interviewer might be interested in hearing about how you handle the\nfollowing system failures:\n\n* Load balancer failure: If a load balancer fails, the secondary would become active and\npick up the traffic. Load balancers usually monitor each other using a heartbeat, a periodic\nsignal sent between load balancers. A load balancer is considered as failed if it has not sent\na heartbeat for some time.\n\n* Block server failure: If a block server fails, other servers pick up unfinished or pending\njobs.\n\n* Cloud storage failure: S3 buckets are replicated multiple times in different regions. If\nfiles are not available in one region, they can be fetched from different regions.\n\n¢ API server failure: It is a stateless service. If an API server fails, the traffic is redirected\nto other API servers by a load balancer.\n\n* Metadata cache failure: Metadata cache servers are replicated multiple times. If one node\ngoes down, you can still access other nodes to fetch data. We will bring up a new cache\nserver to replace the failed one.\n\n* Metadata DB failure.\n\n* Master down: If the master is down, promote one of the slaves to act as a new master\nand bring up a new slave node.\n\n* Slave down: If a slave is down, you can use another slave for read operations and"
    },
    {
      "page": 261,
      "text": "bring another database server to replace the failed one.\n* Notification service failure: Every online user keeps a long poll connection with the\nnotification server. Thus, each notification server is connected with many users. According\nto the Dropbox talk in 2012 [6], over 1 million connections are open per machine. If a\nserver goes down, all the long poll connections are lost so clients must reconnect to a\ndifferent server. Even though one server can keep many open connections, it cannot\nreconnect all the lost connections at once. Reconnecting with all the lost clients is a\nrelatively slow process.\n* Offline backup queue failure: Queues are replicated multiple times. If one queue fails,\nconsumers of the queue may need to re-subscribe to the backup queue."
    },
    {
      "page": 262,
      "text": "Step 4 - Wrap up\n\nIn this chapter, we proposed a system design to support Google Drive. The combination of\nstrong consistency, low network bandwidth and fast sync make the design interesting. Our\ndesign contains two flows: manage file metadata and file sync. Notification service is another\nimportant component of the system. It uses long polling to keep clients up to date with file\nchanges.\n\nLike any system design interview questions, there is no perfect solution. Every company has\nits unique constraints and you must design a system to fit those constraints. Knowing the\ntradeoffs of your design and technology choices are important. If there are a few minutes left,\nyou can talk about different design choices.\n\nFor example, we can upload files directly to cloud storage from the client instead of going\nthrough block servers. The advantage of this approach is that it makes file upload faster\nbecause a file only needs to be transferred once to the cloud storage. In our design, a file is\ntransferred to block servers first, and then to the cloud storage. However, the new approach\nhas a few drawbacks:\n\n« First, the same chunking, compression, and encryption logic must be implemented on\ndifferent platforms (iOS, Android, Web). It is error-prone and requires a lot of engineering\neffort. In our design, all those logics are implemented in a centralized place: block servers.\n\n* Second, as a client can easily be hacked or manipulated, implementing encrypting logic\non the client side is not ideal.\n\nAnother interesting evolution of the system is moving online/offline logic to a separate\nservice. Let us call it presence service. By moving presence service out of notification\nservers, online/offline functionality can easily be integrated by other services.\n\nCongratulations on getting this far! Now give yourself a pat on the back. Good job!"
    },
    {
      "page": 263,
      "text": "Reference materials\n\n[1] Google Drive: https://www.google.com/drive/\n\n[2] Upload file data: https://developers.google.com/drive/api/v2/manage-uploads\n\n[3] Amazon S3: https://aws.amazon.com/s3\n\n[4] Differential Synchronization https://neil.fraser.name/writing/sync/\n\n[5] Differential Synchronization youtube talk https://www.youtube.com/watch?\nv=S2Hp liqpY8\n\n[6] How We’ve Scaled Dropbox: https://youtu.be/PE4gwstWhmc\n\n[7] Tridgell, A., & Mackerras, P. (1996). The rsync algorithm.\n\n[8] Librsync. (n.d.). Retrieved April 18, 2015, from https://github.com/librsync/librsync\n[9] ACID: https://en.wikipedia.org/wiki/ACID\n\n[10] Dropbox security white paper:\nhttps://www.dropbox.com/static/business/resources/Security_Whitepaper.pdf\n\n[11] Amazon S3 Glacier: https://aws.amazon.com/glacier/faqs/"
    },
    {
      "page": 264,
      "text": "CHAPTER 16: THE LEARNING CONTINUES\n\nDesigning good systems requires years of accumulation of knowledge. One shortcut is to dive\ninto real-world system architectures. Below is a collection of helpful reading materials. We\nhighly recommend you pay attention to both the shared principles and the underlying\ntechnologies. Researching each technology and understanding what problems it solves is a\ngreat way to strengthen your knowledge base and refine the design process."
    },
    {
      "page": 265,
      "text": "Real-world systems\n\nThe following materials can help you understand general design ideas of real system\narchitectures behind different companies.\n\nFacebook Timeline: Brought To You By The Power Of Denormalization:\nhttps://go0.gl/FCNrbm\n\nScale at Facebook: https://goo.gl/NGTdCs\nBuilding Timeline: Scaling up to hold your life story: https://goo.gl/8p5wDV\n\nErlang at Facebook (Facebook chat): https://go00.g]/zSLHrj\nFacebook Chat: https://go0.gl/qzSiwC\nFinding a needle in Haystack: Facebook’s photo storage: https://g00.gl/edj4FL\n\nServing Facebook Multifeed: Efficiency, performance gains through redesign:\nhttps://goo.g]/adF VMQ\n\nScaling Memcache at Facebook: https://go0.gl/rZiAhX\nTAO: Facebook’s Distributed Data Store for the Social Graph: https://goo.gl/Tk1DyH\n\nAmazon Architecture: https://go00.gl/k4feow\n\nDynamo: Amazon’s Highly Available Key-value Store: https://g00.gl/C7zxDL\n\nA 360 Degree View Of The Entire Netflix Stack: https://goo.gl/rYSDTz\n\nIt’s All A/Bout Testing: The Netflix Experimentation Platform: https://goo.gl/agbA4K\nNetflix Recommendations: Beyond the 5 stars (Part 1): https://go0.gl/A4FkYi\n\nNetflix Recommendations: Beyond the 5 stars (Part 2): https://go0.gl/K NPMXm\n\nGoogle Architecture: https://goo.g]/dvkDiY\n\nThe Google File System (Google Docs): https://go00.g]/xj5n9R\n\nDifferential Synchronization (Google Docs): https://go00.g]/9zqG7x\n\nYouTube Architecture: https://goo.g]/mCPRUF\n\nSeattle Conference on Scalability: YouTube Scalability: https://goo.gl/dH3zYq\n\nBigtable: A Distributed Storage System for Structured Data: https://goo.gl/6NaZca\n\nInstagram Architecture: 14 Million Users, Terabytes Of Photos, 100s Of Instances, Dozens\nOf Technologies: https://g00.gl/s1VcW5S\n\nThe Architecture Twitter Uses To Deal With 150M Active Users: https://goo.gl/EwvfRd\nScaling Twitter: Making Twitter 10000 Percent Faster: https://goo.gl/nYGC1k\n\nAnnouncing Snowflake (Snowflake is a network service for generating unique ID numbers at\nhigh scale with some simple guarantees): https://goo.gl/GzZVWYm\n\nTimelines at Scale: https://g00.g//8KbqTy\nHow Uber Scales Their Real-Time Market Platform: https://goo.gl/kGZuVy\n\nScaling Pinterest: https://g00.gl/KtmjW3\n\nPinterest Architecture Update: https://goo.gl/w6rRsf"
    },
    {
      "page": 266,
      "text": "A Brief History of Scaling LinkedIn: https://goo.gl/8A1Pi8\n\nFlickr Architecture: https://goo.gl/dWtgYa\n\nHow We've Scaled Dropbox: https://goo.gl/NjBDtC\n\nThe WhatsApp Architecture Facebook Bought For $19 Billion: https://bit.ly/2AHJnFn"
    },
    {
      "page": 267,
      "text": "Company engineering blogs\n\nIf you are going to interview with a company, it is a great idea to read their engineering blogs\nand get familiar with technologies and systems adopted and implemented there. Besides,\nengineering blogs provide invaluable insights about certain fields. Reading them regularly\ncould help us become better engineers.\n\nHere is a list of engineering blogs of well-known large companies and startups.\n\nAirbnb: https://medium.com/airbnb-engineering\nAmazon: https://developer.amazon.com/blogs\n\nAsana: https://blog.asana.com/category/eng\n\nAtlassian: https://developer.atlassian.com/blog\n\nBittorrent: http://engineering.bittorrent.com\nCloudera: https://blog.cloudera.com\n\nDocker: https://blog.docker.com\n\nDropbox: https://blogs.dropbox.com/tech\n\neBay: http://www.ebaytechblog.com\n\nFacebook: https://code.facebook.com/posts\n\nGitHub: https://githubengineering.com\nGoogle: https://developers.googleblog.com\n\nGroupon: https://engineering.groupon.com\nHighscalability: http://highscalability.com\nInstacart: https://tech.instacart.com\n\nInstagram: https://engineering.instagram.com\n\nLinkedin: https://engineering.linkedin.com/blog\n\nMixpanel: https://mixpanel.com/blog\nNetflix: https://medium.com/netflix-techblog\nNextdoor: https://engblog.nextdoor.com\n\nPayPal: https://www.paypal-engineering.com\n\nPinterest: https://engineering.pinterest.com\n\nQuora: https://engineering.quora.com\n\nReddit: https://redditblog.com\nSalesforce: https://developer.salesforce.com/blogs/engineering\n\nShopify: https://engineering.shopify.com\n\nSlack: https://slack.engineering\n\nSoundcloud: https://developers.soundcloud.com/blog\nSpotify: https://labs.spotify.com\n\nStripe: https://stripe.com/blog/engineering\n\nSystem design primer: https://github.com/donnemartin/system-design-primer\n\nTwitter: https://blog.twitter.com/engineering/en_us.html\n\nThumbtack: https://www.thumbtack.com/engineering\nUber: http://eng.uber.com"
    },
    {
      "page": 268,
      "text": "Yahoo: https://yahooeng.tumblr.com\n\nYelp: https://engineeringblog.yelp.com\n\nZoom: https://medium.com/zoom-developer-blog"
    },
    {
      "page": 269,
      "text": "AFTERWORD\n\nCongratulations! You are at the end of this interview guide. You have accumulated skills and\nknowledge to design systems. Not everyone has the discipline to learn what you have learned.\nTake a moment and pat yourself on the back. Your hard work will be paid off.\n\nLanding a dream job is a long journey and requires lots of time and effort. Practice makes\nperfect. Best luck!\n\nThank you for buying and reading this book. Without readers like you, our work would not\nexist. We hope you have enjoyed the book!\n\nIf you don’t mind, please review this book on Amazon: https://tinyurl.com/y7d3ltbe It would\nhelp me attract more wonderful readers like you.\n\nPlease subscribe to our email list if you want to be notified when new chapters are available:\nhttps://bit.ly/3dtIcsE\n\nIf you have comments or questions about this book, feel free to send us an email at\nsystemdesigninsider@gmail.com. Besides, if you notice any errors, please let us know so we\ncan make corrections in the next edition. Thank you!"
    }
  ]
}